#!/usr/bin/env python3
"""
Claim ë°ì´í„° ê¸°ë°˜ Mock ë¶„ì„ DB ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ë¶„ì„ ì „ì— UI ê°œë°œì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
import json
import random
from pathlib import Path
import sys
import logging

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.database import get_database_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockAnalyticsGenerator:
    """Mock ë¶„ì„ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, source_db_path: str = None, target_db_path: str = None):
        self.source_db = source_db_path or str(project_root / 'data' / 'sambio.db')
        self.target_db = target_db_path or str(project_root / 'data' / 'sambio_analytics_mock.db')
        
        # íš¨ìœ¨ì„± ë¶„í¬ ì„¤ì • (í˜„ì‹¤ì ì¸ ë¶„í¬)
        self.efficiency_dist = {
            'Lv.1': {'mean': 85, 'std': 8},   # ì‚¬ì›
            'Lv.2': {'mean': 88, 'std': 7},   # ëŒ€ë¦¬
            'Lv.3': {'mean': 90, 'std': 6},   # ê³¼ì¥
            'Lv.4': {'mean': 92, 'std': 5},   # ë¶€ì¥
            'ê²½ì˜ì§„': {'mean': 95, 'std': 3}   # ê²½ì˜ì§„
        }
        
        logger.info(f"MockAnalyticsGenerator ì´ˆê¸°í™”")
        logger.info(f"Source DB: {self.source_db}")
        logger.info(f"Target DB: {self.target_db}")
    
    def init_mock_db(self):
        """Mock DB ì´ˆê¸°í™” (ì‹¤ì œ ë¶„ì„ DBì™€ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆ)"""
        conn = sqlite3.connect(self.target_db)
        cursor = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
        cursor.execute("DROP TABLE IF EXISTS daily_analysis")
        cursor.execute("DROP TABLE IF EXISTS team_daily_summary")
        cursor.execute("DROP TABLE IF EXISTS center_daily_summary")
        
        # daily_analysis í…Œì´ë¸”
        cursor.execute("""
        CREATE TABLE daily_analysis (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            employee_id TEXT NOT NULL,
            employee_name TEXT,
            analysis_date DATE NOT NULL,
            
            -- ì¡°ì§ ì •ë³´
            center_id TEXT,
            center_name TEXT,
            team_id TEXT,
            team_name TEXT,
            group_id TEXT,
            group_name TEXT,
            job_grade TEXT,
            
            -- ê·¼ë¬´ ì‹œê°„ ë¶„ì„
            total_hours REAL,
            work_hours REAL,
            focused_work_hours REAL,
            meeting_hours REAL,
            break_hours REAL,
            meal_hours REAL,
            movement_hours REAL,
            idle_hours REAL,
            
            -- íš¨ìœ¨ì„± ì§€í‘œ
            efficiency_ratio REAL,
            focus_ratio REAL,
            productivity_score REAL,
            
            -- ì‹ì‚¬ ë¶„ì„
            breakfast_taken INTEGER DEFAULT 0,
            lunch_taken INTEGER DEFAULT 0,
            dinner_taken INTEGER DEFAULT 0,
            midnight_meal_taken INTEGER DEFAULT 0,
            
            -- íŒ¨í„´ ë¶„ì„ (JSON)
            peak_hours TEXT,
            activity_distribution TEXT,
            location_patterns TEXT,
            hourly_efficiency TEXT,
            
            -- Claim ë¹„êµ
            claim_hours REAL,
            claim_vs_actual_diff REAL,
            
            -- ë©”íƒ€ ì •ë³´
            analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            analysis_version TEXT DEFAULT '1.0.0-mock',
            processing_time_ms INTEGER,
            
            UNIQUE(employee_id, analysis_date)
        )""")
        
        # team_daily_summary í…Œì´ë¸”
        cursor.execute("""
        CREATE TABLE team_daily_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            team_id TEXT NOT NULL,
            team_name TEXT,
            center_id TEXT,
            center_name TEXT,
            analysis_date DATE NOT NULL,
            
            total_employees INTEGER,
            analyzed_employees INTEGER,
            avg_efficiency_ratio REAL,
            avg_work_hours REAL,
            avg_focus_ratio REAL,
            avg_productivity_score REAL,
            
            grade_distribution TEXT,
            efficiency_by_grade TEXT,
            hourly_patterns TEXT,
            
            analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            UNIQUE(team_id, analysis_date)
        )""")
        
        # center_daily_summary í…Œì´ë¸”
        cursor.execute("""
        CREATE TABLE center_daily_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            center_id TEXT NOT NULL,
            center_name TEXT,
            analysis_date DATE NOT NULL,
            
            total_employees INTEGER,
            analyzed_employees INTEGER,
            total_teams INTEGER,
            avg_efficiency_ratio REAL,
            avg_work_hours REAL,
            avg_focus_ratio REAL,
            
            team_performance TEXT,
            grade_distribution TEXT,
            
            analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            UNIQUE(center_id, analysis_date)
        )""")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX idx_mock_daily_employee_date ON daily_analysis(employee_id, analysis_date)")
        cursor.execute("CREATE INDEX idx_mock_daily_date ON daily_analysis(analysis_date)")
        cursor.execute("CREATE INDEX idx_mock_daily_center ON daily_analysis(center_id, analysis_date)")
        cursor.execute("CREATE INDEX idx_mock_daily_team ON daily_analysis(team_id, analysis_date)")
        
        conn.commit()
        conn.close()
        
        logger.info("Mock DB ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_claim_based_targets(self) -> pd.DataFrame:
        """Claim ë°ì´í„° ê¸°ë°˜ ëŒ€ìƒ ì¶”ì¶œ"""
        conn = sqlite3.connect(self.source_db)
        
        # Claim ë°ì´í„°ì™€ ì§ì› ì •ë³´ ì¡°ì¸
        query = """
        SELECT DISTINCT
            ec.employee_id,
            e.employee_name,
            ec.work_date,
            ec.total_hours as claim_hours,
            e.center_id,
            e.center_name,
            e.team_id,
            e.team_name,
            e.group_id,
            e.group_name,
            e.job_grade
        FROM employee_claims ec
        INNER JOIN employees e ON ec.employee_id = e.employee_id
        WHERE ec.total_hours > 0
        ORDER BY ec.work_date, ec.employee_id
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        logger.info(f"Claim ê¸°ë°˜ ëŒ€ìƒ ì¶”ì¶œ: {len(df)}ê±´")
        return df
    
    def generate_mock_metrics(self, row: pd.Series) -> dict:
        """Mock ë©”íŠ¸ë¦­ ìƒì„± (í˜„ì‹¤ì ì¸ íŒ¨í„´)"""
        
        # ì§ê¸‰ë³„ íš¨ìœ¨ì„± ê¸°ë³¸ê°’
        grade = row.get('job_grade', 'Lv.2')
        if grade not in self.efficiency_dist:
            grade = 'Lv.2'
        
        # íš¨ìœ¨ì„± ìƒì„± (ì •ê·œë¶„í¬)
        base_efficiency = np.random.normal(
            self.efficiency_dist[grade]['mean'],
            self.efficiency_dist[grade]['std']
        )
        
        # ìš”ì¼ë³„ ë³€ë™ (ì›”ìš”ì¼ ë‚®ìŒ, ìˆ˜ìš”ì¼ ë†’ìŒ)
        work_date = pd.to_datetime(row['work_date'])
        weekday = work_date.weekday()
        weekday_adjustment = [0.95, 1.0, 1.05, 1.02, 0.98, 0.9, 0.85][weekday]
        
        efficiency_ratio = np.clip(base_efficiency * weekday_adjustment, 60, 100)
        
        # Claim ì‹œê°„ ê¸°ë°˜ ì‹¤ì œ ì‹œê°„ ê³„ì‚°
        claim_hours = float(row.get('claim_hours', 8))
        
        # ì‹¤ì œ ì‹œê°„ì€ Claimê³¼ ì•½ê°„ì˜ ì°¨ì´ (Â±2ì‹œê°„)
        actual_variation = np.random.normal(0, 0.5)
        total_hours = np.clip(claim_hours + actual_variation, 4, 14)
        
        # ê·¼ë¬´ ì‹œê°„ ë¹„ìœ¨
        work_ratio = efficiency_ratio / 100
        work_hours = total_hours * work_ratio
        
        # ì„¸ë¶€ ì‹œê°„ ë¶„ë°°
        focused_work_hours = work_hours * np.random.uniform(0.5, 0.7)
        meeting_hours = work_hours * np.random.uniform(0.1, 0.3)
        
        # ì‹ì‚¬ ë° íœ´ì‹
        meal_hours = np.random.uniform(1.5, 2.5)
        break_hours = np.random.uniform(0.5, 1.5)
        movement_hours = np.random.uniform(0.3, 0.8)
        idle_hours = max(0, total_hours - work_hours - meal_hours - break_hours - movement_hours)
        
        # ì§‘ì¤‘ë„ ë° ìƒì‚°ì„±
        focus_ratio = (focused_work_hours / work_hours * 100) if work_hours > 0 else 0
        productivity_score = efficiency_ratio * (focus_ratio / 100) * np.random.uniform(0.9, 1.1)
        
        # ì‹ì‚¬ íŒ¨í„´ (í˜„ì‹¤ì )
        breakfast_taken = 1 if np.random.random() < 0.3 else 0  # 30% í™•ë¥ 
        lunch_taken = 1 if np.random.random() < 0.95 else 0     # 95% í™•ë¥ 
        dinner_taken = 1 if total_hours > 10 and np.random.random() < 0.7 else 0
        midnight_meal_taken = 1 if row.get('team_name', '').startswith('P2') and np.random.random() < 0.2 else 0
        
        # í”¼í¬ ì‹œê°„ëŒ€ (Mock)
        peak_hours = ["09:00-11:00", "14:00-16:00"] if weekday < 5 else ["10:00-12:00"]
        
        # í™œë™ ë¶„í¬ (Mock)
        activity_distribution = {
            "WORK": round(work_hours, 1),
            "FOCUSED_WORK": round(focused_work_hours, 1),
            "MEETING": round(meeting_hours, 1),
            "BREAK": round(break_hours, 1),
            "MEAL": round(meal_hours, 1),
            "MOVEMENT": round(movement_hours, 1),
            "IDLE": round(idle_hours, 1)
        }
        
        # ìœ„ì¹˜ íŒ¨í„´ (Mock)
        location_patterns = {
            "OFFICE": round(work_hours * 0.7, 1),
            "MEETING_ROOM": round(meeting_hours, 1),
            "CAFETERIA": round(meal_hours, 1),
            "OTHER": round(movement_hours, 1)
        }
        
        # ì‹œê°„ëŒ€ë³„ íš¨ìœ¨ì„± (Mock)
        hourly_efficiency = {}
        for hour in range(8, 20):
            if hour < 12:
                hourly_efficiency[f"{hour:02d}:00"] = round(efficiency_ratio * np.random.uniform(0.8, 1.1), 1)
            else:
                hourly_efficiency[f"{hour:02d}:00"] = round(efficiency_ratio * np.random.uniform(0.7, 1.0), 1)
        
        return {
            'total_hours': round(total_hours, 2),
            'work_hours': round(work_hours, 2),
            'focused_work_hours': round(focused_work_hours, 2),
            'meeting_hours': round(meeting_hours, 2),
            'break_hours': round(break_hours, 2),
            'meal_hours': round(meal_hours, 2),
            'movement_hours': round(movement_hours, 2),
            'idle_hours': round(idle_hours, 2),
            'efficiency_ratio': round(efficiency_ratio, 1),
            'focus_ratio': round(focus_ratio, 1),
            'productivity_score': round(productivity_score, 1),
            'breakfast_taken': breakfast_taken,
            'lunch_taken': lunch_taken,
            'dinner_taken': dinner_taken,
            'midnight_meal_taken': midnight_meal_taken,
            'peak_hours': json.dumps(peak_hours),
            'activity_distribution': json.dumps(activity_distribution),
            'location_patterns': json.dumps(location_patterns),
            'hourly_efficiency': json.dumps(hourly_efficiency),
            'claim_vs_actual_diff': round(total_hours - claim_hours, 2),
            'processing_time_ms': np.random.randint(800, 1200)
        }
    
    def generate_mock_data(self, limit: int = None, sample_rate: float = 1.0):
        """Mock ë°ì´í„° ìƒì„± ë° ì €ì¥"""
        
        # Claim ê¸°ë°˜ ëŒ€ìƒ ì¶”ì¶œ
        targets = self.get_claim_based_targets()
        
        # ìƒ˜í”Œë§ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
        if sample_rate < 1.0:
            targets = targets.sample(frac=sample_rate)
        
        if limit:
            targets = targets.head(limit)
        
        logger.info(f"Mock ë°ì´í„° ìƒì„± ì‹œì‘: {len(targets)}ê±´")
        
        # DB ì—°ê²°
        conn = sqlite3.connect(self.target_db)
        cursor = conn.cursor()
        
        # ì§„í–‰ë¥  ì¶”ì 
        total = len(targets)
        batch_size = 1000
        
        for i in range(0, total, batch_size):
            batch = targets.iloc[i:i+batch_size]
            records = []
            
            for _, row in batch.iterrows():
                # Mock ë©”íŠ¸ë¦­ ìƒì„±
                metrics = self.generate_mock_metrics(row)
                
                # ë ˆì½”ë“œ ìƒì„±
                record = {
                    'employee_id': row['employee_id'],
                    'employee_name': row['employee_name'],
                    'analysis_date': row['work_date'],
                    'center_id': row.get('center_id'),
                    'center_name': row.get('center_name'),
                    'team_id': row.get('team_id'),
                    'team_name': row.get('team_name'),
                    'group_id': row.get('group_id'),
                    'group_name': row.get('group_name'),
                    'job_grade': row.get('job_grade'),
                    'claim_hours': float(row.get('claim_hours', 8)),
                    **metrics
                }
                records.append(record)
            
            # ë°°ì¹˜ ì €ì¥
            for record in records:
                cursor.execute("""
                INSERT OR REPLACE INTO daily_analysis (
                    employee_id, employee_name, analysis_date,
                    center_id, center_name, team_id, team_name,
                    group_id, group_name, job_grade,
                    total_hours, work_hours, focused_work_hours,
                    meeting_hours, break_hours, meal_hours,
                    movement_hours, idle_hours,
                    efficiency_ratio, focus_ratio, productivity_score,
                    breakfast_taken, lunch_taken, dinner_taken, midnight_meal_taken,
                    peak_hours, activity_distribution, location_patterns, hourly_efficiency,
                    claim_hours, claim_vs_actual_diff, processing_time_ms
                ) VALUES (
                    :employee_id, :employee_name, :analysis_date,
                    :center_id, :center_name, :team_id, :team_name,
                    :group_id, :group_name, :job_grade,
                    :total_hours, :work_hours, :focused_work_hours,
                    :meeting_hours, :break_hours, :meal_hours,
                    :movement_hours, :idle_hours,
                    :efficiency_ratio, :focus_ratio, :productivity_score,
                    :breakfast_taken, :lunch_taken, :dinner_taken, :midnight_meal_taken,
                    :peak_hours, :activity_distribution, :location_patterns, :hourly_efficiency,
                    :claim_hours, :claim_vs_actual_diff, :processing_time_ms
                )
                """, record)
            
            conn.commit()
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress = min(i + batch_size, total)
            pct = progress * 100 / total
            logger.info(f"ì§„í–‰: {progress}/{total} ({pct:.1f}%)")
        
        conn.close()
        logger.info("Mock ë°ì´í„° ìƒì„± ì™„ë£Œ")
    
    def generate_aggregations(self):
        """ì§‘ê³„ í…Œì´ë¸” ìƒì„±"""
        logger.info("ì§‘ê³„ ë°ì´í„° ìƒì„± ì‹œì‘...")
        
        conn = sqlite3.connect(self.target_db)
        cursor = conn.cursor()
        
        # íŒ€ë³„ ì§‘ê³„
        cursor.execute("""
        INSERT OR REPLACE INTO team_daily_summary (
            team_id, team_name, center_id, center_name, analysis_date,
            total_employees, analyzed_employees,
            avg_efficiency_ratio, avg_work_hours, avg_focus_ratio, avg_productivity_score
        )
        SELECT 
            team_id, 
            MAX(team_name) as team_name,
            MAX(center_id) as center_id,
            MAX(center_name) as center_name,
            analysis_date,
            COUNT(DISTINCT employee_id) as total_employees,
            COUNT(DISTINCT CASE WHEN total_hours > 0 THEN employee_id END) as analyzed_employees,
            AVG(efficiency_ratio) as avg_efficiency_ratio,
            AVG(work_hours) as avg_work_hours,
            AVG(focus_ratio) as avg_focus_ratio,
            AVG(productivity_score) as avg_productivity_score
        FROM daily_analysis
        WHERE team_id IS NOT NULL
        GROUP BY team_id, analysis_date
        """)
        
        # ì§ê¸‰ë³„ ë¶„í¬ ì¶”ê°€
        teams = cursor.execute("""
            SELECT DISTINCT team_id, analysis_date 
            FROM team_daily_summary
        """).fetchall()
        
        for team_id, analysis_date in teams:
            grade_dist = cursor.execute("""
                SELECT job_grade, COUNT(*) as count
                FROM daily_analysis
                WHERE team_id = ? AND analysis_date = ?
                GROUP BY job_grade
            """, (team_id, analysis_date)).fetchall()
            
            grade_dict = {grade: count for grade, count in grade_dist}
            
            cursor.execute("""
                UPDATE team_daily_summary
                SET grade_distribution = ?
                WHERE team_id = ? AND analysis_date = ?
            """, (json.dumps(grade_dict), team_id, analysis_date))
        
        # ì„¼í„°ë³„ ì§‘ê³„
        cursor.execute("""
        INSERT OR REPLACE INTO center_daily_summary (
            center_id, center_name, analysis_date,
            total_employees, analyzed_employees, total_teams,
            avg_efficiency_ratio, avg_work_hours, avg_focus_ratio
        )
        SELECT 
            center_id,
            MAX(center_name) as center_name,
            analysis_date,
            COUNT(DISTINCT employee_id) as total_employees,
            COUNT(DISTINCT CASE WHEN total_hours > 0 THEN employee_id END) as analyzed_employees,
            COUNT(DISTINCT team_id) as total_teams,
            AVG(efficiency_ratio) as avg_efficiency_ratio,
            AVG(work_hours) as avg_work_hours,
            AVG(focus_ratio) as avg_focus_ratio
        FROM daily_analysis  
        WHERE center_id IS NOT NULL
        GROUP BY center_id, analysis_date
        """)
        
        conn.commit()
        conn.close()
        
        logger.info("ì§‘ê³„ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    
    def print_summary(self):
        """ìƒì„±ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥"""
        conn = sqlite3.connect(self.target_db)
        
        # ì „ì²´ í†µê³„
        stats = pd.read_sql_query("""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT employee_id) as unique_employees,
                COUNT(DISTINCT analysis_date) as unique_dates,
                MIN(analysis_date) as first_date,
                MAX(analysis_date) as last_date,
                AVG(efficiency_ratio) as avg_efficiency,
                AVG(claim_vs_actual_diff) as avg_diff
            FROM daily_analysis
        """, conn)
        
        print("\n" + "="*60)
        print("ğŸ“Š Mock ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print("="*60)
        
        if not stats.empty:
            row = stats.iloc[0]
            print(f"ğŸ“Œ ì´ ë ˆì½”ë“œ: {row['total_records']:,}ê°œ")
            print(f"ğŸ‘¥ ì§ì› ìˆ˜: {row['unique_employees']:,}ëª…")
            print(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {row['first_date']} ~ {row['last_date']} ({row['unique_dates']}ì¼)")
            print(f"ğŸ“ˆ í‰ê·  íš¨ìœ¨ì„±: {row['avg_efficiency']:.1f}%")
            print(f"â±ï¸  Claim vs ì‹¤ì œ ì°¨ì´: {row['avg_diff']:.2f}ì‹œê°„")
        
        # ì¡°ì§ë³„ ìš”ì•½
        org_stats = pd.read_sql_query("""
            SELECT 
                center_name,
                COUNT(DISTINCT team_id) as teams,
                COUNT(DISTINCT employee_id) as employees,
                AVG(efficiency_ratio) as avg_efficiency
            FROM daily_analysis
            WHERE center_name IS NOT NULL
            GROUP BY center_name
            ORDER BY employees DESC
            LIMIT 5
        """, conn)
        
        if not org_stats.empty:
            print("\nğŸ“Š ìƒìœ„ 5ê°œ ì„¼í„°:")
            for _, row in org_stats.iterrows():
                print(f"  â€¢ {row['center_name']}: {row['employees']}ëª…, "
                      f"{row['teams']}íŒ€, íš¨ìœ¨ì„± {row['avg_efficiency']:.1f}%")
        
        conn.close()
        
        print("\nâœ… Mock DB ê²½ë¡œ:")
        print(f"   {self.target_db}")
        print("\nğŸš€ UI ê°œë°œ ì‹œì‘ ê°€ëŠ¥!")
        print("   FastAPI ë°±ì—”ë“œë¥¼ Mock DBì™€ ì—°ê²°í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Mock ë¶„ì„ ë°ì´í„° ìƒì„±')
    parser.add_argument('--limit', type=int, help='ìƒì„±í•  ë ˆì½”ë“œ ìˆ˜ ì œí•œ')
    parser.add_argument('--sample-rate', type=float, default=1.0,
                       help='ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = 10%)')
    parser.add_argument('--quick', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ìµœê·¼ 7ì¼, 10% ìƒ˜í”Œ)')
    
    args = parser.parse_args()
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = MockAnalyticsGenerator()
    
    # Mock DB ì´ˆê¸°í™”
    generator.init_mock_db()
    
    # ì˜µì…˜ ì„¤ì •
    if args.quick:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        generator.generate_mock_data(limit=5000, sample_rate=0.1)
    else:
        # ì „ì²´ ìƒì„±
        generator.generate_mock_data(limit=args.limit, sample_rate=args.sample_rate)
    
    # ì§‘ê³„ ìƒì„±
    generator.generate_aggregations()
    
    # ìš”ì•½ ì¶œë ¥
    generator.print_summary()


if __name__ == "__main__":
    main()