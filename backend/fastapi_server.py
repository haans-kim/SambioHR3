#!/usr/bin/env python3
"""
FastAPI Î∞±ÏóîÎìú ÏÑúÎ≤Ñ
Mock ÎòêÎäî Ïã§Ï†ú Î∂ÑÏÑù DBÏôÄ Ïó∞ÎèôÌïòÏó¨ React UIÏóê Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ
"""

from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any
import sqlite3
import pandas as pd
import json
from pathlib import Path
import logging

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI Ïï± Ï¥àÍ∏∞Ìôî
app = FastAPI(
    title="Sambio Analytics API",
    description="Ï°∞ÏßÅ Î∂ÑÏÑù ÎåÄÏãúÎ≥¥Îìú API",
    version="1.0.0"
)

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],  # React Í∞úÎ∞ú ÏÑúÎ≤Ñ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# DB Í≤ΩÎ°ú ÏÑ§Ï†ï
project_root = Path(__file__).parent.parent
# Mock DBÎ•º Í∏∞Î≥∏ÏúºÎ°ú ÏÇ¨Ïö© (Ïã§Ï†ú Î∂ÑÏÑù ÏôÑÎ£å ÌõÑ Î≥ÄÍ≤Ω)
DB_PATH = project_root / 'data' / 'sambio_analytics_mock.db'
# DB_PATH = project_root / 'data' / 'sambio_analytics.db'  # Ïã§Ï†ú Î∂ÑÏÑù DB


def get_db_connection():
    """DB Ïó∞Í≤∞ ÏÉùÏÑ±"""
    if not DB_PATH.exists():
        raise HTTPException(status_code=500, detail=f"Database not found: {DB_PATH}")
    return sqlite3.connect(str(DB_PATH))


@app.get("/")
async def root():
    """API ÏÉÅÌÉú ÌôïÏù∏"""
    return {
        "status": "running",
        "api": "Sambio Analytics API",
        "version": "1.0.0",
        "database": DB_PATH.name,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/centers")
async def get_centers(
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú (YYYY-MM-DD)")
):
    """ÏÑºÌÑ∞ Î™©Î°ù Î∞è ÏöîÏïΩ Ï†ïÎ≥¥"""
    try:
        conn = get_db_connection()
        query = """
            SELECT 
                center_id,
                center_name,
                total_employees,
                analyzed_employees,
                total_teams,
                avg_efficiency_ratio,
                avg_work_hours,
                avg_focus_ratio,
                team_performance,
                grade_distribution
            FROM center_daily_summary
            WHERE analysis_date = ?
            ORDER BY center_name
        """
        
        df = pd.read_sql_query(query, conn, params=(analysis_date.isoformat(),))
        conn.close()
        
        # JSON ÌïÑÎìú ÌååÏã±
        records = df.to_dict('records')
        for record in records:
            if record.get('team_performance'):
                try:
                    record['team_performance'] = json.loads(record['team_performance'])
                except:
                    record['team_performance'] = {}
            if record.get('grade_distribution'):
                try:
                    record['grade_distribution'] = json.loads(record['grade_distribution'])
                except:
                    record['grade_distribution'] = {}
        
        return {
            "date": analysis_date.isoformat(),
            "total_centers": len(records),
            "centers": records
        }
        
    except Exception as e:
        logger.error(f"Error fetching centers: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/teams/{center_id}")
async def get_teams_by_center(
    center_id: str,
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú")
):
    """ÌäπÏ†ï ÏÑºÌÑ∞Ïùò ÌåÄ Î™©Î°ù"""
    try:
        conn = get_db_connection()
        query = """
            SELECT 
                team_id,
                team_name,
                center_id,
                center_name,
                total_employees,
                analyzed_employees,
                avg_efficiency_ratio,
                avg_work_hours,
                avg_focus_ratio,
                avg_productivity_score,
                grade_distribution,
                efficiency_by_grade
            FROM team_daily_summary
            WHERE center_id = ? AND analysis_date = ?
            ORDER BY team_name
        """
        
        df = pd.read_sql_query(query, conn, params=(center_id, analysis_date.isoformat()))
        conn.close()
        
        # JSON ÌïÑÎìú ÌååÏã±
        records = df.to_dict('records')
        for record in records:
            for field in ['grade_distribution', 'efficiency_by_grade']:
                if record.get(field):
                    try:
                        record[field] = json.loads(record[field])
                    except:
                        record[field] = {}
        
        return {
            "center_id": center_id,
            "date": analysis_date.isoformat(),
            "total_teams": len(records),
            "teams": records
        }
        
    except Exception as e:
        logger.error(f"Error fetching teams: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/groups/{team_id}")
async def get_groups_by_team(
    team_id: str,
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú")
):
    """ÌäπÏ†ï ÌåÄÏùò Í∑∏Î£πÎ≥Ñ ÏßëÍ≥Ñ"""
    try:
        conn = get_db_connection()
        query = """
            SELECT 
                group_id,
                group_name,
                COUNT(DISTINCT employee_id) as total_employees,
                AVG(efficiency_ratio) as avg_efficiency_ratio,
                AVG(work_hours) as avg_work_hours,
                AVG(focus_ratio) as avg_focus_ratio,
                AVG(productivity_score) as avg_productivity_score
            FROM daily_analysis
            WHERE team_id = ? AND analysis_date = ?
                AND group_id IS NOT NULL
            GROUP BY group_id, group_name
            ORDER BY group_name
        """
        
        df = pd.read_sql_query(query, conn, params=(team_id, analysis_date.isoformat()))
        conn.close()
        
        records = df.to_dict('records')
        
        return {
            "team_id": team_id,
            "date": analysis_date.isoformat(),
            "total_groups": len(records),
            "groups": records
        }
        
    except Exception as e:
        logger.error(f"Error fetching groups: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/employees/{team_id}")
async def get_employees_by_team(
    team_id: str,
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú"),
    group_id: Optional[str] = Query(None, description="Í∑∏Î£π ID (ÏÑ†ÌÉù)")
):
    """ÌäπÏ†ï ÌåÄ/Í∑∏Î£πÏùò ÏßÅÏõê ÏÉÅÏÑ∏"""
    try:
        conn = get_db_connection()
        
        # Í∏∞Î≥∏ ÏøºÎ¶¨
        query = """
            SELECT 
                employee_id,
                employee_name,
                job_grade,
                group_id,
                group_name,
                total_hours,
                work_hours,
                focused_work_hours,
                meeting_hours,
                break_hours,
                meal_hours,
                efficiency_ratio,
                focus_ratio,
                productivity_score,
                breakfast_taken,
                lunch_taken,
                dinner_taken,
                midnight_meal_taken,
                peak_hours,
                activity_distribution,
                claim_hours,
                claim_vs_actual_diff
            FROM daily_analysis
            WHERE team_id = ? AND analysis_date = ?
        """
        
        params = [team_id, analysis_date.isoformat()]
        
        # Í∑∏Î£π ÌïÑÌÑ∞ Ï∂îÍ∞Ä
        if group_id:
            query += " AND group_id = ?"
            params.append(group_id)
        
        query += " ORDER BY efficiency_ratio DESC"
        
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        
        # JSON ÌïÑÎìú ÌååÏã±
        records = df.to_dict('records')
        for record in records:
            for field in ['peak_hours', 'activity_distribution']:
                if record.get(field):
                    try:
                        record[field] = json.loads(record[field])
                    except:
                        record[field] = []
        
        return {
            "team_id": team_id,
            "group_id": group_id,
            "date": analysis_date.isoformat(),
            "total_employees": len(records),
            "employees": records
        }
        
    except Exception as e:
        logger.error(f"Error fetching employees: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/employee/{employee_id}")
async def get_employee_detail(
    employee_id: str,
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú")
):
    """ÌäπÏ†ï ÏßÅÏõê ÏÉÅÏÑ∏ Ï†ïÎ≥¥"""
    try:
        conn = get_db_connection()
        query = """
            SELECT *
            FROM daily_analysis
            WHERE employee_id = ? AND analysis_date = ?
        """
        
        df = pd.read_sql_query(query, conn, params=(employee_id, analysis_date.isoformat()))
        conn.close()
        
        if df.empty:
            raise HTTPException(status_code=404, detail="Employee data not found")
        
        record = df.to_dict('records')[0]
        
        # JSON ÌïÑÎìú ÌååÏã±
        for field in ['peak_hours', 'activity_distribution', 'location_patterns', 'hourly_efficiency']:
            if record.get(field):
                try:
                    record[field] = json.loads(record[field])
                except:
                    record[field] = {}
        
        return record
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching employee detail: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/employee/{employee_id}/trend")
async def get_employee_trend(
    employee_id: str,
    days: int = Query(default=30, description="Ï°∞Ìöå Í∏∞Í∞Ñ (Ïùº)")
):
    """ÏßÅÏõê Ï∂îÏÑ∏ Îç∞Ïù¥ÌÑ∞"""
    try:
        conn = get_db_connection()
        query = """
            SELECT 
                analysis_date,
                efficiency_ratio,
                work_hours,
                focus_ratio,
                productivity_score,
                claim_hours,
                claim_vs_actual_diff
            FROM daily_analysis
            WHERE employee_id = ?
            ORDER BY analysis_date DESC
            LIMIT ?
        """
        
        df = pd.read_sql_query(query, conn, params=(employee_id, days))
        conn.close()
        
        # ÎÇ†ÏßúÏàú Ï†ïÎ†¨ (Ïò§ÎûòÎêú Í≤ÉÎ∂ÄÌÑ∞)
        df = df.sort_values('analysis_date')
        records = df.to_dict('records')
        
        return {
            "employee_id": employee_id,
            "days": days,
            "trend_data": records
        }
        
    except Exception as e:
        logger.error(f"Error fetching employee trend: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/summary")
async def get_analytics_summary(
    analysis_date: date = Query(..., description="Î∂ÑÏÑù ÎÇ†Ïßú")
):
    """Ï†ÑÏ≤¥ ÏöîÏïΩ ÌÜµÍ≥Ñ"""
    try:
        conn = get_db_connection()
        
        # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ
        total_query = """
            SELECT 
                COUNT(DISTINCT employee_id) as total_employees,
                COUNT(DISTINCT center_id) as total_centers,
                COUNT(DISTINCT team_id) as total_teams,
                AVG(efficiency_ratio) as avg_efficiency,
                AVG(work_hours) as avg_work_hours,
                AVG(focus_ratio) as avg_focus_ratio,
                AVG(productivity_score) as avg_productivity
            FROM daily_analysis
            WHERE analysis_date = ?
        """
        
        total_df = pd.read_sql_query(total_query, conn, params=(analysis_date.isoformat(),))
        
        # ÏßÅÍ∏âÎ≥Ñ Î∂ÑÌè¨
        grade_query = """
            SELECT 
                job_grade,
                COUNT(*) as count,
                AVG(efficiency_ratio) as avg_efficiency,
                AVG(work_hours) as avg_work_hours
            FROM daily_analysis
            WHERE analysis_date = ?
                AND job_grade IS NOT NULL
            GROUP BY job_grade
            ORDER BY job_grade
        """
        
        grade_df = pd.read_sql_query(grade_query, conn, params=(analysis_date.isoformat(),))
        
        # ÏÉÅÏúÑ/ÌïòÏúÑ ÌåÄ
        team_ranking_query = """
            SELECT 
                team_name,
                avg_efficiency_ratio,
                analyzed_employees
            FROM team_daily_summary
            WHERE analysis_date = ?
            ORDER BY avg_efficiency_ratio DESC
        """
        
        team_df = pd.read_sql_query(team_ranking_query, conn, params=(analysis_date.isoformat(),))
        
        conn.close()
        
        return {
            "date": analysis_date.isoformat(),
            "summary": total_df.to_dict('records')[0] if not total_df.empty else {},
            "grade_distribution": grade_df.to_dict('records'),
            "top_teams": team_df.head(5).to_dict('records'),
            "bottom_teams": team_df.tail(5).to_dict('records')
        }
        
    except Exception as e:
        logger.error(f"Error fetching summary: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/dates/available")
async def get_available_dates():
    """Î∂ÑÏÑù Í∞ÄÎä•Ìïú ÎÇ†Ïßú Î™©Î°ù"""
    try:
        conn = get_db_connection()
        query = """
            SELECT DISTINCT analysis_date
            FROM daily_analysis
            ORDER BY analysis_date DESC
            LIMIT 30
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        dates = df['analysis_date'].tolist()
        
        return {
            "available_dates": dates,
            "latest_date": dates[0] if dates else None,
            "oldest_date": dates[-1] if dates else None,
            "total_dates": len(dates)
        }
        
    except Exception as e:
        logger.error(f"Error fetching available dates: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/status")
async def get_system_status():
    """ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏"""
    try:
        conn = get_db_connection()
        
        # DB ÌÜµÍ≥Ñ
        stats_query = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT employee_id) as total_employees,
                COUNT(DISTINCT analysis_date) as total_dates,
                MIN(analysis_date) as first_date,
                MAX(analysis_date) as last_date
            FROM daily_analysis
        """
        
        stats_df = pd.read_sql_query(stats_query, conn)
        conn.close()
        
        stats = stats_df.to_dict('records')[0] if not stats_df.empty else {}
        
        return {
            "status": "healthy",
            "database": {
                "path": str(DB_PATH),
                "size_mb": DB_PATH.stat().st_size / (1024 * 1024) if DB_PATH.exists() else 0,
                "is_mock": "mock" in DB_PATH.name
            },
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error checking status: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


if __name__ == "__main__":
    import uvicorn
    
    # ÏÑúÎ≤Ñ Ïã§Ìñâ
    print(f"üöÄ Starting FastAPI server...")
    print(f"üìÅ Using database: {DB_PATH}")
    print(f"üìç API docs: http://localhost:8000/docs")
    print(f"üåê API URL: http://localhost:8000")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True,  # Í∞úÎ∞ú Ï§ë ÏûêÎèô Î¶¨Î°úÎìú
        log_level="info"
    )