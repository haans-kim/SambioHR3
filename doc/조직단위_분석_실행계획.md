# 조직단위 분석 실행계획

## 📋 개요

5,000명 직원의 30일간 데이터를 분석하여 조직단위(센터/팀/그룹)별 통합 대시보드를 구축하는 프로젝트입니다. 대규모 데이터 처리를 위해 병렬 배치 분석 시스템을 구현하고, 분석 결과를 별도 DB에 저장하여 React 기반 UI에서 활용합니다.

### 🚀 병행 개발 전략
**핵심**: Claim 데이터 기반 Mock DB를 먼저 생성하여 분석과 UI 개발을 동시에 진행
- **Mock DB**: 실제 분석 DB와 동일한 스키마로 테스트 데이터 생성
- **병행 작업**: 분석 팀은 실제 데이터 처리, UI 팀은 Mock 데이터로 개발
- **통합 단계**: 분석 완료 후 DB 경로만 변경하여 즉시 연동

## 🎯 목표 및 요구사항

### 핵심 목표
- **분석 대상**: 5,000명 × 30일 = 150,000건의 개인 분석
- **완료 기한**: 3일 이내 (2025년 1월 10일 금요일 ~ 1월 12일 일요일)
- **성능 목표**: 병렬 처리로 전체 분석 4시간 이내 완료
- **품질 목표**: 모든 개인 분석 지표를 조직 단위로 집계

### 기술 요구사항
- Claim 데이터 기반 필터링으로 불필요한 분석 제거
- 별도 분석 DB (`sambio_analytics.db`) 구축
- React + TypeScript 기반 독립 UI 프로젝트
- 계층적 드릴다운 네비게이션 (센터 → 팀 → 그룹)

## 🏗️ 시스템 아키텍처

### 전체 구조
```
┌─────────────────────────────────────────────────────────┐
│                    원본 데이터 (sambio.db)               │
│                    - employees (조직 정보)               │
│                    - employee_claims (근무 신청)         │
│                    - daily_logs (실제 로그)              │
└─────────────────────────────────────────────────────────┘
                              ↓
                    [Batch Analysis System]
                    - Claim 필터링 (33% 감소)
                    - 8코어 병렬 처리
                    - 예상 시간: 3.5시간
                              ↓
┌─────────────────────────────────────────────────────────┐
│                분석 결과 DB (sambio_analytics.db)        │
│                    - daily_analysis (개인)               │
│                    - team_daily_summary (팀)             │
│                    - center_daily_summary (센터)         │
└─────────────────────────────────────────────────────────┘
                              ↓
                    [FastAPI Backend :8000]
                              ↓
                    [React Frontend :3000]
                    - 조직 계층 네비게이션
                    - 실시간 메트릭 카드
                    - 드릴다운 분석
```

## ⏱️ 시간 분석 및 최적화

### 처리 시간 예측

#### 기본 상황 (순차 처리)
- 전체 작업량: 5,000명 × 30일 = 150,000 분석
- 개당 처리 시간: 약 1초
- 총 소요 시간: **41.7시간** ❌

#### Claim 필터링 적용 (실제 데이터 기준)
- Excel 원본: 154,849건
- 근무시간 00:00 제외: 67,170건 (43.4%)
- **실제 처리 대상: 87,679건** (56.6%)
- 순차 처리 시간: **24.4시간** (41.5% 감소)

#### 병렬 처리 최적화 (M4 Max 기준)
- 최종 작업량: 87,679 분석
- 병렬 처리 (8 P-코어): 87,679초 ÷ 8 = 10,960초
- **최종 예상 시간: 약 3.0시간** ✅

### M4 Max 시스템 사양 및 병렬 처리 성능

#### 하드웨어 사양
- **프로세서**: Apple M4 Max
- **CPU 코어**: 16개 (Performance 8개 + Efficiency 8개)
- **메모리**: 16GB
- **아키텍처**: ARM64 (Rosetta 2로 x86_64 에뮬레이션)

#### Python 병렬 처리 지원
| 방식 | 지원 | 성능 | 권장도 |
|------|------|------|--------|
| **multiprocessing.Pool** | ✅ | 좋음 | ⭐⭐⭐ |
| **ProcessPoolExecutor** | ✅ | 좋음 | ⭐⭐⭐⭐⭐ |
| **ThreadPoolExecutor** | ✅ | I/O용 | ⭐⭐⭐ |
| **Pickle 파일 호환** | ✅ | 완벽 | ⭐⭐⭐⭐⭐ |

#### 예상 성능 비교
```
순차 처리 (1코어):  24.4시간
병렬 처리 (4코어):   6.1시간  
병렬 처리 (8코어):   3.0시간 ← 권장
병렬 처리 (16코어):  1.5시간 (E-코어 포함 시 불안정)
```

### 3일 실행 계획 (M4 Max 8코어 기준)

| 일차 | 작업 내용 | 예상 시간 | 데이터 범위 |
|------|-----------|-----------|-------------|
| **Day 1** (금) | 테스트 + 첫 주 | 1시간 | 최근 3일 + 7일 |
| **Day 2** (토) | 중간 2주 처리 | 1.5시간 | 14일 |
| **Day 3** (일) | 마지막 주 + 검증 | 0.5시간 | 9일 + 검증 |
| **총 소요 시간** | - | **3시간** | 30일 |

## 💾 데이터베이스 설계

### 분석 결과 DB 스키마 (`sambio_analytics.db`)

#### 1. daily_analysis (개인별 일일 분석)
```sql
CREATE TABLE daily_analysis (
    id INTEGER PRIMARY KEY,
    employee_id TEXT NOT NULL,
    employee_name TEXT,
    analysis_date DATE NOT NULL,
    
    -- 조직 정보
    center_id, center_name,
    team_id, team_name,
    group_id, group_name,
    job_grade TEXT,
    
    -- 시간 분석 (모든 개인 지표)
    total_hours, work_hours, focused_work_hours,
    meeting_hours, break_hours, meal_hours,
    movement_hours, idle_hours,
    
    -- 효율성 지표
    efficiency_ratio, focus_ratio, productivity_score,
    
    -- 식사 분석
    breakfast_taken, lunch_taken, 
    dinner_taken, midnight_meal_taken,
    
    -- 패턴 분석 (JSON)
    peak_hours, activity_distribution,
    location_patterns, hourly_efficiency,
    
    -- Claim 비교
    claim_hours, claim_vs_actual_diff,
    
    UNIQUE(employee_id, analysis_date)
);
```

#### 2. team_daily_summary (팀별 집계)
```sql
CREATE TABLE team_daily_summary (
    team_id, team_name, center_id, center_name,
    analysis_date,
    total_employees, analyzed_employees,
    avg_efficiency_ratio, avg_work_hours,
    avg_focus_ratio, avg_productivity_score,
    grade_distribution TEXT,  -- JSON
    efficiency_by_grade TEXT, -- JSON
    UNIQUE(team_id, analysis_date)
);
```

#### 3. center_daily_summary (센터별 집계)
```sql
CREATE TABLE center_daily_summary (
    center_id, center_name, analysis_date,
    total_employees, analyzed_employees, total_teams,
    avg_efficiency_ratio, avg_work_hours, avg_focus_ratio,
    team_performance TEXT,    -- JSON
    grade_distribution TEXT,   -- JSON
    UNIQUE(center_id, analysis_date)
);
```

## 🚀 실행 방법

### UI 개발 환경 설정 (즉시 시작 가능)
```bash
# Mock 데이터 생성 및 서버 실행
./scripts/setup_ui_development.sh

# API 문서 확인
open http://localhost:8000/docs

# React 프로젝트 실행 (별도 터미널)
./scripts/setup_ui_development.sh react
```

### 배치 분석 실행

#### M4 Max 최적화 설정
```python
# batch_analysis.py에 이미 포함된 설정
multiprocessing.set_start_method('spawn', force=True)  # macOS 호환
num_workers = 8  # P-코어만 사용 (E-코어 제외)
```

#### 실행 준비
```bash
# 1. 가상환경 활성화
source venv/bin/activate

# 2. 실행 권한 부여
chmod +x scripts/run_batch_analysis.sh
chmod +x scripts/setup_ui_development.sh

# 3. 상태 확인
python scripts/check_analysis_status.py
```

### Day 1 실행 (금요일)
```bash
# 방법 1: Shell Script
./scripts/run_batch_analysis.sh day1

# 방법 2: Makefile
make -f scripts/batch_makefile.mk day1

# 방법 3: 직접 실행 (테스트)
python scripts/batch_analysis.py \
    --start-date 2025-01-04 \
    --end-date 2025-01-07 \
    --workers 4
```

### Day 2 실행 (토요일)
```bash
./scripts/run_batch_analysis.sh day2
# 또는
make -f scripts/batch_makefile.mk day2
```

### Day 3 실행 (일요일)
```bash
./scripts/run_batch_analysis.sh day3
# 검증 자동 실행됨
```

### 전체 한 번에 실행 (옵션)
```bash
./scripts/run_batch_analysis.sh all
# 주의: 3-4시간 소요
```

## 📊 진행 상태 모니터링

### 실시간 상태 확인
```bash
# 진행 상태 확인
./scripts/run_batch_analysis.sh status

# 또는
python scripts/check_analysis_status.py
```

### 출력 예시
```
📊 전체 분석 통계
=====================================
📌 총 분석 레코드: 100,000개
👥 분석된 직원 수: 5,000명
📅 분석된 날짜 수: 20일
📆 기간: 2024-12-08 ~ 2025-01-07
⏱️ 평균 처리 시간: 950ms

📊 완료율: 67.3%
⏳ 남은 작업: 32,700개
⏱️ 예상 소요 시간: 1.1시간 (8코어 기준)
```

## 🔧 문제 해결

### 처리 중단 시 재시작
```bash
# 50000번째 작업부터 재시작
./scripts/run_batch_analysis.sh resume 50000

# 또는 Makefile 사용
make -f scripts/batch_makefile.mk resume-50000
```

### 데이터 검증
```bash
# 수동 검증 실행
python scripts/verify_batch_results.py
```

### DB 초기화 (처음부터 다시)
```bash
# 분석 DB 삭제
rm -f data/sambio_analytics.db

# 또는 Makefile 사용
make -f scripts/batch_makefile.mk clean
```

## 🎨 React UI 프로젝트

### Mock 데이터로 즉시 개발 시작
```bash
# 1. Mock DB 생성 (5분 소요)
python scripts/create_mock_analytics_db.py --quick

# 2. FastAPI 서버 실행
python backend/fastapi_server.py

# 3. API 테스트
curl http://localhost:8000/api/centers?analysis_date=2025-01-07
```

### 프로젝트 구조
```
sambio-analytics-ui/
├── src/
│   ├── api/              # API 통신
│   ├── components/
│   │   ├── organization/ # 조직 컴포넌트
│   │   │   ├── CenterGrid.tsx
│   │   │   ├── TeamGrid.tsx
│   │   │   └── GroupGrid.tsx
│   │   └── common/       # 공통 컴포넌트
│   ├── hooks/            # 커스텀 훅
│   ├── pages/            # 페이지 컴포넌트
│   └── store/            # Redux 상태 관리
└── package.json
```

### 백엔드 API (FastAPI)
```python
# 주요 엔드포인트
GET /api/centers?date=2025-01-07          # 센터 목록
GET /api/teams/{center_id}?date=2025-01-07  # 팀 목록
GET /api/employees/{team_id}?date=2025-01-07 # 직원 상세
GET /api/employee/{id}/trend?days=30      # 개인 추세
```

### UI 주요 기능
1. **계층적 네비게이션**: 센터 → 팀 → 그룹 드릴다운
2. **메트릭 카드**: 효율성, 근무시간, 인원수 표시
3. **색상 코딩**: 성과별 시각적 피드백 (녹색/파란색/빨간색)
4. **실시간 업데이트**: WebSocket 기반 실시간 데이터
5. **반응형 디자인**: 다양한 화면 크기 지원

## 📈 예상 결과

### 데이터 규모 (실제 측정값)
- **Claim 원본**: 154,849건
- **필터링 후**: 87,679건 (근무시간 > 0)
- **분석 레코드**: 87,679개
- **DB 크기**: 약 2-3GB
- **처리 시간**: **3.0시간** (M4 Max 8코어)
- **완료율**: 100% (Claim 데이터 기준)

### 성능 지표
- **개인 분석**: 1초/건
- **병렬 처리**: 8배 향상
- **API 응답**: <200ms
- **UI 렌더링**: <100ms

## ✅ 체크리스트

### 즉시 시작 가능 (UI 개발)
- [ ] Mock DB 생성 (`python scripts/create_mock_analytics_db.py`)
- [ ] FastAPI 서버 실행 (`python backend/fastapi_server.py`)
- [ ] API 문서 확인 (http://localhost:8000/docs)
- [ ] React 프로젝트 생성
- [ ] UI 컴포넌트 개발 시작

### Day 1 (금요일) - 분석
- [ ] 원본 데이터 백업
- [ ] 테스트 실행 (3일 데이터)
- [ ] 첫 주 데이터 처리 시작
- [ ] 진행 상태 확인
- [ ] 로그 모니터링

### Day 2 (토요일)
- [ ] Day 1 결과 확인
- [ ] 중간 2주 데이터 처리
- [ ] 중간 검증 실행
- [ ] 이슈 대응

### Day 3 (일요일)
- [ ] 마지막 주 데이터 처리
- [ ] 최종 검증 실행
- [ ] React 프로젝트 초기화
- [ ] FastAPI 백엔드 설정
- [ ] 통합 테스트

## 🔧 M4 Max 특화 최적화 팁

### ProcessPoolExecutor 사용 예시
```python
from concurrent.futures import ProcessPoolExecutor

def run_parallel_analysis(self, start_date, end_date):
    """M4 Max 최적화된 병렬 처리"""
    with ProcessPoolExecutor(max_workers=8) as executor:
        # P-코어만 사용하여 일관된 성능
        futures = {
            executor.submit(self.process_batch, batch): i 
            for i, batch in enumerate(batches)
        }
        for future in as_completed(futures):
            result = future.result()
            # 결과 처리
```

### 메모리 관리
- **spawn 방식**: 각 프로세스가 독립 메모리 사용 (약 200MB/프로세스)
- **총 메모리 사용**: 약 1.6GB (8 프로세스)
- **여유 메모리**: 14.4GB (시스템 전체 16GB 중)

### 성능 모니터링
```bash
# CPU 사용률 모니터링
top -o cpu

# 프로세스별 메모리 확인
ps aux | grep python | grep batch_analysis
```

## 📞 지원 및 참고

### 관련 문서
- `/doc/batch_analysis_plan.md` - 상세 기술 설계
- `/doc/react_ui_architecture.md` - UI 아키텍처
- `/scripts/batch_analysis.py` - 배치 처리 코드
- `/scripts/check_analysis_status.py` - 상태 확인
- `/scripts/test_parallel_m4.py` - M4 Max 병렬 처리 테스트

### 로그 파일
- `batch_analysis.log` - 처리 로그
- `data/sambio_analytics.db` - 분석 결과 DB
- `data/sambio_analytics_mock.db` - Mock 테스트 DB

### 예상 이슈 및 대응
1. **메모리 부족**: 배치 크기 조절 (`batch_size` 파라미터)
2. **DB 락**: 별도 DB 사용으로 해결됨
3. **처리 실패**: 체크포인트에서 재시작
4. **성능 저하**: 워커 수 조절 (`--workers` 파라미터, 최대 8 권장)
5. **E-코어 불균형**: P-코어만 사용 (`--workers 8`)

---

*문서 작성일: 2025년 1월 7일*  
*작성자: Sambio Human Analytics Team*