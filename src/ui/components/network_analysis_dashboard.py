"""
ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸
ì¡°ì§ ì „ì²´ì˜ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë° ì‹œê°í™”
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta, date
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import matplotlib.pyplot as plt
import networkx as nx
import sqlite3

from ...analysis.network_analyzer import NetworkAnalyzer
from ...database import DatabaseManager

class NetworkAnalysisDashboard:
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
        self.network_analyzer = NetworkAnalyzer("data/sambio_human.db")
        self._tag_data_cache = None
        self._date_range_cache = None
    
    def get_data_date_range(self):
        """ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ ë²”ìœ„ ë°˜í™˜"""
        if self._date_range_cache is not None:
            return self._date_range_cache
            
        try:
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            
            if tag_data is None or tag_data.empty:
                return None, None, pd.Series()
            
            # ë‚ ì§œ ë³€í™˜ ë° ë²”ìœ„ ê³„ì‚°
            tag_data['ENTE_DT'] = pd.to_numeric(tag_data['ENTE_DT'], errors='coerce')
            tag_data['time_str'] = tag_data['ì¶œì…ì‹œê°'].astype(str).str.zfill(6)
            tag_data['timestamp'] = pd.to_datetime(
                tag_data['ENTE_DT'].astype(str) + ' ' + tag_data['time_str'],
                format='%Y%m%d %H%M%S',
                errors='coerce'
            )
            
            dates = tag_data['timestamp'].dt.date
            min_date = dates.min()
            max_date = dates.max()
            date_counts = dates.value_counts().sort_index()
            
            self._date_range_cache = (min_date, max_date, date_counts)
            return min_date, max_date, date_counts
            
        except Exception as e:
            self.logger.error(f"ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None, pd.Series()
    
    def render(self):
        """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
        st.markdown("### ğŸŒ ì¡°ì§ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        
        # ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸
        min_date, max_date, date_counts = self.get_data_date_range()
        
        if min_date is None or max_date is None:
            st.error("ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ê¸°ê°„ ì •ë³´ í‘œì‹œ
        st.info(f"ğŸ“… ë°ì´í„° ê¸°ê°„: {min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')} (ì´ {len(date_counts)}ì¼)")
        
        # ë¶„ì„ ìœ í˜• ì„ íƒ
        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ì§ì› ê°„ ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬", "ê³µê°„ ì´ë™ ë„¤íŠ¸ì›Œí¬", "ì‹œê³„ì—´ ë™ì  ë„¤íŠ¸ì›Œí¬", "í™œë™ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬"]
        )
        
        # ê¸°ê°„ ì„ íƒ
        col1, col2 = st.columns(2)
        
        # ê¸°ë³¸ ë‚ ì§œ ì„¤ì • (ì „ì²´ ê¸°ê°„)
        default_end = max_date
        default_start = min_date
        
        with col1:
            start_date = st.date_input(
                "ì‹œì‘ ë‚ ì§œ",
                value=default_start,
                min_value=min_date,
                max_value=max_date,
                key="network_start_date",
                help=f"ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ: {min_date} ~ {max_date}"
            )
        with col2:
            end_date = st.date_input(
                "ì¢…ë£Œ ë‚ ì§œ",
                value=default_end,
                min_value=min_date,
                max_value=max_date,
                key="network_end_date",
                help=f"ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ: {min_date} ~ {max_date}"
            )
        
        # ë‚ ì§œ ìœ íš¨ì„± ê²€ì‚¬
        if start_date > end_date:
            st.error("ì‹œì‘ ë‚ ì§œëŠ” ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return
        
        # ì„ íƒí•œ ê¸°ê°„ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        selected_dates = pd.date_range(start_date, end_date).date
        data_exists = any(d in date_counts.index for d in selected_dates)
        
        if not data_exists:
            st.warning(f"ì„ íƒí•œ ê¸°ê°„({start_date} ~ {end_date})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            # ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ í‘œì‹œ
            with st.expander("ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ í™•ì¸"):
                # ì›”ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
                monthly_data = date_counts.groupby(pd.Grouper(freq='M')).sum()
                if not monthly_data.empty:
                    fig = px.bar(
                        x=monthly_data.index,
                        y=monthly_data.values,
                        labels={'x': 'ì›”', 'y': 'ë°ì´í„° ê°œìˆ˜'},
                        title='ì›”ë³„ ë°ì´í„° ë¶„í¬'
                    )
                    st.plotly_chart(fig, use_container_width=True)
            return
        
        if analysis_type == "ì§ì› ê°„ ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬":
            self.render_interaction_network(start_date, end_date)
        elif analysis_type == "ê³µê°„ ì´ë™ ë„¤íŠ¸ì›Œí¬":
            self.render_movement_network(start_date, end_date)
        elif analysis_type == "ì‹œê³„ì—´ ë™ì  ë„¤íŠ¸ì›Œí¬":
            self.render_temporal_network(start_date, end_date)
        elif analysis_type == "í™œë™ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬":
            self.render_activity_network(start_date, end_date)
    
    def render_interaction_network(self, start_date: date, end_date: date):
        """ì§ì› ê°„ ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        st.subheader("ğŸ‘¥ ì§ì› ê°„ ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬")
        
        # ë¶„ì„ ì˜µì…˜
        col1, col2, col3 = st.columns(3)
        with col1:
            interaction_threshold = st.slider(
                "ìµœì†Œ ìƒí˜¸ì‘ìš© ì‹œê°„ (ë¶„)",
                min_value=5,
                max_value=60,
                value=10,
                step=5
            )
        with col2:
            department_filter = st.selectbox(
                "ë¶€ì„œ í•„í„°",
                ["ì „ì²´"] + self.get_departments()
            )
        with col3:
            visualization_type = st.selectbox(
                "ì‹œê°í™” ìœ í˜•",
                ["Force-directed", "Circular", "Hierarchical"]
            )
        
        # ìƒí˜¸ì‘ìš© ë°ì´í„° ë¶„ì„
        interaction_data = self.analyze_interactions(
            start_date, end_date, interaction_threshold, department_filter
        )
        
        if interaction_data:
            # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ í‘œì‹œ
            self.display_network_metrics(interaction_data)
            
            # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
            self.visualize_interaction_network(interaction_data, visualization_type)
            
            # ì¤‘ì‹¬ì„± ë¶„ì„
            self.display_centrality_analysis(interaction_data)
            
            # ì»¤ë®¤ë‹ˆí‹° íƒì§€
            self.display_community_detection(interaction_data)
        else:
            st.info("ì„ íƒí•œ ê¸°ê°„ì— ë¶„ì„í•  ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def render_movement_network(self, start_date: date, end_date: date):
        """ê³µê°„ ì´ë™ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        st.subheader("ğŸ¢ ê³µê°„ ì´ë™ ë„¤íŠ¸ì›Œí¬")
        
        # ë¶„ì„ ì˜µì…˜
        col1, col2 = st.columns(2)
        with col1:
            analysis_level = st.selectbox(
                "ë¶„ì„ ìˆ˜ì¤€",
                ["ê°œì¸ë³„", "ë¶€ì„œë³„", "ì „ì²´"]
            )
        with col2:
            time_window = st.selectbox(
                "ì‹œê°„ëŒ€",
                ["ì „ì²´", "ì£¼ê°„(08:00-20:00)", "ì•¼ê°„(20:00-08:00)", 
                 "ì˜¤ì „(06:00-12:00)", "ì˜¤í›„(12:00-18:00)", "ì €ë…(18:00-24:00)"]
            )
        
        # ì´ë™ ë°ì´í„° ë¶„ì„
        movement_data = self.analyze_movement_patterns(
            start_date, end_date, analysis_level, time_window
        )
        
        if movement_data:
            # ì „ì²´ ì´ë™ í†µê³„
            self.display_movement_statistics(movement_data)
            
            # ê³µê°„ ì´ë™ ë§µ ì‹œê°í™”
            self.visualize_movement_map(movement_data)
            
            # ì´ë™ íŒ¨í„´ ë¶„ì„
            self.display_movement_patterns(movement_data)
            
            # ë³‘ëª© ì§€ì  ë¶„ì„
            self.display_bottleneck_analysis(movement_data)
        else:
            st.info("ì„ íƒí•œ ê¸°ê°„ì— ë¶„ì„í•  ì´ë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def render_temporal_network(self, start_date: date, end_date: date):
        """ì‹œê³„ì—´ ë™ì  ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        st.subheader("ğŸ“ˆ ì‹œê³„ì—´ ë™ì  ë„¤íŠ¸ì›Œí¬")
        
        # ë¶„ì„ ì˜µì…˜
        col1, col2, col3 = st.columns(3)
        with col1:
            time_granularity = st.selectbox(
                "ì‹œê°„ ë‹¨ìœ„",
                ["ì‹œê°„ë³„", "ì¼ë³„", "ì£¼ë³„"]
            )
        with col2:
            network_type = st.selectbox(
                "ë„¤íŠ¸ì›Œí¬ ìœ í˜•",
                ["ìƒí˜¸ì‘ìš©", "ì´ë™", "í˜‘ì—…"]
            )
        with col3:
            animation_speed = st.slider(
                "ì• ë‹ˆë©”ì´ì…˜ ì†ë„",
                min_value=100,
                max_value=2000,
                value=500,
                step=100
            )
        
        # ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ë¶„ì„
        temporal_data = self.analyze_temporal_network(
            start_date, end_date, time_granularity, network_type
        )
        
        if temporal_data and temporal_data.get('networks'):
            # ë„¤íŠ¸ì›Œí¬ ì§„í™” ë©”íŠ¸ë¦­
            self.display_network_evolution_metrics(temporal_data)
            
            # ì• ë‹ˆë©”ì´ì…˜ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
            self.visualize_animated_network(temporal_data, animation_speed)
            
            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
            self.display_temporal_patterns(temporal_data)
            
            # ì´ìƒ íŒ¨í„´ íƒì§€
            self.display_anomaly_detection(temporal_data)
        else:
            st.info("ì„ íƒí•œ ê¸°ê°„ì— ë¶„ì„í•  ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def render_activity_network(self, start_date: date, end_date: date):
        """í™œë™ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        st.subheader("âš¡ í™œë™ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬")
        
        # í™œë™ ìœ í˜• ì„ íƒ
        activity_types = st.multiselect(
            "ë¶„ì„í•  í™œë™ ìœ í˜•",
            ["ì—…ë¬´", "íšŒì˜", "ì‹ì‚¬", "íœ´ì‹", "ì´ë™"],
            default=["ì—…ë¬´", "íšŒì˜"]
        )
        
        # ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ë°©ë²•
        network_method = st.selectbox(
            "ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ë°©ë²•",
            ["ë™ì‹œ í™œë™", "ìˆœì°¨ í™œë™", "í™œë™ ì „í™˜"]
        )
        
        # í™œë™ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ë¶„ì„
        activity_data = self.analyze_activity_network(
            start_date, end_date, activity_types, network_method
        )
        
        if activity_data:
            # í™œë™ ë„¤íŠ¸ì›Œí¬ í†µê³„
            self.display_activity_statistics(activity_data)
            
            # í™œë™ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
            self.visualize_activity_network(activity_data)
            
            # í™œë™ í´ëŸ¬ìŠ¤í„° ë¶„ì„
            self.display_activity_clusters(activity_data)
            
            # í™œë™ íš¨ìœ¨ì„± ë¶„ì„
            self.display_activity_efficiency(activity_data)
        else:
            st.info("ì„ íƒí•œ ê¸°ê°„ì— ë¶„ì„í•  í™œë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def analyze_interactions(self, start_date: date, end_date: date, 
                           threshold: int, department: str) -> Optional[Dict]:
        """ì§ì› ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„"""
        try:
            # Pickle íŒŒì¼ì—ì„œ íƒœê·¸ ë°ì´í„° ë¡œë“œ
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            
            if tag_data is None or tag_data.empty:
                return None
            
            # ë‚ ì§œ í•„í„°ë§
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')
            
            # ENTE_DTë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            tag_data['ENTE_DT'] = pd.to_numeric(tag_data['ENTE_DT'], errors='coerce')
            filtered_data = tag_data[
                (tag_data['ENTE_DT'] >= int(start_str)) & 
                (tag_data['ENTE_DT'] <= int(end_str))
            ].copy()
            
            if filtered_data.empty:
                return None
            
            # ë¶€ì„œ í•„í„°ë§
            if department != "ì „ì²´":
                filtered_data = filtered_data[filtered_data['TEAM'] == department]
            
            # ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            # ì¶œì…ì‹œê°ì„ 6ìë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜ (HHMMSS í˜•ì‹)
            filtered_data['time'] = filtered_data['ì¶œì…ì‹œê°'].astype(str).str.zfill(6)
            filtered_data['timestamp'] = pd.to_datetime(
                filtered_data['ENTE_DT'].astype(str) + ' ' + filtered_data['time'],
                format='%Y%m%d %H%M%S',
                errors='coerce'
            )
            
            # ìƒí˜¸ì‘ìš© ì°¾ê¸° - ê°™ì€ ì‹œê°„(threshold ë¶„ ì´ë‚´), ê°™ì€ ì¥ì†Œì— ìˆëŠ” ì§ì›ë“¤
            interactions = []
            
            # ìœ„ì¹˜ë³„ë¡œ ê·¸ë£¹í™”
            for location, location_group in filtered_data.groupby('DR_NM'):
                # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
                location_group = location_group.sort_values('timestamp')
                
                # ê° ë ˆì½”ë“œì— ëŒ€í•´ threshold ì‹œê°„ ë‚´ì˜ ë‹¤ë¥¸ ì§ì› ì°¾ê¸°
                for i, row1 in location_group.iterrows():
                    for j, row2 in location_group.iterrows():
                        if i >= j:  # ì¤‘ë³µ ë°©ì§€
                            continue
                        
                        if row1['ì‚¬ë²ˆ'] == row2['ì‚¬ë²ˆ']:  # ê°™ì€ ì§ì› ì œì™¸
                            continue
                        
                        # ì‹œê°„ ì°¨ì´ ê³„ì‚°
                        time_diff = abs((row2['timestamp'] - row1['timestamp']).total_seconds() / 60)
                        
                        if time_diff <= threshold:
                            interactions.append({
                                'employee1': str(row1['ì‚¬ë²ˆ']),
                                'employee2': str(row2['ì‚¬ë²ˆ']),
                                'timestamp': row1['timestamp'],
                                'location': location,
                                'time_diff': time_diff
                            })
            
            if not interactions:
                return None
            
            df = pd.DataFrame(interactions)
            
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            
            # ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€
            for _, row in df.iterrows():
                G.add_edge(
                    row['employee1'], 
                    row['employee2'], 
                    weight=row['interaction_count']
                )
            
            # ì¤‘ì‹¬ì„± ê³„ì‚°
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            closeness_centrality = nx.closeness_centrality(G)
            
            return {
                'graph': G,
                'interactions': df,
                'degree_centrality': degree_centrality,
                'betweenness_centrality': betweenness_centrality,
                'closeness_centrality': closeness_centrality,
                'num_nodes': G.number_of_nodes(),
                'num_edges': G.number_of_edges(),
                'density': nx.density(G) if G.number_of_nodes() > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"ìƒí˜¸ì‘ìš© ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_movement_patterns(self, start_date: date, end_date: date,
                                level: str, time_window: str) -> Optional[Dict]:
        """ê³µê°„ ì´ë™ íŒ¨í„´ ë¶„ì„"""
        try:
            # Pickle íŒŒì¼ì—ì„œ íƒœê·¸ ë°ì´í„° ë¡œë“œ
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            
            if tag_data is None or tag_data.empty:
                return None
            
            # ë‚ ì§œ í•„í„°ë§
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')
            
            # ENTE_DTë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            tag_data['ENTE_DT'] = pd.to_numeric(tag_data['ENTE_DT'], errors='coerce')
            filtered_data = tag_data[
                (tag_data['ENTE_DT'] >= int(start_str)) & 
                (tag_data['ENTE_DT'] <= int(end_str))
            ].copy()
            
            if filtered_data.empty:
                return None
            
            # ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
            # ì¶œì…ì‹œê°ì„ 6ìë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜ (HHMMSS í˜•ì‹)
            filtered_data['time'] = filtered_data['ì¶œì…ì‹œê°'].astype(str).str.zfill(6)
            filtered_data['timestamp'] = pd.to_datetime(
                filtered_data['ENTE_DT'].astype(str) + ' ' + filtered_data['time'],
                format='%Y%m%d %H%M%S',
                errors='coerce'
            )
            
            # ì‹œê°„ëŒ€ í•„í„°ë§
            if time_window != "ì „ì²´":
                hour = filtered_data['timestamp'].dt.hour
                if time_window == "ì£¼ê°„(08:00-20:00)":
                    filtered_data = filtered_data[(hour >= 8) & (hour < 20)]
                elif time_window == "ì•¼ê°„(20:00-08:00)":
                    filtered_data = filtered_data[(hour >= 20) | (hour < 8)]
                elif time_window == "ì˜¤ì „(06:00-12:00)":
                    filtered_data = filtered_data[(hour >= 6) & (hour < 12)]
                elif time_window == "ì˜¤í›„(12:00-18:00)":
                    filtered_data = filtered_data[(hour >= 12) & (hour < 18)]
                elif time_window == "ì €ë…(18:00-24:00)":
                    filtered_data = filtered_data[(hour >= 18) & (hour < 24)]
            
            # ì§ì›ë³„ë¡œ ì •ë ¬
            df = filtered_data.sort_values(['ì‚¬ë²ˆ', 'timestamp'])
            
            # ì´ì „ ìœ„ì¹˜ ê³„ì‚°
            df['prev_location'] = df.groupby('ì‚¬ë²ˆ')['DR_NM'].shift(1)
            df['employee_id'] = df['ì‚¬ë²ˆ'].astype(str)
            df['location'] = df['DR_NM']
            
            # ì´ë™ ë¶„ì„
            movements = df[df['prev_location'].notna() & (df['location'] != df['prev_location'])].copy()
            
            # ë””ë²„ê¹… ì •ë³´
            with st.expander("ì´ë™ ë°ì´í„° ë¶„ì„", expanded=False):
                st.write(f"ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
                st.write(f"ì´ë™ ê°ì§€ ìˆ˜ (ìœ„ì¹˜ ë³€ê²½): {len(movements)}")
                if len(movements) > 0:
                    st.write("ì´ë™ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
                    st.dataframe(movements[['employee_id', 'prev_location', 'location']].head())
            
            # ê±´ë¬¼ ë§¤í•‘
            movements['from_building'] = movements['prev_location'].apply(
                self.network_analyzer.mapper.get_building_from_location
            )
            movements['to_building'] = movements['location'].apply(
                self.network_analyzer.mapper.get_building_from_location
            )
            
            # ê±´ë¬¼ ë§¤í•‘ í™•ì¸
            with st.expander("ê±´ë¬¼ ë§¤í•‘ ê²°ê³¼", expanded=False):
                st.write(f"ê±´ë¬¼ ë§¤í•‘ ì „: {len(movements)}")
                st.write(f"from_building null: {movements['from_building'].isna().sum()}")
                st.write(f"to_building null: {movements['to_building'].isna().sum()}")
                
                # ë§¤í•‘ ì‹¤íŒ¨ ìƒ˜í”Œ
                failed_mapping = movements[movements['from_building'].isna() | movements['to_building'].isna()]
                if len(failed_mapping) > 0:
                    st.write("ë§¤í•‘ ì‹¤íŒ¨ ìƒ˜í”Œ:")
                    st.dataframe(failed_mapping[['prev_location', 'location', 'from_building', 'to_building']].head(10))
            
            # ìœ íš¨í•œ ì´ë™ë§Œ í•„í„°ë§
            valid_movements = movements[
                movements['from_building'].notna() & 
                movements['to_building'].notna()
            ]
            
            st.write(f"ìœ íš¨í•œ ê±´ë¬¼ ê°„ ì´ë™: {len(valid_movements)}")
            
            # ì´ë™ í†µê³„ ê³„ì‚°
            movement_counts = valid_movements.groupby(
                ['from_building', 'to_building']
            ).size().reset_index(name='count')
            
            # ê±´ë¬¼ë³„ ë°©ë¬¸ í†µê³„
            building_visits = valid_movements['to_building'].value_counts()
            
            return {
                'movements': valid_movements,
                'movement_counts': movement_counts,
                'building_visits': building_visits,
                'total_movements': len(valid_movements),
                'unique_paths': len(movement_counts)
            }
            
        except Exception as e:
            self.logger.error(f"ì´ë™ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def display_network_metrics(self, interaction_data: Dict):
        """ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ í‘œì‹œ"""
        st.markdown("#### ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ë…¸ë“œ ìˆ˜", interaction_data['num_nodes'])
        with col2:
            st.metric("ì—£ì§€ ìˆ˜", interaction_data['num_edges'])
        with col3:
            st.metric("ë„¤íŠ¸ì›Œí¬ ë°€ë„", f"{interaction_data['density']:.3f}")
        with col4:
            avg_degree = (2 * interaction_data['num_edges']) / interaction_data['num_nodes'] if interaction_data['num_nodes'] > 0 else 0
            st.metric("í‰ê·  ì—°ê²°ë„", f"{avg_degree:.2f}")
    
    def visualize_interaction_network(self, interaction_data: Dict, viz_type: str):
        """ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
        st.markdown("#### ğŸŒ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")
        
        G = interaction_data['graph']
        
        # ë ˆì´ì•„ì›ƒ ì„ íƒ
        if viz_type == "Force-directed":
            pos = nx.spring_layout(G, k=1/np.sqrt(G.number_of_nodes()), iterations=50)
        elif viz_type == "Circular":
            pos = nx.circular_layout(G)
        else:  # Hierarchical
            pos = nx.kamada_kawai_layout(G)
        
        # Plotly ê·¸ë˜í”„ ìƒì„±
        edge_trace = []
        for edge in G.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_trace.append(go.Scatter(
                x=[x0, x1, None],
                y=[y0, y1, None],
                mode='lines',
                line=dict(width=0.5 + edge[2]['weight']/10, color='#888'),
                hoverinfo='none'
            ))
        
        node_trace = go.Scatter(
            x=[pos[node][0] for node in G.nodes()],
            y=[pos[node][1] for node in G.nodes()],
            mode='markers+text',
            text=[str(node) for node in G.nodes()],
            textposition="top center",
            hoverinfo='text',
            marker=dict(
                size=[10 + interaction_data['degree_centrality'][node] * 50 for node in G.nodes()],
                color=[interaction_data['betweenness_centrality'][node] for node in G.nodes()],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(
                    thickness=15,
                    title='Betweenness<br>Centrality',
                    xanchor='left',
                    titleside='right'
                )
            )
        )
        
        fig = go.Figure(data=edge_trace + [node_trace])
        fig.update_layout(
            showlegend=False,
            hovermode='closest',
            margin=dict(b=0, l=0, r=0, t=0),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def display_centrality_analysis(self, interaction_data: Dict):
        """ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("#### ğŸ¯ ì¤‘ì‹¬ì„± ë¶„ì„")
        
        # ìƒìœ„ 10ëª…ì˜ ì¤‘ì‹¬ ì¸ë¬¼
        centrality_df = pd.DataFrame({
            'Employee': list(interaction_data['degree_centrality'].keys()),
            'Degree Centrality': list(interaction_data['degree_centrality'].values()),
            'Betweenness Centrality': [interaction_data['betweenness_centrality'][k] 
                                      for k in interaction_data['degree_centrality'].keys()],
            'Closeness Centrality': [interaction_data['closeness_centrality'][k] 
                                   for k in interaction_data['degree_centrality'].keys()]
        })
        
        # ìƒìœ„ 10ëª…ë§Œ í‘œì‹œ
        top_central = centrality_df.nlargest(10, 'Degree Centrality')
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig1 = px.bar(top_central, x='Employee', y='Degree Centrality',
                         title='ì—°ê²° ì¤‘ì‹¬ì„± ìƒìœ„ 10ëª…')
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            fig2 = px.bar(top_central, x='Employee', y='Betweenness Centrality',
                         title='ë§¤ê°œ ì¤‘ì‹¬ì„± ìƒìœ„ 10ëª…')
            st.plotly_chart(fig2, use_container_width=True)
    
    def visualize_movement_map(self, movement_data: Dict):
        """ê³µê°„ ì´ë™ ë§µ ì‹œê°í™”"""
        facility_image_path = Path(__file__).parent.parent.parent.parent / 'data' / 'Sambio.png'
        
        if not facility_image_path.exists():
            st.warning("ì‹œì„¤ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì´ë™ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        G = nx.DiGraph()
        
        movement_counts = movement_data['movement_counts']
        for _, row in movement_counts.iterrows():
            G.add_edge(row['from_building'], row['to_building'], weight=row['count'])
        
        # matplotlib ì‹œê°í™”
        from PIL import Image
        img = Image.open(facility_image_path)
        
        fig, ax = plt.subplots(figsize=(20, 12))
        ax.imshow(img, alpha=0.7)
        
        # ë…¸ë“œ ìœ„ì¹˜ ì„¤ì •
        img_width, img_height = img.size
        pos = {}
        for node in G.nodes():
            coords = self.network_analyzer.mapper.get_coordinates(node, img_width, img_height)
            if coords:
                pos[node] = coords
        
        # ë…¸ë“œ í¬ê¸° (ë°©ë¬¸ íšŸìˆ˜ ê¸°ë°˜)
        node_sizes = []
        for node in G.nodes():
            if node in movement_data['building_visits']:
                size = 500 + movement_data['building_visits'][node] * 10
            else:
                size = 500
            node_sizes.append(size)
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë¦¬ê¸°
        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, 
                             node_color='lightblue', alpha=0.8, ax=ax)
        
        # ì—£ì§€ ê·¸ë¦¬ê¸°
        edges = G.edges()
        weights = [G[u][v]['weight'] for u, v in edges]
        max_weight = max(weights) if weights else 1
        
        nx.draw_networkx_edges(G, pos, width=[1 + (w/max_weight)*5 for w in weights],
                             edge_color='blue', alpha=0.6, arrows=True,
                             arrowsize=20, ax=ax)
        
        # ë ˆì´ë¸”
        labels = {}
        for node in G.nodes():
            count = movement_data['building_visits'].get(node, 0)
            labels[node] = f"{node}\\n({count})"
        
        nx.draw_networkx_labels(G, pos, labels, font_size=12, font_weight='bold', ax=ax)
        
        ax.set_title("ê³µê°„ ì´ë™ ë„¤íŠ¸ì›Œí¬", fontsize=16)
        ax.axis('off')
        
        st.pyplot(fig, use_container_width=True)
        plt.close()
    
    def display_movement_statistics(self, movement_data: Dict):
        """ì´ë™ í†µê³„ í‘œì‹œ"""
        st.markdown("#### ğŸ“Š ì´ë™ í†µê³„")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì´ ì´ë™ íšŸìˆ˜", movement_data['total_movements'])
        with col2:
            st.metric("ê³ ìœ  ì´ë™ ê²½ë¡œ", movement_data['unique_paths'])
        with col3:
            st.metric("ë°©ë¬¸ ê±´ë¬¼ ìˆ˜", len(movement_data['building_visits']))
        
        # ìƒìœ„ ì´ë™ ê²½ë¡œ
        st.markdown("##### ğŸ” ìƒìœ„ ì´ë™ ê²½ë¡œ")
        top_paths = movement_data['movement_counts'].nlargest(10, 'count')
        
        fig = px.bar(top_paths, 
                    x='count', 
                    y=top_paths['from_building'] + ' â†’ ' + top_paths['to_building'],
                    orientation='h',
                    title='ê°€ì¥ ë¹ˆë²ˆí•œ ì´ë™ ê²½ë¡œ Top 10')
        st.plotly_chart(fig, use_container_width=True)
    
    def get_departments(self) -> List[str]:
        """ë¶€ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Pickle íŒŒì¼ì—ì„œ ë¶€ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            
            # ì¡°ì§ ë°ì´í„°ì—ì„œ ë¶€ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            org_data = pickle_manager.load_dataframe(name='organization_data')
            if org_data is not None and 'TEAM' in org_data.columns:
                departments = org_data['TEAM'].dropna().unique().tolist()
                return sorted(departments)
            
            # íƒœê·¸ ë°ì´í„°ì—ì„œ ë¶€ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            if tag_data is not None and 'TEAM' in tag_data.columns:
                departments = tag_data['TEAM'].dropna().unique().tolist()
                return sorted(departments)
            
            return []
        except Exception as e:
            self.logger.error(f"ë¶€ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def display_community_detection(self, interaction_data: Dict):
        """ì»¤ë®¤ë‹ˆí‹° íƒì§€ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("#### ğŸ‘¥ ì»¤ë®¤ë‹ˆí‹° íƒì§€")
        
        G = interaction_data['graph']
        
        # Louvain ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° íƒì§€
        try:
            import community as community_louvain
            communities = community_louvain.best_partition(G)
            
            # ì»¤ë®¤ë‹ˆí‹°ë³„ ë©¤ë²„ ìˆ˜
            community_sizes = {}
            for node, comm in communities.items():
                if comm not in community_sizes:
                    community_sizes[comm] = 0
                community_sizes[comm] += 1
            
            # ì»¤ë®¤ë‹ˆí‹° ì •ë³´ í‘œì‹œ
            st.write(f"ë°œê²¬ëœ ì»¤ë®¤ë‹ˆí‹° ìˆ˜: {len(community_sizes)}")
            
            # ì»¤ë®¤ë‹ˆí‹° í¬ê¸° ë¶„í¬
            fig = px.bar(x=list(community_sizes.keys()), 
                        y=list(community_sizes.values()),
                        labels={'x': 'ì»¤ë®¤ë‹ˆí‹° ID', 'y': 'ë©¤ë²„ ìˆ˜'},
                        title='ì»¤ë®¤ë‹ˆí‹°ë³„ í¬ê¸° ë¶„í¬')
            st.plotly_chart(fig, use_container_width=True)
            
        except ImportError:
            st.info("ì»¤ë®¤ë‹ˆí‹° íƒì§€ë¥¼ ìœ„í•´ python-louvain íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def analyze_temporal_network(self, start_date: date, end_date: date,
                               time_granularity: str, network_type: str) -> Optional[Dict]:
        """ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ë¶„ì„"""
        try:
            # Pickle íŒŒì¼ì—ì„œ íƒœê·¸ ë°ì´í„° ë¡œë“œ
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            
            if tag_data is None or tag_data.empty:
                st.error("íƒœê·¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë‚ ì§œ í•„í„°ë§
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')
            
            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            with st.expander("ë°ì´í„° í™•ì¸", expanded=False):
                st.write(f"ì„ íƒí•œ ê¸°ê°„: {start_str} ~ {end_str}")
                st.write(f"ì „ì²´ ë°ì´í„° ìˆ˜: {len(tag_data)}")
                
            tag_data['ENTE_DT'] = pd.to_numeric(tag_data['ENTE_DT'], errors='coerce')
            filtered_data = tag_data[
                (tag_data['ENTE_DT'] >= int(start_str)) & 
                (tag_data['ENTE_DT'] <= int(end_str))
            ].copy()
            
            if filtered_data.empty:
                st.warning(f"ì„ íƒí•œ ê¸°ê°„({start_str} ~ {end_str})ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì‹œê°„ ì •ë³´ ìƒì„±
            # ì¶œì…ì‹œê°ì„ 6ìë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜ (HHMMSS í˜•ì‹)
            filtered_data['time'] = filtered_data['ì¶œì…ì‹œê°'].astype(str).str.zfill(6)
            filtered_data['timestamp'] = pd.to_datetime(
                filtered_data['ENTE_DT'].astype(str) + ' ' + filtered_data['time'],
                format='%Y%m%d %H%M%S',
                errors='coerce'
            )
            
            # ì‹œê°„ ë‹¨ìœ„ë³„ë¡œ ê·¸ë£¹í™”
            if time_granularity == "ì‹œê°„ë³„":
                filtered_data['time_group'] = filtered_data['timestamp'].dt.floor('H')
            elif time_granularity == "ì¼ë³„":
                filtered_data['time_group'] = filtered_data['timestamp'].dt.date
            else:  # ì£¼ë³„
                filtered_data['time_group'] = filtered_data['timestamp'].dt.to_period('W')
            
            # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
            with st.expander("ì‹œê°„ ê·¸ë£¹ ì •ë³´", expanded=False):
                st.write(f"í•„í„°ë§ëœ ë°ì´í„° ìˆ˜: {len(filtered_data)}")
                st.write(f"ê³ ìœ  ì‹œê°„ ê·¸ë£¹ ìˆ˜: {filtered_data['time_group'].nunique()}")
                
            # ì‹œê°„ë³„ ë„¤íŠ¸ì›Œí¬ ìƒì„±
            temporal_networks = {}
            
            for time_group, group_data in filtered_data.groupby('time_group'):
                if network_type == "ìƒí˜¸ì‘ìš©":
                    # ê°™ì€ ì‹œê°„, ê°™ì€ ì¥ì†Œì˜ ì§ì›ë“¤
                    G = nx.Graph()
                    for location, loc_group in group_data.groupby('DR_NM'):
                        employees = loc_group['ì‚¬ë²ˆ'].unique()
                        for i, emp1 in enumerate(employees):
                            for emp2 in employees[i+1:]:
                                G.add_edge(str(emp1), str(emp2))
                elif network_type == "ì´ë™":
                    # ì´ë™ ë„¤íŠ¸ì›Œí¬
                    G = nx.DiGraph()
                    for emp_id, emp_data in group_data.groupby('ì‚¬ë²ˆ'):
                        emp_data = emp_data.sort_values('timestamp')
                        for i in range(len(emp_data) - 1):
                            loc1 = emp_data.iloc[i]['DR_NM']
                            loc2 = emp_data.iloc[i+1]['DR_NM']
                            if loc1 != loc2:
                                G.add_edge(loc1, loc2)
                else:  # í˜‘ì—…
                    G = nx.Graph()
                    # ê°„ë‹¨í•œ í˜‘ì—… ë„¤íŠ¸ì›Œí¬ (ê°™ì€ ì¥ì†Œì— ìˆëŠ” ì§ì›ë“¤)
                    for location, loc_group in group_data.groupby('DR_NM'):
                        employees = loc_group['ì‚¬ë²ˆ'].unique()
                        for i, emp1 in enumerate(employees):
                            for emp2 in employees[i+1:]:
                                G.add_edge(str(emp1), str(emp2))
                
                temporal_networks[str(time_group)] = {
                    'graph': G,
                    'num_nodes': G.number_of_nodes(),
                    'num_edges': G.number_of_edges(),
                    'density': nx.density(G) if G.number_of_nodes() > 0 else 0
                }
            
            # ë„¤íŠ¸ì›Œí¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not temporal_networks:
                st.info("ìƒì„±ëœ ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì‹œê°„ ë‹¨ìœ„ë‚˜ ë„¤íŠ¸ì›Œí¬ ìœ í˜•ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
                return None
                
            return {
                'networks': temporal_networks,
                'time_granularity': time_granularity,
                'network_type': network_type
            }
            
        except Exception as e:
            self.logger.error(f"ì‹œê³„ì—´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_activity_network(self, start_date: date, end_date: date,
                               activity_types: List[str], network_method: str) -> Optional[Dict]:
        """í™œë™ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        try:
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ í™œë™ì„ ì¶”ì •
            # ì‹¤ì œë¡œëŠ” HMM ëª¨ë¸ ê²°ê³¼ë‚˜ í™œë™ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
            
            from ...data_processing import PickleManager
            pickle_manager = PickleManager()
            tag_data = pickle_manager.load_dataframe(name='tag_data')
            
            if tag_data is None or tag_data.empty:
                return None
            
            # ë‚ ì§œ í•„í„°ë§
            start_str = start_date.strftime('%Y%m%d')
            end_str = end_date.strftime('%Y%m%d')
            
            tag_data['ENTE_DT'] = pd.to_numeric(tag_data['ENTE_DT'], errors='coerce')
            filtered_data = tag_data[
                (tag_data['ENTE_DT'] >= int(start_str)) & 
                (tag_data['ENTE_DT'] <= int(end_str))
            ].copy()
            
            if filtered_data.empty:
                return None
            
            # timestamp ìƒì„±
            # ì¶œì…ì‹œê°ì„ 6ìë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜ (HHMMSS í˜•ì‹)
            filtered_data['time'] = filtered_data['ì¶œì…ì‹œê°'].astype(str).str.zfill(6)
            filtered_data['timestamp'] = pd.to_datetime(
                filtered_data['ENTE_DT'].astype(str) + ' ' + filtered_data['time'],
                format='%Y%m%d %H%M%S',
                errors='coerce'
            )
            
            # ê°„ë‹¨í•œ í™œë™ ë¶„ë¥˜ (ìœ„ì¹˜ ê¸°ë°˜)
            filtered_data['activity'] = 'ì—…ë¬´'  # ê¸°ë³¸ê°’
            filtered_data.loc[filtered_data['DR_NM'].str.contains('ì‹ë‹¹|CAFETERIA', case=False, na=False), 'activity'] = 'ì‹ì‚¬'
            filtered_data.loc[filtered_data['DR_NM'].str.contains('íšŒì˜|MEETING', case=False, na=False), 'activity'] = 'íšŒì˜'
            filtered_data.loc[filtered_data['DR_NM'].str.contains('íœ´ê²Œ|REST', case=False, na=False), 'activity'] = 'íœ´ì‹'
            filtered_data.loc[filtered_data['DR_GB'] == 'ì¶œì…ê²Œì´íŠ¸', 'activity'] = 'ì´ë™'
            
            # ì„ íƒëœ í™œë™ë§Œ í•„í„°ë§
            filtered_data = filtered_data[filtered_data['activity'].isin(activity_types)]
            
            if filtered_data.empty:
                return None
            
            # ë„¤íŠ¸ì›Œí¬ ìƒì„±
            G = nx.Graph()
            
            if network_method == "ë™ì‹œ í™œë™":
                # ê°™ì€ ì‹œê°„, ê°™ì€ í™œë™ì„ í•˜ëŠ” ì§ì›ë“¤
                filtered_data['time_slot'] = filtered_data['timestamp'].dt.floor('30min')
                
                for (activity, time_slot), group in filtered_data.groupby(['activity', 'time_slot']):
                    employees = group['ì‚¬ë²ˆ'].unique()
                    for i, emp1 in enumerate(employees):
                        for emp2 in employees[i+1:]:
                            if G.has_edge(str(emp1), str(emp2)):
                                G[str(emp1)][str(emp2)]['weight'] += 1
                            else:
                                G.add_edge(str(emp1), str(emp2), weight=1, activity=activity)
            
            elif network_method == "ìˆœì°¨ í™œë™":
                # ê°™ì€ í™œë™ì„ ìˆœì°¨ì ìœ¼ë¡œ í•˜ëŠ” ì§ì›ë“¤
                for activity in activity_types:
                    activity_data = filtered_data[filtered_data['activity'] == activity]
                    for location, loc_group in activity_data.groupby('DR_NM'):
                        loc_group = loc_group.sort_values('timestamp')
                        employees = loc_group['ì‚¬ë²ˆ'].tolist()
                        for i in range(len(employees) - 1):
                            G.add_edge(str(employees[i]), str(employees[i+1]), activity=activity)
            
            else:  # í™œë™ ì „í™˜
                # í™œë™ ì „í™˜ íŒ¨í„´ì´ ìœ ì‚¬í•œ ì§ì›ë“¤
                # ê°„ë‹¨íˆ êµ¬í˜„: ê°™ì€ í™œë™ ì‹œí€€ìŠ¤ë¥¼ ê°€ì§„ ì§ì›ë“¤
                pass
            
            # í™œë™ë³„ í†µê³„
            activity_stats = filtered_data['activity'].value_counts().to_dict()
            
            return {
                'graph': G,
                'activity_stats': activity_stats,
                'num_nodes': G.number_of_nodes(),
                'num_edges': G.number_of_edges(),
                'network_method': network_method
            }
            
        except Exception as e:
            self.logger.error(f"í™œë™ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def display_network_evolution_metrics(self, temporal_data: Dict):
        """ë„¤íŠ¸ì›Œí¬ ì§„í™” ë©”íŠ¸ë¦­ í‘œì‹œ"""
        st.markdown("#### ğŸ“Š ë„¤íŠ¸ì›Œí¬ ì§„í™” ë©”íŠ¸ë¦­")
        
        networks = temporal_data['networks']
        
        # ì‹œê°„ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        time_points = sorted(networks.keys())
        metrics = {
            'nodes': [networks[t]['num_nodes'] for t in time_points],
            'edges': [networks[t]['num_edges'] for t in time_points],
            'density': [networks[t]['density'] for t in time_points]
        }
        
        # ë©”íŠ¸ë¦­ ì‹œê°í™”
        fig = make_subplots(
            rows=3, cols=1,
            subplot_titles=('ë…¸ë“œ ìˆ˜ ë³€í™”', 'ì—£ì§€ ìˆ˜ ë³€í™”', 'ë„¤íŠ¸ì›Œí¬ ë°€ë„ ë³€í™”')
        )
        
        fig.add_trace(
            go.Scatter(x=time_points, y=metrics['nodes'], mode='lines+markers'),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(x=time_points, y=metrics['edges'], mode='lines+markers'),
            row=2, col=1
        )
        
        fig.add_trace(
            go.Scatter(x=time_points, y=metrics['density'], mode='lines+markers'),
            row=3, col=1
        )
        
        fig.update_layout(height=900, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    def visualize_animated_network(self, temporal_data: Dict, animation_speed: int):
        """ì• ë‹ˆë©”ì´ì…˜ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
        st.markdown("#### ğŸ¬ ë„¤íŠ¸ì›Œí¬ ì• ë‹ˆë©”ì´ì…˜")
        st.info("ì‹œê°„ì— ë”°ë¥¸ ë„¤íŠ¸ì›Œí¬ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        # ê°„ë‹¨í•œ ì •ì  ì‹œê°í™”ë¡œ ëŒ€ì²´
        networks = temporal_data['networks']
        if networks:
            # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë„¤íŠ¸ì›Œí¬ ë¹„êµ
            time_points = sorted(networks.keys())
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"ì‹œì‘ ì‹œì : {time_points[0]}")
                st.write(f"ë…¸ë“œ: {networks[time_points[0]]['num_nodes']}, ì—£ì§€: {networks[time_points[0]]['num_edges']}")
            
            with col2:
                st.write(f"ì¢…ë£Œ ì‹œì : {time_points[-1]}")
                st.write(f"ë…¸ë“œ: {networks[time_points[-1]]['num_nodes']}, ì—£ì§€: {networks[time_points[-1]]['num_edges']}")
    
    def display_temporal_patterns(self, temporal_data: Dict):
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„ í‘œì‹œ"""
        st.markdown("#### ğŸ• ì‹œê°„ëŒ€ë³„ íŒ¨í„´")
        
        networks = temporal_data['networks']
        
        # ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´
        if temporal_data['time_granularity'] == "ì‹œê°„ë³„":
            # ì‹œê°„ëŒ€ë³„ ë„¤íŠ¸ì›Œí¬ í¬ê¸°
            hourly_stats = {}
            for time_str, network in networks.items():
                try:
                    hour = pd.to_datetime(time_str).hour
                    if hour not in hourly_stats:
                        hourly_stats[hour] = []
                    hourly_stats[hour].append(network['num_edges'])
                except:
                    pass
            
            if hourly_stats:
                avg_edges_by_hour = {h: np.mean(edges) for h, edges in hourly_stats.items()}
                
                fig = px.bar(
                    x=list(avg_edges_by_hour.keys()),
                    y=list(avg_edges_by_hour.values()),
                    labels={'x': 'ì‹œê°„', 'y': 'í‰ê·  ì—°ê²° ìˆ˜'},
                    title='ì‹œê°„ëŒ€ë³„ í‰ê·  ë„¤íŠ¸ì›Œí¬ í™œë™'
                )
                st.plotly_chart(fig, use_container_width=True)
    
    def display_anomaly_detection(self, temporal_data: Dict):
        """ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("#### ğŸ” ì´ìƒ íŒ¨í„´ íƒì§€")
        
        networks = temporal_data['networks']
        
        # ê°„ë‹¨í•œ ì´ìƒì¹˜ íƒì§€ (í‰ê· ì—ì„œ 2 í‘œì¤€í¸ì°¨ ì´ìƒ ë²—ì–´ë‚œ ê²½ìš°)
        edge_counts = [n['num_edges'] for n in networks.values()]
        
        if len(edge_counts) > 3:
            mean_edges = np.mean(edge_counts)
            std_edges = np.std(edge_counts)
            
            anomalies = []
            for time_point, network in networks.items():
                if abs(network['num_edges'] - mean_edges) > 2 * std_edges:
                    anomalies.append({
                        'time': time_point,
                        'edges': network['num_edges'],
                        'deviation': (network['num_edges'] - mean_edges) / std_edges
                    })
            
            if anomalies:
                st.warning(f"{len(anomalies)}ê°œì˜ ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                anomaly_df = pd.DataFrame(anomalies)
                st.dataframe(anomaly_df)
            else:
                st.success("ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì´ìƒ íŒ¨í„´ íƒì§€ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def display_activity_statistics(self, activity_data: Dict):
        """í™œë™ ë„¤íŠ¸ì›Œí¬ í†µê³„ í‘œì‹œ"""
        st.markdown("#### ğŸ“Š í™œë™ í†µê³„")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì°¸ì—¬ ì§ì› ìˆ˜", activity_data['num_nodes'])
        with col2:
            st.metric("í™œë™ ì—°ê²° ìˆ˜", activity_data['num_edges'])
        with col3:
            st.metric("ë„¤íŠ¸ì›Œí¬ ë°©ì‹", activity_data['network_method'])
        
        # í™œë™ë³„ ë¶„í¬
        if activity_data['activity_stats']:
            fig = px.pie(
                values=list(activity_data['activity_stats'].values()),
                names=list(activity_data['activity_stats'].keys()),
                title='í™œë™ ìœ í˜•ë³„ ë¶„í¬'
            )
            st.plotly_chart(fig, use_container_width=True)
    
    def visualize_activity_network(self, activity_data: Dict):
        """í™œë™ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
        st.markdown("#### ğŸŒ í™œë™ ë„¤íŠ¸ì›Œí¬")
        
        G = activity_data['graph']
        
        if G.number_of_nodes() > 0:
            # ë ˆì´ì•„ì›ƒ ê³„ì‚°
            pos = nx.spring_layout(G)
            
            # ì—£ì§€ ê·¸ë¦¬ê¸°
            edge_trace = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                edge_trace.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=1, color='#888'),
                    hoverinfo='none'
                ))
            
            # ë…¸ë“œ ê·¸ë¦¬ê¸°
            node_trace = go.Scatter(
                x=[pos[node][0] for node in G.nodes()],
                y=[pos[node][1] for node in G.nodes()],
                mode='markers+text',
                text=[str(node) for node in G.nodes()],
                textposition="top center",
                marker=dict(size=10, color='lightblue'),
                hoverinfo='text'
            )
            
            fig = go.Figure(data=edge_trace + [node_trace])
            fig.update_layout(
                showlegend=False,
                hovermode='closest',
                margin=dict(b=0, l=0, r=0, t=0),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ë„¤íŠ¸ì›Œí¬ë¥¼ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def display_activity_clusters(self, activity_data: Dict):
        """í™œë™ í´ëŸ¬ìŠ¤í„° ë¶„ì„ í‘œì‹œ"""
        st.markdown("#### ğŸ‘¥ í™œë™ í´ëŸ¬ìŠ¤í„°")
        
        G = activity_data['graph']
        
        if G.number_of_nodes() > 0:
            # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ì°¾ê¸°
            components = list(nx.connected_components(G))
            
            st.write(f"ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(components)}")
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            cluster_sizes = [len(c) for c in components]
            
            fig = px.histogram(
                x=cluster_sizes,
                nbins=20,
                labels={'x': 'í´ëŸ¬ìŠ¤í„° í¬ê¸°', 'y': 'ê°œìˆ˜'},
                title='í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° ì •ë³´
            if components:
                largest_cluster = max(components, key=len)
                st.write(f"ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°: {len(largest_cluster)}ëª…")
    
    def display_activity_efficiency(self, activity_data: Dict):
        """í™œë™ íš¨ìœ¨ì„± ë¶„ì„ í‘œì‹œ"""
        st.markdown("#### ğŸ“ˆ í™œë™ íš¨ìœ¨ì„±")
        
        # ê°„ë‹¨í•œ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        if activity_data['num_nodes'] > 0:
            avg_connections = (2 * activity_data['num_edges']) / activity_data['num_nodes']
            st.metric("í‰ê·  ì—°ê²° ìˆ˜", f"{avg_connections:.2f}")
            
            # ë„¤íŠ¸ì›Œí¬ ë°€ë„
            G = activity_data['graph']
            if G.number_of_nodes() > 1:
                density = nx.density(G)
                st.metric("ë„¤íŠ¸ì›Œí¬ ë°€ë„", f"{density:.3f}")
                
                # íš¨ìœ¨ì„± í‰ê°€
                if density > 0.5:
                    st.success("ë†’ì€ í˜‘ì—… ë°€ë„ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
                elif density > 0.2:
                    st.info("ì ì ˆí•œ í˜‘ì—… ìˆ˜ì¤€ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("í˜‘ì—… ë°€ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. íŒ€ì›Œí¬ í–¥ìƒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")