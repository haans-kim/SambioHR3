"""
ë³‘ë ¬ ë°°ì¹˜ ë¶„ì„ ëª¨ë“ˆ - ê¸°ì¡´ ë¶„ì„ ë¡œì§ì„ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ê°€ì†í™”
"""

import os
import sys
import logging
import pickle
import json
from datetime import datetime, date
from typing import List, Dict, Any, Optional, Tuple
from multiprocessing import Pool, Manager, Queue, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd
from tqdm import tqdm
import psutil
import time

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.database import get_database_manager, get_pickle_manager
from src.analysis import IndividualAnalyzer
from src.analysis.analysis_result_saver import AnalysisResultSaver
from src.ui.components.individual_dashboard import IndividualDashboard


class ParallelBatchAnalyzer:
    """ì´ˆê³ ì† ë³‘ë ¬ ë°°ì¹˜ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, num_workers: int = None):
        """
        Args:
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ - 1)
        """
        # ì›Œì»¤ ìˆ˜ ê²°ì • (M4 MaxëŠ” 12ê°œ P-ì½”ì–´)
        self.num_workers = num_workers or min(cpu_count() - 1, 12)
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ë°ì´í„° ì‚¬ì „ ë¡œë“œ ë° ì¸ë±ì‹±
        self.logger.info(f"ë³‘ë ¬ ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” (ì›Œì»¤: {self.num_workers})")
        self.prepare_indexed_data()
        
        # ì§„í–‰ ìƒí™© í
        self.manager = Manager()
        self.progress_queue = self.manager.Queue()
        self.result_queue = self.manager.Queue()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        self.logger = logging.getLogger(__name__)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - [%(levelname)s] - %(message)s',
            datefmt='%H:%M:%S'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def prepare_indexed_data(self):
        """ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ê³  ì¸ë±ì‹±"""
        self.logger.info("ğŸ“¥ ë°ì´í„° ì¸ë±ì‹± ì‹œì‘...")
        
        # Pickle Managerë¡œ ë°ì´í„° ë¡œë“œ
        pickle_manager = get_pickle_manager()
        
        # ì¡°ì§ ë°ì´í„° ë¡œë“œ
        org_data = pickle_manager.load_dataframe('organization_data')
        
        # ì§ì›ë³„ ì¸ë±ìŠ¤ ìƒì„±
        self.employee_index = {}
        for _, row in org_data.iterrows():
            emp_id = row['ì‚¬ë²ˆ']
            self.employee_index[emp_id] = {
                'employee_id': emp_id,
                'employee_name': row['ì„±ëª…'],
                'center_id': row['ì„¼í„°'],
                'group_id': row['ê·¸ë£¹'],
                'team_id': row['íŒ€'],
                'job_grade': row.get('ì§ê¸‰2*', '')
            }
        
        self.logger.info(f"âœ… {len(self.employee_index):,}ëª… ì§ì› ì¸ë±ì‹± ì™„ë£Œ")
        
        # íƒœê·¸ ë°ì´í„° ì‚¬ì „ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            daily_tags = pickle_manager.load_dataframe('daily_tags')
            if daily_tags is not None:
                # ì§ì›-ë‚ ì§œë³„ ì¸ë±ìŠ¤ ìƒì„±
                self.tag_index = {}
                for emp_id, group in daily_tags.groupby('employee_id'):
                    self.tag_index[emp_id] = group.to_dict('records')
                self.logger.info(f"âœ… íƒœê·¸ ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ")
        except:
            self.tag_index = None
            self.logger.info("íƒœê·¸ ë°ì´í„° ì‚¬ì „ ë¡œë“œ ìŠ¤í‚µ")
    
    @staticmethod
    def analyze_single_employee(args: Tuple) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì§ì› ë¶„ì„ (ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)
        ì •ì  ë©”ì„œë“œë¡œ í”¼í´ë§ ê°€ëŠ¥
        """
        employee_id, analysis_date, employee_info = args
        
        try:
            # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ê°ì²´ ìƒì„±
            from src.database import get_database_manager
            from src.analysis import IndividualAnalyzer
            from src.ui.components.individual_dashboard import IndividualDashboard
            
            db_manager = get_database_manager()
            analyzer = IndividualAnalyzer(db_manager)
            dashboard = IndividualDashboard(analyzer)
            
            # ë¶„ì„ ìˆ˜í–‰
            daily_data = dashboard.get_daily_tag_data(employee_id, analysis_date)
            
            if daily_data is None or daily_data.empty:
                return {
                    'employee_id': employee_id,
                    'status': 'no_data',
                    'analysis_date': analysis_date.isoformat()
                }
            
            # í™œë™ ë¶„ë¥˜
            classified_data = dashboard.classify_activities(daily_data, employee_id, analysis_date)
            
            # ì¼ì¼ ë¶„ì„
            result = dashboard.analyze_daily_data(
                employee_id,
                analysis_date,
                classified_data
            )
            
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ë°ì´í„° ì •ë¦¬
            if 'raw_data' in result:
                del result['raw_data']
            if 'timeline_data' in result:
                del result['timeline_data']
            
            result['status'] = 'success'
            result['employee_info'] = employee_info
            
            return result
            
        except Exception as e:
            return {
                'employee_id': employee_id,
                'status': 'error',
                'error': str(e),
                'analysis_date': analysis_date.isoformat()
            }
    
    def batch_analyze_parallel(self, 
                             analysis_date: date,
                             employee_ids: List[str] = None,
                             center_id: str = None,
                             group_id: str = None,
                             team_id: str = None,
                             save_to_db: bool = True) -> Dict[str, Any]:
        """
        ë³‘ë ¬ ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
        
        Args:
            analysis_date: ë¶„ì„ ë‚ ì§œ
            employee_ids: íŠ¹ì • ì§ì› ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì¡°ì§ ê¸°ì¤€)
            center_id: ì„¼í„° ID
            group_id: ê·¸ë£¹ ID  
            team_id: íŒ€ ID
            save_to_db: DB ì €ì¥ ì—¬ë¶€
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ìš”ì•½
        """
        start_time = time.time()
        
        # ë¶„ì„í•  ì§ì› ëª©ë¡ ì¤€ë¹„
        if employee_ids:
            employees = [self.employee_index[emp_id] for emp_id in employee_ids 
                        if emp_id in self.employee_index]
        else:
            employees = self._filter_employees(center_id, group_id, team_id)
        
        if not employees:
            return {'status': 'no_employees', 'total': 0}
        
        total_count = len(employees)
        self.logger.info(f"ğŸš€ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {total_count:,}ëª…, ì›Œì»¤: {self.num_workers}ê°œ")
        
        # ë¶„ì„ íƒœìŠ¤í¬ ì¤€ë¹„
        tasks = [
            (emp['employee_id'], analysis_date, emp)
            for emp in employees
        ]
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        results = []
        success_count = 0
        error_count = 0
        
        # ProcessPoolExecutor ì‚¬ìš© (ë” ì•ˆì •ì )
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            # ëª¨ë“  íƒœìŠ¤í¬ ì œì¶œ
            futures = {
                executor.submit(self.analyze_single_employee, task): task[0]
                for task in tasks
            }
            
            # ì§„í–‰ë¥  í‘œì‹œ
            with tqdm(total=total_count, desc="ë¶„ì„ ì§„í–‰") as pbar:
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                        
                        if result['status'] == 'success':
                            success_count += 1
                            
                            # DB ì €ì¥
                            if save_to_db:
                                self._save_result(result)
                            
                            results.append(result)
                        else:
                            error_count += 1
                            
                    except Exception as e:
                        error_count += 1
                        self.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
                    
                    pbar.update(1)
                    
                    # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
                    if pbar.n % 100 == 0:
                        elapsed = time.time() - start_time
                        rate = pbar.n / elapsed
                        eta = (total_count - pbar.n) / rate if rate > 0 else 0
                        
                        pbar.set_postfix({
                            'ì„±ê³µ': success_count,
                            'ì‹¤íŒ¨': error_count,
                            'ì†ë„': f'{rate:.1f}/s',
                            'ë‚¨ì€ì‹œê°„': f'{eta/60:.1f}ë¶„'
                        })
        
        # ìµœì¢… í†µê³„
        elapsed_time = time.time() - start_time
        
        summary = {
            'status': 'completed',
            'analysis_date': analysis_date.isoformat(),
            'total_employees': total_count,
            'analyzed_count': success_count,
            'error_count': error_count,
            'success_rate': round(success_count / total_count * 100, 1),
            'elapsed_seconds': round(elapsed_time, 1),
            'processing_rate': round(total_count / elapsed_time, 1),
            'workers_used': self.num_workers,
            'saved_to_db': save_to_db
        }
        
        # í‰ê·  ì§€í‘œ ê³„ì‚°
        if results:
            valid_results = [r for r in results if 'work_time_analysis' in r]
            if valid_results:
                avg_efficiency = sum(r['work_time_analysis']['efficiency_ratio'] 
                                   for r in valid_results) / len(valid_results)
                avg_work_hours = sum(r['work_time_analysis']['actual_work_hours'] 
                                   for r in valid_results) / len(valid_results)
                
                summary['averages'] = {
                    'efficiency_ratio': round(avg_efficiency, 1),
                    'actual_work_hours': round(avg_work_hours, 1)
                }
        
        self.logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_count:,}ê±´ in {elapsed_time:.1f}ì´ˆ")
        self.logger.info(f"âš¡ ì²˜ë¦¬ ì†ë„: {summary['processing_rate']:.1f} ê±´/ì´ˆ")
        
        return summary
    
    def _filter_employees(self, center_id=None, group_id=None, team_id=None):
        """ì¡°ì§ ê¸°ì¤€ìœ¼ë¡œ ì§ì› í•„í„°ë§"""
        employees = []
        
        for emp_id, emp_info in self.employee_index.items():
            if center_id and emp_info['center_id'] != center_id:
                continue
            if group_id and emp_info['group_id'] != group_id:
                continue
            if team_id and emp_info['team_id'] != team_id:
                continue
            
            employees.append(emp_info)
        
        return employees
    
    def _save_result(self, result):
        """ë¶„ì„ ê²°ê³¼ DB ì €ì¥"""
        try:
            saver = AnalysisResultSaver()
            saver.save_individual_analysis(
                result,
                result.get('employee_info', {})
            )
        except Exception as e:
            self.logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        return {
            'cpu_count': cpu_count(),
            'workers': self.num_workers,
            'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
            'cpu_percent': psutil.cpu_percent(interval=1),
            'available_memory_gb': psutil.virtual_memory().available / 1024 / 1024 / 1024
        }


# CLI ì‹¤í–‰ìš©
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë³‘ë ¬ ë°°ì¹˜ ë¶„ì„')
    parser.add_argument('--date', type=str, required=True, help='ë¶„ì„ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--workers', type=int, help='ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜')
    parser.add_argument('--center', type=str, help='ì„¼í„° ID')
    parser.add_argument('--group', type=str, help='ê·¸ë£¹ ID')
    parser.add_argument('--team', type=str, help='íŒ€ ID')
    
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = ParallelBatchAnalyzer(num_workers=args.workers)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    sys_info = analyzer.get_system_info()
    print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  - CPU ì½”ì–´: {sys_info['cpu_count']}ê°œ")
    print(f"  - ì›Œì»¤: {sys_info['workers']}ê°œ")
    print(f"  - ë©”ëª¨ë¦¬: {sys_info['available_memory_gb']:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
    
    # ë¶„ì„ ì‹¤í–‰
    analysis_date = datetime.strptime(args.date, '%Y-%m-%d').date()
    
    summary = analyzer.batch_analyze_parallel(
        analysis_date,
        center_id=args.center,
        group_id=args.group,
        team_id=args.team
    )
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"  - ì´ ì§ì›: {summary['total_employees']:,}ëª…")
    print(f"  - ì„±ê³µ: {summary['analyzed_count']:,}ëª…")
    print(f"  - ì‹¤íŒ¨: {summary['error_count']:,}ëª…")
    print(f"  - ì†Œìš” ì‹œê°„: {summary['elapsed_seconds']:.1f}ì´ˆ")
    print(f"  - ì²˜ë¦¬ ì†ë„: {summary['processing_rate']:.1f}ê±´/ì´ˆ")