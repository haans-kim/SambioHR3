"""
ì‹±ê¸€í†¤ ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ìˆœì°¨ì  ì²˜ë¦¬
ë°ì´í„° ì •í™•ì„±ì„ ìœ„í•´ ë©€í‹°í”„ë¡œì„¸ì‹± ì—†ì´ ì‘ë™
"""

import os
import sys
import logging
import pickle
import json
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from tqdm import tqdm
import time

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.database import get_database_manager, get_pickle_manager
from src.analysis import IndividualAnalyzer
from src.analysis.analysis_result_saver import AnalysisResultSaver
from src.ui.components.individual_dashboard import IndividualDashboard


class SingletonBatchProcessor:
    """ì‹±ê¸€í†¤ ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ë°ì´í„° ì •í™•ì„±ì„ ìœ„í•œ ìˆœì°¨ ì²˜ë¦¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ì‹±ê¸€í†¤ ë§¤ë‹ˆì € ì‚¬ìš©
        self.db_manager = get_database_manager()
        self.pickle_manager = get_pickle_manager()
        
        # ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.individual_analyzer = IndividualAnalyzer(self.db_manager)
        self.result_saver = AnalysisResultSaver()
        
        # ë°ì´í„° ë¡œë“œ
        self.logger.info("ì‹±ê¸€í†¤ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”...")
        self.load_data()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        self.logger = logging.getLogger(__name__)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - [%(levelname)s] - %(message)s',
            datefmt='%H:%M:%S'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì‹œì‘...")
        
        # ì¡°ì§ ë°ì´í„° ë¡œë“œ
        self.org_data = self.pickle_manager.load_dataframe('organization_data')
        if self.org_data is None:
            raise ValueError("ì¡°ì§ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        self.logger.info(f"âœ… {len(self.org_data):,}ëª… ì§ì› ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
    def process_all_employees(self, start_date: date, end_date: date, 
                            save_to_db: bool = True, 
                            skip_existing: bool = True) -> Dict[str, Any]:
        """ëª¨ë“  ì§ì› ë¶„ì„ ì²˜ë¦¬ (ìˆœì°¨ì )
        
        Args:
            start_date: ë¶„ì„ ì‹œì‘ì¼
            end_date: ë¶„ì„ ì¢…ë£Œì¼  
            save_to_db: DB ì €ì¥ ì—¬ë¶€
            skip_existing: ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ìŠ¤í‚µ ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ì‹±ê¸€í†¤ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        self.logger.info(f"ë¶„ì„ ê¸°ê°„: {start_date} ~ {end_date}")
        self.logger.info(f"ì§ì› ìˆ˜: {len(self.org_data):,}ëª…")
        self.logger.info(f"{'='*60}\n")
        
        # ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ ëª©ë¡
        dates = pd.date_range(start_date, end_date).to_list()
        total_analyses = len(self.org_data) * len(dates)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'errors': [],
            'processing_time': 0
        }
        
        start_time = time.time()
        
        # ì§ì›ë³„ ìˆœì°¨ ì²˜ë¦¬
        with tqdm(total=total_analyses, desc="ì „ì²´ ì§„í–‰ë¥ ") as pbar:
            for _, employee in self.org_data.iterrows():
                employee_id = str(employee['ì‚¬ë²ˆ'])
                employee_name = employee['ì„±ëª…']
                
                # ì§ì›ë³„ ì²˜ë¦¬
                for analysis_date in dates:
                    try:
                        # ê¸°ì¡´ ê²°ê³¼ í™•ì¸
                        if skip_existing and self._check_existing_result(employee_id, analysis_date):
                            results['skipped'] += 1
                            pbar.update(1)
                            continue
                        
                        # ê°œë³„ ë¶„ì„ ìˆ˜í–‰
                        result = self._analyze_single_employee_date(
                            employee_id, 
                            employee_name,
                            analysis_date,
                            save_to_db
                        )
                        
                        if result:
                            results['success'] += 1
                        else:
                            results['failed'] += 1
                            
                    except Exception as e:
                        results['failed'] += 1
                        results['errors'].append({
                            'employee_id': employee_id,
                            'date': analysis_date.strftime('%Y-%m-%d'),
                            'error': str(e)
                        })
                        self.logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {employee_id} ({analysis_date}): {e}")
                    
                    pbar.update(1)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        results['processing_time'] = time.time() - start_time
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary(results, total_analyses)
        
        return results
    
    def _analyze_single_employee_date(self, employee_id: str, employee_name: str,
                                    analysis_date: date, save_to_db: bool) -> Optional[Dict]:
        """ë‹¨ì¼ ì§ì›-ë‚ ì§œ ë¶„ì„
        
        Args:
            employee_id: ì§ì› ID
            employee_name: ì§ì› ì´ë¦„
            analysis_date: ë¶„ì„ ë‚ ì§œ
            save_to_db: DB ì €ì¥ ì—¬ë¶€
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        try:
            # IndividualDashboard ì´ˆê¸°í™”
            dashboard = IndividualDashboard(
                individual_analyzer=self.individual_analyzer
            )
            
            # ë°ì´í„° ë¡œë“œ
            daily_data = dashboard.get_daily_tag_data(employee_id, analysis_date)
            
            if daily_data is None or daily_data.empty:
                return None
            
            # í™œë™ ë¶„ë¥˜
            classified_data = dashboard.classify_activities(daily_data)
            
            if classified_data is None or classified_data.empty:
                return None
            
            # ì¼ì¼ ë¶„ì„
            analysis_result = dashboard.analyze_daily_data(
                employee_id, 
                analysis_date, 
                classified_data
            )
            
            if analysis_result:
                # activity_summaryë¥¼ activity_analysis í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (FastBatchProcessorì™€ ë™ì¼)
                activity_summary = analysis_result.get('activity_summary', {})
                
                # ì˜ë¬¸ activity_codeë¥¼ í•œê¸€ë¡œ ë§¤í•‘
                activity_mapping = {
                    'WORK': 'ì—…ë¬´',
                    'FOCUSED_WORK': 'ì—…ë¬´(í™•ì‹¤)',
                    'EQUIPMENT_OPERATION': 'ì—…ë¬´',
                    'WORK_PREPARATION': 'ì¤€ë¹„',
                    'WORKING': 'ì—…ë¬´',
                    'MEETING': 'íšŒì˜',
                    'TRAINING': 'êµìœ¡',
                    'EDUCATION': 'êµìœ¡',
                    'BREAKFAST': 'ì‹ì‚¬',
                    'LUNCH': 'ì‹ì‚¬',
                    'DINNER': 'ì‹ì‚¬',
                    'MIDNIGHT_MEAL': 'ì‹ì‚¬',
                    'REST': 'íœ´ê²Œ',
                    'FITNESS': 'íœ´ê²Œ',
                    'LEAVE': 'íœ´ê²Œ',
                    'IDLE': 'íœ´ê²Œ',
                    'MOVEMENT': 'ì´ë™',
                    'TRANSIT': 'ê²½ìœ ',
                    'COMMUTE_IN': 'ì¶œì…(IN)',
                    'COMMUTE_OUT': 'ì¶œì…(OUT)',
                    'NON_WORK': 'ë¹„ì—…ë¬´',
                    'UNKNOWN': 'ë¹„ì—…ë¬´'
                }
                
                # í•œê¸€ í‚¤ë¡œ ë³€í™˜ëœ activity_distribution ìƒì„±
                activity_distribution = {}
                for code, minutes in activity_summary.items():
                    korean_key = activity_mapping.get(code, code)
                    if korean_key in activity_distribution:
                        activity_distribution[korean_key] += minutes
                    else:
                        activity_distribution[korean_key] = minutes
                
                # ë””ë²„ê¹…: ë³€í™˜ ê²°ê³¼ í™•ì¸
                if employee_id in ['20120203', '20150276']:  # ì²˜ìŒ ë‘ ì§ì›ë§Œ
                    self.logger.info(f"[DEBUG] {employee_id} - activity_summary: {activity_summary}")
                    self.logger.info(f"[DEBUG] {employee_id} - activity_distribution (í•œê¸€): {activity_distribution}")
                
                # activity_analysis êµ¬ì¡° ìƒì„±
                activity_analysis = {
                    'activity_distribution': activity_distribution,
                    'primary_activity': max(activity_distribution.items(), key=lambda x: x[1])[0] if activity_distribution else 'UNKNOWN',
                    'activity_diversity': len(activity_distribution)
                }
                
                # ê²°ê³¼ì— activity_analysis ì¶”ê°€
                analysis_result['activity_analysis'] = activity_analysis
                
                # timeline_analysis í˜•ì‹ ë§ì¶”ê¸°
                if 'activity_segments' in analysis_result:
                    analysis_result['timeline_analysis'] = {
                        'timeline': analysis_result.get('activity_segments', []),
                        'daily_timelines': []
                    }
                
                if save_to_db:
                    # DB ì €ì¥
                    self.result_saver.save_individual_analysis(analysis_result)
                
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"ì§ì› ë¶„ì„ ì˜¤ë¥˜ {employee_id}: {e}")
            return None
    
    def _check_existing_result(self, employee_id: str, analysis_date: date) -> bool:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        
        Args:
            employee_id: ì§ì› ID
            analysis_date: ë¶„ì„ ë‚ ì§œ
            
        Returns:
            ì¡´ì¬ ì—¬ë¶€
        """
        try:
            query = """
            SELECT 1 FROM individual_analysis_results 
            WHERE employee_id = :employee_id 
            AND analysis_date = :analysis_date
            LIMIT 1
            """
            
            result = self.db_manager.execute_query(query, {
                'employee_id': employee_id,
                'analysis_date': analysis_date.strftime('%Y-%m-%d')
            })
            
            return len(result) > 0
            
        except:
            return False
    
    def _print_summary(self, results: Dict[str, Any], total_analyses: int):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        
        Args:
            results: ì²˜ë¦¬ ê²°ê³¼
            total_analyses: ì „ì²´ ë¶„ì„ ìˆ˜
        """
        processing_time = results['processing_time']
        success_count = results['success']
        failed_count = results['failed']
        skipped_count = results['skipped']
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ì‹±ê¸€í†¤ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"ì „ì²´ ë¶„ì„ ìˆ˜: {total_analyses:,}")
        self.logger.info(f"ì„±ê³µ: {success_count:,} ({success_count/total_analyses*100:.1f}%)")
        self.logger.info(f"ì‹¤íŒ¨: {failed_count:,} ({failed_count/total_analyses*100:.1f}%)")
        self.logger.info(f"ìŠ¤í‚µ: {skipped_count:,} ({skipped_count/total_analyses*100:.1f}%)")
        self.logger.info(f"ì²˜ë¦¬ ì‹œê°„: {processing_time/60:.1f}ë¶„")
        self.logger.info(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {total_analyses/processing_time:.1f} ë¶„ì„/ì´ˆ")
        
        if results['errors']:
            self.logger.warning(f"\nì˜¤ë¥˜ ë°œìƒ ëª©ë¡ (ìƒìœ„ 10ê°œ):")
            for error in results['errors'][:10]:
                self.logger.warning(f"  - {error['employee_id']} ({error['date']}): {error['error']}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë‚ ì§œ ì„¤ì •
    start_date = date(2024, 10, 1)
    end_date = date(2024, 10, 31)
    
    # ì‹±ê¸€í†¤ í”„ë¡œì„¸ì„œ ìƒì„±
    processor = SingletonBatchProcessor()
    
    # ì „ì²´ ì§ì› ì²˜ë¦¬
    results = processor.process_all_employees(
        start_date=start_date,
        end_date=end_date,
        save_to_db=True,
        skip_existing=True
    )
    
    return results


if __name__ == "__main__":
    main()