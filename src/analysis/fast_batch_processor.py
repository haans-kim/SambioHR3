"""
ê³ ì† ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import date, datetime, timedelta
import logging
import time
import sqlite3
from pathlib import Path
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import tempfile
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.analysis.individual_analyzer import IndividualAnalyzer


class FastBatchProcessor:
    """ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, num_workers: int = 4, db_path: str = None):
        """
        Args:
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        self.num_workers = min(num_workers, os.cpu_count() or 4)
        self.logger = logging.getLogger(__name__)
        
        # DB ê²½ë¡œ ì„¤ì •
        if db_path:
            self.db_path = db_path
        else:
            db_file = Path(project_root) / 'data' / 'sambio_human.db'
            self.db_path = str(db_file) if db_file.exists() else 'data/sambio_human.db'
        
        self.logger.info(f"FastBatchProcessor ì´ˆê¸°í™” (ì›Œì»¤: {self.num_workers}, DB: {self.db_path})")
    
    def preload_data_for_date(self, target_date: date) -> str:
        """
        íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ê³  ì„ì‹œ íŒŒì¼ì— ì €ì¥
        Returns:
            ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        """
        self.logger.info(f"ğŸ“¥ {target_date} ë°ì´í„° ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # ë‚ ì§œ í˜•ì‹ ì¤€ë¹„
            prev_date = target_date - timedelta(days=1)
            target_date_str = target_date.strftime('%Y%m%d')
            prev_date_str = prev_date.strftime('%Y%m%d')
            
            # 1. íƒœê·¸ ë°ì´í„° ë¡œë“œ
            tag_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ENTE_DT, ì¶œì…ì‹œê°, DR_NM, INOUT_GB,
                       CENTER, BU, TEAM, GROUP_A, PART
                FROM tag_data 
                WHERE ENTE_DT BETWEEN {prev_date_str} AND {target_date_str}
                ORDER BY ì‚¬ë²ˆ, ì¶œì…ì‹œê°
            """
            tag_data = pd.read_sql_query(tag_query, conn)
            self.logger.info(f"  íƒœê·¸ ë°ì´í„°: {len(tag_data):,}ê±´")
            
            # 2. ì‹ì‚¬ ë°ì´í„° ë¡œë“œ
            meal_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ì·¨ì‹ì¼ì‹œ, ì •ì‚°ì¼, ì‹ë‹¹ëª…, 
                       ì‹ì‚¬êµ¬ë¶„ëª…, ì„±ëª…, ë¶€ì„œ
                FROM meal_data
                WHERE DATE(ì •ì‚°ì¼) BETWEEN '{prev_date}' AND '{target_date}'
            """
            meal_data = pd.read_sql_query(meal_query, conn)
            self.logger.info(f"  ì‹ì‚¬ ë°ì´í„°: {len(meal_data):,}ê±´")
            
            # 3. Claim ë°ì´í„° ë¡œë“œ
            claim_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ê·¼ë¬´ì¼, WORKSCHDTYPNM, 
                       ê·¼ë¬´ì‹œê°„, ì‹œì‘, ì¢…ë£Œ, ì„±ëª…, ë¶€ì„œ, ì§ê¸‰
                FROM claim_data
                WHERE DATE(ê·¼ë¬´ì¼) = '{target_date}'
            """
            claim_data = pd.read_sql_query(claim_query, conn)
            self.logger.info(f"  Claim ë°ì´í„°: {len(claim_data):,}ê±´")
            
        finally:
            conn.close()
        
        # ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ì— ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')
        data_cache = {
            'tag_data': tag_data,
            'meal_data': meal_data,
            'claim_data': claim_data,
            'target_date': target_date
        }
        
        with open(temp_file.name, 'wb') as f:
            pickle.dump(data_cache, f)
        
        elapsed = time.time() - start_time
        self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
        
        return temp_file.name
    
    def batch_analyze_employees(self, employee_ids: List[str], target_date: date, 
                              progress_callback=None) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ì›ì„ ì‹¤ì œ ë³‘ë ¬ë¡œ ë¶„ì„
        
        Args:
            employee_ids: ë¶„ì„í•  ì§ì› ID ëª©ë¡
            target_date: ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (completed_count, total_count, message)
        """
        self.logger.info(f"ğŸš€ ê³ ì† ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(employee_ids)}ëª…, {self.num_workers}ê°œ ì›Œì»¤")
        start_time = time.time()
        
        # 1. ë°ì´í„° ì‚¬ì „ ë¡œë“œ ë° ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file_path = self.preload_data_for_date(target_date)
        
        try:
            # 2. ì‘ì—…ì„ ì²­í¬ë¡œ ë¶„í• 
            chunk_size = max(1, len(employee_ids) // (self.num_workers * 4))  # ê° ì›Œì»¤ê°€ ì—¬ëŸ¬ ì²­í¬ ì²˜ë¦¬
            chunks = [employee_ids[i:i+chunk_size] for i in range(0, len(employee_ids), chunk_size)]
            
            self.logger.info(f"  ì²­í¬ ìˆ˜: {len(chunks)}, ì²­í¬ í¬ê¸°: ~{chunk_size}ëª…")
            
            # 3. ë³‘ë ¬ ì²˜ë¦¬
            results = []
            completed_count = 0
            
            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
                # ì‘ì—… ì œì¶œ
                future_to_chunk = {
                    executor.submit(process_employee_chunk, temp_file_path, chunk, target_date): chunk
                    for chunk in chunks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_chunk):
                    chunk = future_to_chunk[future]
                    try:
                        chunk_results = future.result()
                        results.extend(chunk_results)
                        completed_count += len(chunk_results)
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ
                        if completed_count % 100 == 0 or completed_count == len(employee_ids):
                            elapsed = time.time() - start_time
                            rate = completed_count / elapsed if elapsed > 0 else 0
                            remaining = (len(employee_ids) - completed_count) / rate if rate > 0 else 0
                            progress_msg = f"ì§„í–‰: {completed_count}/{len(employee_ids)} ({rate:.1f}ëª…/ì´ˆ)"
                            self.logger.info(f"  {progress_msg}, ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„")
                            
                            # ì½œë°± í˜¸ì¶œ
                            if progress_callback:
                                progress_callback(completed_count, len(employee_ids), progress_msg)
                    except Exception as e:
                        self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•œ ì²­í¬ì˜ ì§ì›ë“¤ì— ëŒ€í•´ ì—ëŸ¬ ê²°ê³¼ ì¶”ê°€
                        for emp_id in chunk:
                            results.append({
                                'employee_id': emp_id,
                                'analysis_date': target_date.isoformat(),
                                'status': 'error',
                                'error': str(e)
                            })
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.unlink(temp_file_path)
            except:
                pass
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r in results if r.get('status') == 'success')
        
        self.logger.info(f"âœ… ê³ ì† ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        self.logger.info(f"  - ì´ ì§ì›: {len(employee_ids)}ëª…")
        self.logger.info(f"  - ì„±ê³µ: {success_count}ëª…")
        self.logger.info(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        self.logger.info(f"  - ì²˜ë¦¬ ì†ë„: {len(employee_ids)/elapsed:.1f}ëª…/ì´ˆ")
        
        return results
    
    def save_results_to_db(self, results: List[Dict[str, Any]]) -> int:
        """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        self.logger.info(f"ğŸ’¾ {len(results)}ê±´ DB ì €ì¥ ì‹œì‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        try:
            for result in results:
                if result.get('status') != 'success':
                    continue
                
                # ë°ì´í„° ì¤€ë¹„
                work_time = result.get('work_time_analysis', {})
                meal_time = result.get('meal_time_analysis', {})
                
                # timelineì—ì„œ ì²« íƒœê·¸ì™€ ë§ˆì§€ë§‰ íƒœê·¸ ì‹œê°„ ì¶”ì¶œ
                timeline = result.get('timeline_analysis', {}).get('daily_timelines', [])
                work_start = None
                work_end = None
                total_hours = 0
                
                if timeline:
                    for daily in timeline:
                        events = daily.get('timeline', [])
                        if events:
                            first_event = events[0]
                            last_event = events[-1]
                            if work_start is None or first_event.get('timestamp') < work_start:
                                work_start = first_event.get('timestamp')
                            if work_end is None or last_event.get('timestamp') > work_end:
                                work_end = last_event.get('timestamp')
                            
                            # ì´ ì²´ë¥˜ì‹œê°„ ê³„ì‚°
                            if work_start and work_end:
                                start_dt = pd.to_datetime(work_start)
                                end_dt = pd.to_datetime(work_end)
                                total_hours = (end_dt - start_dt).total_seconds() / 3600
                
                # work_efficiency ê°’ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                efficiency_ratio = 0
                if work_time and 'work_efficiency' in work_time:
                    efficiency_ratio = work_time.get('work_efficiency', 0)
                elif work_time and 'efficiency_ratio' in work_time:
                    efficiency_ratio = work_time.get('efficiency_ratio', 0)
                
                data = {
                    'employee_id': result['employee_id'],
                    'analysis_date': result['analysis_date'],
                    'work_start': work_start,
                    'work_end': work_end,
                    'total_hours': total_hours,
                    'actual_work_hours': work_time.get('actual_work_hours', 0) if work_time else 0,
                    'claimed_work_hours': work_time.get('claimed_work_hours', 0) if work_time else 0,
                    'efficiency_ratio': efficiency_ratio,
                    'meal_count': (meal_time.get('lunch_count', 0) + meal_time.get('dinner_count', 0) + 
                                  meal_time.get('breakfast_count', 0) + meal_time.get('midnight_meal_count', 0)) if meal_time else 0,
                    'tag_count': result.get('data_quality', {}).get('total_tags', 0),
                    'updated_at': datetime.now().isoformat()
                }
                
                # í™œë™ë³„ ì‹œê°„ ë°ì´í„° ì¶”ê°€ (activity_analysisì—ì„œ ê°€ì ¸ì˜´)
                activity = result.get('activity_analysis', {})
                # activity_distributionì„ ì‚¬ìš©í•˜ê³  í•œê¸€ í‚¤ë¡œ ì ‘ê·¼
                activity_dist = activity.get('activity_distribution', {}) if activity else {}
                
                # ë””ë²„ê¹…
                if saved_count == 0:  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ë¡œê·¸
                    self.logger.info(f"[DEBUG] result keys: {list(result.keys())}")
                    self.logger.info(f"[DEBUG] activity_analysis: {activity}")
                    self.logger.info(f"[DEBUG] activity_distribution: {activity_dist}")
                
                data.update({
                    'work_minutes': activity_dist.get('ì—…ë¬´', 0) + activity_dist.get('ì—…ë¬´(í™•ì‹¤)', 0),
                    'meeting_minutes': activity_dist.get('íšŒì˜', 0) + activity_dist.get('êµìœ¡', 0),
                    'meal_minutes': activity_dist.get('ì‹ì‚¬', 0),
                    'movement_minutes': activity_dist.get('ê²½ìœ ', 0) + activity_dist.get('ì´ë™', 0),
                    'rest_minutes': activity_dist.get('íœ´ê²Œ', 0),
                    'breakfast_minutes': 0,  # ì„¸ë¶€ ì‹ì‚¬ ì‹œê°„ì€ í˜„ì¬ êµ¬ë¶„ë˜ì§€ ì•ŠìŒ
                    'lunch_minutes': 0,
                    'dinner_minutes': 0,
                    'midnight_meal_minutes': 0
                })
                
                # ì‹ ë¢°ë„ ì¶”ê°€
                data['confidence_score'] = result.get('data_quality', {}).get('data_completeness', 50)
                
                # UPSERT ì¿¼ë¦¬ (í™œë™ë³„ ì‹œê°„ ì»¬ëŸ¼ ì¶”ê°€)
                cursor.execute("""
                    INSERT OR REPLACE INTO daily_analysis_results 
                    (employee_id, analysis_date, work_start, work_end,
                     total_hours, actual_work_hours, claimed_work_hours,
                     efficiency_ratio, meal_count, tag_count,
                     work_minutes, meeting_minutes, meal_minutes,
                     movement_minutes, rest_minutes,
                     breakfast_minutes, lunch_minutes, dinner_minutes, midnight_meal_minutes,
                     confidence_score, updated_at)
                    VALUES 
                    (:employee_id, :analysis_date, :work_start, :work_end,
                     :total_hours, :actual_work_hours, :claimed_work_hours,
                     :efficiency_ratio, :meal_count, :tag_count,
                     :work_minutes, :meeting_minutes, :meal_minutes,
                     :movement_minutes, :rest_minutes,
                     :breakfast_minutes, :lunch_minutes, :dinner_minutes, :midnight_meal_minutes,
                     :confidence_score, :updated_at)
                """, data)
                
                saved_count += 1
                
                if saved_count % 1000 == 0:
                    conn.commit()  # ì£¼ê¸°ì ìœ¼ë¡œ ì»¤ë°‹
                    self.logger.info(f"  {saved_count}ê±´ ì €ì¥...")
            
            conn.commit()  # ìµœì¢… ì»¤ë°‹
            
        except Exception as e:
            self.logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            conn.rollback()
            
        finally:
            conn.close()
        
        self.logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {saved_count}ê±´")
        return saved_count


def process_employee_chunk(temp_file_path: str, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜
    IndividualAnalyzer.analyze_individual()ì„ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ ë¶„ì„ ìˆ˜í–‰
    """
    # IndividualAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    from src.analysis.individual_analyzer import IndividualAnalyzer
    from src.database import DatabaseManager
    from src.ui.components.individual_dashboard import IndividualDashboard
    from datetime import datetime, time
    
    # DatabaseManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    db_manager = DatabaseManager()
    analyzer = IndividualAnalyzer(db_manager)
    
    # IndividualDashboard ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‹±ê¸€í†¤ê³¼ ë™ì¼í•˜ê²Œ)
    dashboard = IndividualDashboard(individual_analyzer=analyzer)
    
    results = []
    
    for employee_id in employee_ids:
        try:
            # ì‹±ê¸€í†¤ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë¶„ì„
            # 1. ë°ì´í„° ë¡œë“œ
            daily_data = dashboard.get_daily_tag_data(employee_id, target_date)
            
            if daily_data is None or daily_data.empty:
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_data'
                })
                continue
            
            # 1-2. ì‹ì‚¬ ë°ì´í„°ë„ ë³„ë„ë¡œ ë¡œë“œ
            meal_data = dashboard.get_meal_data(employee_id, target_date)
            
            # 2. í™œë™ ë¶„ë¥˜
            classified_data = dashboard.classify_activities(daily_data)
            
            if classified_data is None or classified_data.empty:
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_classified_data'
                })
                continue
            
            # 3. ì¼ì¼ ë¶„ì„
            analysis_result = dashboard.analyze_daily_data(
                employee_id, 
                target_date, 
                classified_data
            )
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë°°ì¹˜ í”„ë¡œì„¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if analysis_result:
                # ë””ë²„ê¹…: analysis_result ë‚´ìš© í™•ì¸
                import logging
                logger = logging.getLogger(__name__)
                logger.info(f"[DEBUG] analysis_result keys: {list(analysis_result.keys())}")
                logger.info(f"[DEBUG] activity_summary: {analysis_result.get('activity_summary', {})}")
                
                # activity_summaryë¥¼ activity_analysis í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                activity_summary = analysis_result.get('activity_summary', {})
                
                # ì‹ì‚¬ ë°ì´í„°ê°€ ìˆìœ¼ë©´ activity_summaryì— ì¶”ê°€
                if meal_data is not None and not meal_data.empty:
                    # ì‹ì‚¬ ì¢…ë¥˜ë³„ë¡œ ì§‘ê³„
                    for _, meal in meal_data.iterrows():
                        meal_category = meal.get('ì‹ì‚¬ëŒ€ë¶„ë¥˜', meal.get('meal_category', ''))
                        meal_code_map = {
                            'ì¡°ì‹': 'BREAKFAST',
                            'ì¤‘ì‹': 'LUNCH',
                            'ì„ì‹': 'DINNER',
                            'ì•¼ì‹': 'MIDNIGHT_MEAL'
                        }
                        activity_code = meal_code_map.get(meal_category, 'LUNCH')
                        
                        # í…Œì´í¬ì•„ì›ƒ ì—¬ë¶€ì— ë”°ë¼ duration ì„¤ì •
                        restaurant_info = meal.get('ë°°ì‹êµ¬', meal.get('service_point', ''))
                        is_takeout = 'í…Œì´í¬ì•„ì›ƒ' in str(restaurant_info)
                        duration = 10 if is_takeout else 30
                        
                        # activity_summaryì— ì¶”ê°€
                        if activity_code in activity_summary:
                            activity_summary[activity_code] += duration
                        else:
                            activity_summary[activity_code] = duration
                    
                    logger.info(f"[DEBUG] {employee_id} - ì‹ì‚¬ ë°ì´í„° ì¶”ê°€ í›„ activity_summary: {activity_summary}")
                
                # ì˜ë¬¸ activity_codeë¥¼ í•œê¸€ë¡œ ë§¤í•‘
                activity_mapping = {
                    'WORK': 'ì—…ë¬´',
                    'FOCUSED_WORK': 'ì—…ë¬´(í™•ì‹¤)',
                    'EQUIPMENT_OPERATION': 'ì—…ë¬´',
                    'WORK_PREPARATION': 'ì¤€ë¹„',
                    'WORKING': 'ì—…ë¬´',
                    'MEETING': 'íšŒì˜',
                    'TRAINING': 'êµìœ¡',
                    'EDUCATION': 'êµìœ¡',
                    'BREAKFAST': 'ì‹ì‚¬',
                    'LUNCH': 'ì‹ì‚¬',
                    'DINNER': 'ì‹ì‚¬',
                    'MIDNIGHT_MEAL': 'ì‹ì‚¬',
                    'REST': 'íœ´ê²Œ',
                    'FITNESS': 'íœ´ê²Œ',
                    'LEAVE': 'íœ´ê²Œ',
                    'IDLE': 'íœ´ê²Œ',
                    'MOVEMENT': 'ì´ë™',
                    'TRANSIT': 'ê²½ìœ ',
                    'COMMUTE_IN': 'ì¶œì…(IN)',
                    'COMMUTE_OUT': 'ì¶œì…(OUT)',
                    'NON_WORK': 'ë¹„ì—…ë¬´',
                    'UNKNOWN': 'ë¹„ì—…ë¬´'
                }
                
                # í•œê¸€ í‚¤ë¡œ ë³€í™˜ëœ activity_distribution ìƒì„±
                activity_distribution = {}
                for code, minutes in activity_summary.items():
                    korean_key = activity_mapping.get(code, code)
                    if korean_key in activity_distribution:
                        activity_distribution[korean_key] += minutes
                    else:
                        activity_distribution[korean_key] = minutes
                
                # ë””ë²„ê¹…: ë³€í™˜ ê²°ê³¼ í™•ì¸
                if employee_id in ['20120203', '20150276']:  # ì²˜ìŒ ë‘ ì§ì›ë§Œ
                    logger.info(f"[DEBUG] {employee_id} - activity_summary: {activity_summary}")
                    logger.info(f"[DEBUG] {employee_id} - activity_distribution (í•œê¸€): {activity_distribution}")
                
                # activity_analysis êµ¬ì¡° ìƒì„±
                activity_analysis = {
                    'activity_distribution': activity_distribution,
                    'primary_activity': max(activity_distribution.items(), key=lambda x: x[1])[0] if activity_distribution else 'UNKNOWN',
                    'activity_diversity': len(activity_distribution)
                }
                
                # work_time_analysisì™€ ê¸°íƒ€ ë°ì´í„° ì¶”ê°€
                result_dict = {
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'success',
                    'activity_analysis': activity_analysis,
                    'work_time_analysis': analysis_result.get('work_time_analysis', {}),
                    'data_quality': analysis_result.get('data_quality', {}),
                    'timeline_analysis': {
                        'timeline': analysis_result.get('activity_segments', []),
                        'daily_timelines': []  # í˜¸í™˜ì„±ì„ ìœ„í•´
                    },
                    'meal_time_analysis': analysis_result.get('meal_time_analysis', {})
                }
                
                results.append(result_dict)
            else:
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_analysis_result'
                })
            
        except Exception as e:
            results.append({
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'error',
                'error': str(e)
            })
    
    return results