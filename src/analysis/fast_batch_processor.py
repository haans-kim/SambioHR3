"""
ê³ ì† ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import date, datetime, timedelta
import logging
import time
import sqlite3
from pathlib import Path
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import tempfile
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))


class FastBatchProcessor:
    """ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, num_workers: int = 4, db_path: str = None):
        """
        Args:
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        self.num_workers = min(num_workers, os.cpu_count() or 4)
        self.logger = logging.getLogger(__name__)
        
        # DB ê²½ë¡œ ì„¤ì •
        if db_path:
            self.db_path = db_path
        else:
            db_file = Path(project_root) / 'data' / 'sambio_human.db'
            self.db_path = str(db_file) if db_file.exists() else 'data/sambio_human.db'
        
        self.logger.info(f"FastBatchProcessor ì´ˆê¸°í™” (ì›Œì»¤: {self.num_workers}, DB: {self.db_path})")
    
    def preload_data_for_date(self, target_date: date) -> str:
        """
        íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ê³  ìž„ì‹œ íŒŒì¼ì— ì €ìž¥
        Returns:
            ìž„ì‹œ íŒŒì¼ ê²½ë¡œ
        """
        self.logger.info(f"ðŸ“¥ {target_date} ë°ì´í„° ì‚¬ì „ ë¡œë“œ ì‹œìž‘...")
        start_time = time.time()
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # ë‚ ì§œ í˜•ì‹ ì¤€ë¹„
            prev_date = target_date - timedelta(days=1)
            target_date_str = target_date.strftime('%Y%m%d')
            prev_date_str = prev_date.strftime('%Y%m%d')
            
            # 1. íƒœê·¸ ë°ì´í„° ë¡œë“œ
            tag_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ENTE_DT, ì¶œìž…ì‹œê°, DR_NM, INOUT_GB,
                       CENTER, BU, TEAM, GROUP_A, PART
                FROM tag_data 
                WHERE ENTE_DT BETWEEN {prev_date_str} AND {target_date_str}
                ORDER BY ì‚¬ë²ˆ, ì¶œìž…ì‹œê°
            """
            tag_data = pd.read_sql_query(tag_query, conn)
            self.logger.info(f"  íƒœê·¸ ë°ì´í„°: {len(tag_data):,}ê±´")
            
            # 2. ì‹ì‚¬ ë°ì´í„° ë¡œë“œ
            meal_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ì·¨ì‹ì¼ì‹œ, ì •ì‚°ì¼, ì‹ë‹¹ëª…, 
                       ì‹ì‚¬êµ¬ë¶„ëª…, ì„±ëª…, ë¶€ì„œ
                FROM meal_data
                WHERE DATE(ì •ì‚°ì¼) BETWEEN '{prev_date}' AND '{target_date}'
            """
            meal_data = pd.read_sql_query(meal_query, conn)
            self.logger.info(f"  ì‹ì‚¬ ë°ì´í„°: {len(meal_data):,}ê±´")
            
            # 3. Claim ë°ì´í„° ë¡œë“œ
            claim_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ê·¼ë¬´ì¼, WORKSCHDTYPNM, 
                       ê·¼ë¬´ì‹œê°„, ì‹œìž‘, ì¢…ë£Œ, ì„±ëª…, ë¶€ì„œ, ì§ê¸‰
                FROM claim_data
                WHERE DATE(ê·¼ë¬´ì¼) = '{target_date}'
            """
            claim_data = pd.read_sql_query(claim_query, conn)
            self.logger.info(f"  Claim ë°ì´í„°: {len(claim_data):,}ê±´")
            
        finally:
            conn.close()
        
        # ë°ì´í„°ë¥¼ ìž„ì‹œ íŒŒì¼ì— ì €ìž¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')
        data_cache = {
            'tag_data': tag_data,
            'meal_data': meal_data,
            'claim_data': claim_data,
            'target_date': target_date
        }
        
        with open(temp_file.name, 'wb') as f:
            pickle.dump(data_cache, f)
        
        elapsed = time.time() - start_time
        self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
        
        return temp_file.name
    
    def batch_analyze_employees(self, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ì›ì„ ì‹¤ì œ ë³‘ë ¬ë¡œ ë¶„ì„
        """
        self.logger.info(f"ðŸš€ ê³ ì† ë°°ì¹˜ ë¶„ì„ ì‹œìž‘: {len(employee_ids)}ëª…, {self.num_workers}ê°œ ì›Œì»¤")
        start_time = time.time()
        
        # 1. ë°ì´í„° ì‚¬ì „ ë¡œë“œ ë° ìž„ì‹œ íŒŒì¼ ì €ìž¥
        temp_file_path = self.preload_data_for_date(target_date)
        
        try:
            # 2. ìž‘ì—…ì„ ì²­í¬ë¡œ ë¶„í• 
            chunk_size = max(1, len(employee_ids) // (self.num_workers * 4))  # ê° ì›Œì»¤ê°€ ì—¬ëŸ¬ ì²­í¬ ì²˜ë¦¬
            chunks = [employee_ids[i:i+chunk_size] for i in range(0, len(employee_ids), chunk_size)]
            
            self.logger.info(f"  ì²­í¬ ìˆ˜: {len(chunks)}, ì²­í¬ í¬ê¸°: ~{chunk_size}ëª…")
            
            # 3. ë³‘ë ¬ ì²˜ë¦¬
            results = []
            completed_count = 0
            
            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
                # ìž‘ì—… ì œì¶œ
                future_to_chunk = {
                    executor.submit(process_employee_chunk, temp_file_path, chunk, target_date): chunk
                    for chunk in chunks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_chunk):
                    chunk = future_to_chunk[future]
                    try:
                        chunk_results = future.result()
                        results.extend(chunk_results)
                        completed_count += len(chunk_results)
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ
                        if completed_count % 100 == 0 or completed_count == len(employee_ids):
                            elapsed = time.time() - start_time
                            rate = completed_count / elapsed if elapsed > 0 else 0
                            remaining = (len(employee_ids) - completed_count) / rate if rate > 0 else 0
                            self.logger.info(f"  ì§„í–‰: {completed_count}/{len(employee_ids)} "
                                           f"({rate:.1f}ëª…/ì´ˆ, ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„)")
                    except Exception as e:
                        self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•œ ì²­í¬ì˜ ì§ì›ë“¤ì— ëŒ€í•´ ì—ëŸ¬ ê²°ê³¼ ì¶”ê°€
                        for emp_id in chunk:
                            results.append({
                                'employee_id': emp_id,
                                'analysis_date': target_date.isoformat(),
                                'status': 'error',
                                'error': str(e)
                            })
        
        finally:
            # ìž„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.unlink(temp_file_path)
            except:
                pass
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r in results if r.get('status') == 'success')
        
        self.logger.info(f"âœ… ê³ ì† ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        self.logger.info(f"  - ì´ ì§ì›: {len(employee_ids)}ëª…")
        self.logger.info(f"  - ì„±ê³µ: {success_count}ëª…")
        self.logger.info(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        self.logger.info(f"  - ì²˜ë¦¬ ì†ë„: {len(employee_ids)/elapsed:.1f}ëª…/ì´ˆ")
        
        return results
    
    def save_results_to_db(self, results: List[Dict[str, Any]]) -> int:
        """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ìž¥"""
        self.logger.info(f"ðŸ’¾ {len(results)}ê±´ DB ì €ìž¥ ì‹œìž‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        try:
            for result in results:
                if result.get('status') != 'success':
                    continue
                
                # ë°ì´í„° ì¤€ë¹„
                data = {
                    'employee_id': result['employee_id'],
                    'analysis_date': result['analysis_date'],
                    'work_start': result.get('work_start'),
                    'work_end': result.get('work_end'),
                    'total_hours': result['work_time_analysis']['total_hours'],
                    'actual_work_hours': result['work_time_analysis']['actual_work_hours'],
                    'claimed_work_hours': result['work_time_analysis']['scheduled_hours'],
                    'efficiency_ratio': result['work_time_analysis']['efficiency_ratio'],
                    'meal_count': result['meal_time_analysis']['meal_count'],
                    'tag_count': result.get('tag_count', 0),
                    'updated_at': datetime.now().isoformat()
                }
                
                # UPSERT ì¿¼ë¦¬
                cursor.execute("""
                    INSERT OR REPLACE INTO daily_analysis_results 
                    (employee_id, analysis_date, work_start, work_end,
                     total_hours, actual_work_hours, claimed_work_hours,
                     efficiency_ratio, meal_count, tag_count, updated_at)
                    VALUES 
                    (:employee_id, :analysis_date, :work_start, :work_end,
                     :total_hours, :actual_work_hours, :claimed_work_hours,
                     :efficiency_ratio, :meal_count, :tag_count, :updated_at)
                """, data)
                
                saved_count += 1
                
                if saved_count % 1000 == 0:
                    conn.commit()  # ì£¼ê¸°ì ìœ¼ë¡œ ì»¤ë°‹
                    self.logger.info(f"  {saved_count}ê±´ ì €ìž¥...")
            
            conn.commit()  # ìµœì¢… ì»¤ë°‹
            
        except Exception as e:
            self.logger.error(f"DB ì €ìž¥ ì‹¤íŒ¨: {e}")
            conn.rollback()
            
        finally:
            conn.close()
        
        self.logger.info(f"âœ… DB ì €ìž¥ ì™„ë£Œ: {saved_count}ê±´")
        return saved_count


def process_employee_chunk(temp_file_path: str, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜
    ì²­í¬ ë‹¨ìœ„ë¡œ ì§ì›ë“¤ì„ ë¶„ì„
    """
    # ìž„ì‹œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    with open(temp_file_path, 'rb') as f:
        data_cache = pickle.load(f)
    
    results = []
    
    for employee_id in employee_ids:
        try:
            # ì§ì› ë°ì´í„° í•„í„°ë§
            tag_data = data_cache['tag_data']
            emp_tag_data = tag_data[tag_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            if emp_tag_data.empty:
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_data'
                })
                continue
            
            meal_data = data_cache['meal_data']
            emp_meal_data = meal_data[meal_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            claim_data = data_cache['claim_data']
            emp_claim_data = claim_data[claim_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            # ê·¼ë¬´ ì‹œê°„ ê³„ì‚° (2êµëŒ€ ê·¼ë¬´ ê³ ë ¤)
            if not emp_tag_data.empty:
                try:
                    emp_tag_data['datetime'] = emp_tag_data.apply(
                        lambda row: pd.to_datetime(f"{row['ENTE_DT']} {str(row['ì¶œìž…ì‹œê°']).zfill(6)}", 
                                                  format='%Y%m%d %H%M%S', errors='coerce'),
                        axis=1
                    )
                    
                    emp_tag_data = emp_tag_data.dropna(subset=['datetime'])
                    
                    if not emp_tag_data.empty:
                        # 2êµëŒ€ ê·¼ë¬´ ì‹œìŠ¤í…œ: target_date ê¸°ì¤€ìœ¼ë¡œ ê·¼ë¬´ ì‹œê°„ ê³„ì‚°
                        # ì£¼ê°„ê·¼ë¬´: target_date 08:00 ~ 20:00
                        # ì•¼ê°„ê·¼ë¬´: target_date 20:00 ~ target_date+1 08:00
                        
                        target_date_str = target_date.strftime('%Y%m%d')
                        
                        # í•´ë‹¹ ë‚ ì§œì— ì†í•˜ëŠ” íƒœê·¸ë§Œ í•„í„°ë§
                        # ì£¼ê°„: target_dateì˜ íƒœê·¸
                        # ì•¼ê°„: target_date 20ì‹œ ì´í›„ + target_date+1 08ì‹œ ì´ì „
                        target_start = pd.to_datetime(f"{target_date} 00:00:00")
                        target_end = pd.to_datetime(f"{target_date} 23:59:59")
                        
                        # ë¨¼ì € target_dateì˜ íƒœê·¸ë§Œ í•„í„°ë§
                        day_tags = emp_tag_data[
                            (emp_tag_data['datetime'] >= target_start) & 
                            (emp_tag_data['datetime'] <= target_end)
                        ]
                        
                        if not day_tags.empty:
                            first_tag = day_tags['datetime'].min()
                            last_tag = day_tags['datetime'].max()
                            
                            # ì•¼ê°„ ê·¼ë¬´ì¸ ê²½ìš° ë‹¤ìŒë‚  ì˜¤ì „ íƒœê·¸ë„ í™•ì¸
                            if last_tag.hour >= 20:  # ì•¼ê°„ ê·¼ë¬´ ê°€ëŠ¥ì„±
                                next_day_start = target_end + pd.Timedelta(seconds=1)
                                next_day_end = next_day_start + pd.Timedelta(hours=12)  # ë‹¤ìŒë‚  ì •ì˜¤ê¹Œì§€
                                
                                next_day_tags = emp_tag_data[
                                    (emp_tag_data['datetime'] >= next_day_start) & 
                                    (emp_tag_data['datetime'] <= next_day_end)
                                ]
                                
                                if not next_day_tags.empty:
                                    last_tag = next_day_tags['datetime'].max()
                            
                            total_hours = (last_tag - first_tag).total_seconds() / 3600
                            
                            # ìµœëŒ€ 12ì‹œê°„ìœ¼ë¡œ ì œí•œ (2êµëŒ€ ê·¼ë¬´)
                            total_hours = min(total_hours, 12)
                        else:
                            total_hours = 0
                            first_tag = None
                            last_tag = None
                    else:
                        total_hours = 0
                        first_tag = None
                        last_tag = None
                except Exception as e:
                    total_hours = 0
                    first_tag = None
                    last_tag = None
            else:
                total_hours = 0
                first_tag = None
                last_tag = None
            
            # Claim ë°ì´í„°ì—ì„œ ì˜ˆì • ê·¼ë¬´ì‹œê°„
            if not emp_claim_data.empty:
                scheduled_hours = 8  # ê°„ë‹¨ížˆ 8ì‹œê°„ìœ¼ë¡œ ê°€ì •
                work_type = emp_claim_data.iloc[0].get('WORKSCHDTYPNM', 'ì¼ë°˜ê·¼ë¬´')
            else:
                scheduled_hours = 8
                work_type = 'ì¼ë°˜ê·¼ë¬´'
            
            # ì‹ì‚¬ íšŸìˆ˜
            meal_count = len(emp_meal_data)
            
            # ì‹¤ì œ ê·¼ë¬´ì‹œê°„ ì¶”ì •
            actual_work_hours = max(0, total_hours - (meal_count * 0.5))
            
            # íš¨ìœ¨ì„± ê³„ì‚°
            efficiency_ratio = (actual_work_hours / scheduled_hours * 100) if scheduled_hours > 0 else 0
            
            # ê²°ê³¼ ë°˜í™˜
            results.append({
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'success',
                'work_start': first_tag.isoformat() if first_tag else None,
                'work_end': last_tag.isoformat() if last_tag else None,
                'work_time_analysis': {
                    'total_hours': total_hours,
                    'actual_work_hours': actual_work_hours,
                    'scheduled_hours': scheduled_hours,
                    'efficiency_ratio': efficiency_ratio
                },
                'meal_time_analysis': {
                    'total_meal_time': meal_count * 30,
                    'meal_count': meal_count
                },
                'work_type': work_type,
                'tag_count': len(emp_tag_data),
                'attendance_hours': total_hours,
                'meeting_time': 0,
                'movement_time': 0,
                'rest_time': max(0, (total_hours - actual_work_hours - meal_count * 0.5) * 60) / 60,
                'work_estimation_rate': efficiency_ratio,
                'data_reliability': 80 if len(emp_tag_data) > 10 else 50
            })
            
        except Exception as e:
            results.append({
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'error',
                'error': str(e)
            })
    
    return results