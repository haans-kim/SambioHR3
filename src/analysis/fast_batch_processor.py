"""
ê³ ì† ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import date, datetime, timedelta
import logging
import time
import sqlite3
from pathlib import Path
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import tempfile
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.analysis.individual_analyzer import IndividualAnalyzer


class FastBatchProcessor:
    """ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, num_workers: int = 4, db_path: str = None):
        """
        Args:
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        self.num_workers = min(num_workers, os.cpu_count() or 4)
        self.logger = logging.getLogger(__name__)
        
        # DB ê²½ë¡œ ì„¤ì •
        if db_path:
            self.db_path = db_path
        else:
            db_file = Path(project_root) / 'data' / 'sambio_human.db'
            self.db_path = str(db_file) if db_file.exists() else 'data/sambio_human.db'
        
        self.logger.info(f"FastBatchProcessor ì´ˆê¸°í™” (ì›Œì»¤: {self.num_workers}, DB: {self.db_path})")
    
    def preload_data_for_date(self, target_date: date) -> str:
        """
        íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ê³  ì„ì‹œ íŒŒì¼ì— ì €ì¥
        Returns:
            ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        """
        self.logger.info(f"ğŸ“¥ {target_date} ë°ì´í„° ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # ë‚ ì§œ í˜•ì‹ ì¤€ë¹„
            prev_date = target_date - timedelta(days=1)
            target_date_str = target_date.strftime('%Y%m%d')
            prev_date_str = prev_date.strftime('%Y%m%d')
            
            # 1. íƒœê·¸ ë°ì´í„° ë¡œë“œ
            tag_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ENTE_DT, ì¶œì…ì‹œê°, DR_NM, INOUT_GB,
                       CENTER, BU, TEAM, GROUP_A, PART
                FROM tag_data 
                WHERE ENTE_DT BETWEEN {prev_date_str} AND {target_date_str}
                ORDER BY ì‚¬ë²ˆ, ì¶œì…ì‹œê°
            """
            tag_data = pd.read_sql_query(tag_query, conn)
            self.logger.info(f"  íƒœê·¸ ë°ì´í„°: {len(tag_data):,}ê±´")
            
            # 2. ì‹ì‚¬ ë°ì´í„° ë¡œë“œ
            meal_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ì·¨ì‹ì¼ì‹œ, ì •ì‚°ì¼, ì‹ë‹¹ëª…, 
                       ì‹ì‚¬êµ¬ë¶„ëª…, ì„±ëª…, ë¶€ì„œ
                FROM meal_data
                WHERE DATE(ì •ì‚°ì¼) BETWEEN '{prev_date}' AND '{target_date}'
            """
            meal_data = pd.read_sql_query(meal_query, conn)
            self.logger.info(f"  ì‹ì‚¬ ë°ì´í„°: {len(meal_data):,}ê±´")
            
            # 3. Claim ë°ì´í„° ë¡œë“œ
            claim_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ê·¼ë¬´ì¼, WORKSCHDTYPNM, 
                       ê·¼ë¬´ì‹œê°„, ì‹œì‘, ì¢…ë£Œ, ì„±ëª…, ë¶€ì„œ, ì§ê¸‰
                FROM claim_data
                WHERE DATE(ê·¼ë¬´ì¼) = '{target_date}'
            """
            claim_data = pd.read_sql_query(claim_query, conn)
            self.logger.info(f"  Claim ë°ì´í„°: {len(claim_data):,}ê±´")
            
        finally:
            conn.close()
        
        # ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ì— ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pkl')
        data_cache = {
            'tag_data': tag_data,
            'meal_data': meal_data,
            'claim_data': claim_data,
            'target_date': target_date
        }
        
        with open(temp_file.name, 'wb') as f:
            pickle.dump(data_cache, f)
        
        elapsed = time.time() - start_time
        self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
        
        return temp_file.name
    
    def batch_analyze_employees(self, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ì›ì„ ì‹¤ì œ ë³‘ë ¬ë¡œ ë¶„ì„
        """
        self.logger.info(f"ğŸš€ ê³ ì† ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(employee_ids)}ëª…, {self.num_workers}ê°œ ì›Œì»¤")
        start_time = time.time()
        
        # 1. ë°ì´í„° ì‚¬ì „ ë¡œë“œ ë° ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file_path = self.preload_data_for_date(target_date)
        
        try:
            # 2. ì‘ì—…ì„ ì²­í¬ë¡œ ë¶„í• 
            chunk_size = max(1, len(employee_ids) // (self.num_workers * 4))  # ê° ì›Œì»¤ê°€ ì—¬ëŸ¬ ì²­í¬ ì²˜ë¦¬
            chunks = [employee_ids[i:i+chunk_size] for i in range(0, len(employee_ids), chunk_size)]
            
            self.logger.info(f"  ì²­í¬ ìˆ˜: {len(chunks)}, ì²­í¬ í¬ê¸°: ~{chunk_size}ëª…")
            
            # 3. ë³‘ë ¬ ì²˜ë¦¬
            results = []
            completed_count = 0
            
            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
                # ì‘ì—… ì œì¶œ
                future_to_chunk = {
                    executor.submit(process_employee_chunk, temp_file_path, chunk, target_date): chunk
                    for chunk in chunks
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_chunk):
                    chunk = future_to_chunk[future]
                    try:
                        chunk_results = future.result()
                        results.extend(chunk_results)
                        completed_count += len(chunk_results)
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ
                        if completed_count % 100 == 0 or completed_count == len(employee_ids):
                            elapsed = time.time() - start_time
                            rate = completed_count / elapsed if elapsed > 0 else 0
                            remaining = (len(employee_ids) - completed_count) / rate if rate > 0 else 0
                            self.logger.info(f"  ì§„í–‰: {completed_count}/{len(employee_ids)} "
                                           f"({rate:.1f}ëª…/ì´ˆ, ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„)")
                    except Exception as e:
                        self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•œ ì²­í¬ì˜ ì§ì›ë“¤ì— ëŒ€í•´ ì—ëŸ¬ ê²°ê³¼ ì¶”ê°€
                        for emp_id in chunk:
                            results.append({
                                'employee_id': emp_id,
                                'analysis_date': target_date.isoformat(),
                                'status': 'error',
                                'error': str(e)
                            })
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.unlink(temp_file_path)
            except:
                pass
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r in results if r.get('status') == 'success')
        
        self.logger.info(f"âœ… ê³ ì† ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        self.logger.info(f"  - ì´ ì§ì›: {len(employee_ids)}ëª…")
        self.logger.info(f"  - ì„±ê³µ: {success_count}ëª…")
        self.logger.info(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        self.logger.info(f"  - ì²˜ë¦¬ ì†ë„: {len(employee_ids)/elapsed:.1f}ëª…/ì´ˆ")
        
        return results
    
    def save_results_to_db(self, results: List[Dict[str, Any]]) -> int:
        """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        self.logger.info(f"ğŸ’¾ {len(results)}ê±´ DB ì €ì¥ ì‹œì‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        try:
            for result in results:
                if result.get('status') != 'success':
                    continue
                
                # ë°ì´í„° ì¤€ë¹„
                work_time = result.get('work_time_analysis', {})
                meal_time = result.get('meal_time_analysis', {})
                
                # timelineì—ì„œ ì²« íƒœê·¸ì™€ ë§ˆì§€ë§‰ íƒœê·¸ ì‹œê°„ ì¶”ì¶œ
                timeline = result.get('timeline_analysis', {}).get('daily_timelines', [])
                work_start = None
                work_end = None
                total_hours = 0
                
                if timeline:
                    for daily in timeline:
                        events = daily.get('timeline', [])
                        if events:
                            first_event = events[0]
                            last_event = events[-1]
                            if work_start is None or first_event.get('timestamp') < work_start:
                                work_start = first_event.get('timestamp')
                            if work_end is None or last_event.get('timestamp') > work_end:
                                work_end = last_event.get('timestamp')
                            
                            # ì´ ì²´ë¥˜ì‹œê°„ ê³„ì‚°
                            if work_start and work_end:
                                start_dt = pd.to_datetime(work_start)
                                end_dt = pd.to_datetime(work_end)
                                total_hours = (end_dt - start_dt).total_seconds() / 3600
                
                # work_efficiency ê°’ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                efficiency_ratio = 0
                if work_time and 'work_efficiency' in work_time:
                    efficiency_ratio = work_time.get('work_efficiency', 0)
                elif work_time and 'efficiency_ratio' in work_time:
                    efficiency_ratio = work_time.get('efficiency_ratio', 0)
                
                data = {
                    'employee_id': result['employee_id'],
                    'analysis_date': result['analysis_date'],
                    'work_start': work_start,
                    'work_end': work_end,
                    'total_hours': total_hours,
                    'actual_work_hours': work_time.get('actual_work_hours', 0) if work_time else 0,
                    'claimed_work_hours': work_time.get('claimed_work_hours', 0) if work_time else 0,
                    'efficiency_ratio': efficiency_ratio,
                    'meal_count': (meal_time.get('lunch_count', 0) + meal_time.get('dinner_count', 0) + 
                                  meal_time.get('breakfast_count', 0) + meal_time.get('midnight_meal_count', 0)) if meal_time else 0,
                    'tag_count': result.get('data_quality', {}).get('total_tags', 0),
                    'updated_at': datetime.now().isoformat()
                }
                
                # í™œë™ë³„ ì‹œê°„ ë°ì´í„° ì¶”ê°€ (activity_analysisì—ì„œ ê°€ì ¸ì˜´)
                activity = result.get('activity_analysis', {})
                activity_summary = activity.get('activity_summary', {}) if activity else {}
                
                data.update({
                    'work_minutes': activity_summary.get('WORK', 0) + activity_summary.get('WORK_CONFIRMED', 0),
                    'meeting_minutes': activity_summary.get('MEETING', 0),
                    'meal_minutes': (activity_summary.get('BREAKFAST', 0) + activity_summary.get('LUNCH', 0) + 
                                   activity_summary.get('DINNER', 0) + activity_summary.get('MIDNIGHT_MEAL', 0)),
                    'movement_minutes': activity_summary.get('TRANSIT', 0),
                    'rest_minutes': activity_summary.get('REST', 0),
                    'breakfast_minutes': activity_summary.get('BREAKFAST', 0),
                    'lunch_minutes': activity_summary.get('LUNCH', 0),
                    'dinner_minutes': activity_summary.get('DINNER', 0),
                    'midnight_meal_minutes': activity_summary.get('MIDNIGHT_MEAL', 0)
                })
                
                # ì‹ ë¢°ë„ ì¶”ê°€
                data['confidence_score'] = result.get('data_quality', {}).get('data_completeness', 50)
                
                # UPSERT ì¿¼ë¦¬ (í™œë™ë³„ ì‹œê°„ ì»¬ëŸ¼ ì¶”ê°€)
                cursor.execute("""
                    INSERT OR REPLACE INTO daily_analysis_results 
                    (employee_id, analysis_date, work_start, work_end,
                     total_hours, actual_work_hours, claimed_work_hours,
                     efficiency_ratio, meal_count, tag_count,
                     work_minutes, meeting_minutes, meal_minutes,
                     movement_minutes, rest_minutes,
                     breakfast_minutes, lunch_minutes, dinner_minutes, midnight_meal_minutes,
                     confidence_score, updated_at)
                    VALUES 
                    (:employee_id, :analysis_date, :work_start, :work_end,
                     :total_hours, :actual_work_hours, :claimed_work_hours,
                     :efficiency_ratio, :meal_count, :tag_count,
                     :work_minutes, :meeting_minutes, :meal_minutes,
                     :movement_minutes, :rest_minutes,
                     :breakfast_minutes, :lunch_minutes, :dinner_minutes, :midnight_meal_minutes,
                     :confidence_score, :updated_at)
                """, data)
                
                saved_count += 1
                
                if saved_count % 1000 == 0:
                    conn.commit()  # ì£¼ê¸°ì ìœ¼ë¡œ ì»¤ë°‹
                    self.logger.info(f"  {saved_count}ê±´ ì €ì¥...")
            
            conn.commit()  # ìµœì¢… ì»¤ë°‹
            
        except Exception as e:
            self.logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            conn.rollback()
            
        finally:
            conn.close()
        
        self.logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {saved_count}ê±´")
        return saved_count


def process_employee_chunk(temp_file_path: str, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜
    IndividualAnalyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì¸ë³„ ë¶„ì„ ìˆ˜í–‰
    """
    # IndividualAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    from src.analysis.individual_analyzer import IndividualAnalyzer
    from src.database import DatabaseManager
    
    # DatabaseManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    db_manager = DatabaseManager()
    analyzer = IndividualAnalyzer(db_manager)
    
    results = []
    
    for employee_id in employee_ids:
        try:
            # ê°œì¸ë³„ ë¶„ì„ ì‹¤í–‰ (target_date í•˜ë£¨ë§Œ)
            analysis_result = analyzer.analyze_individual(
                employee_id=employee_id,
                start_date=datetime.combine(target_date, datetime.min.time()),
                end_date=datetime.combine(target_date, datetime.max.time())
            )
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë°°ì¹˜ í”„ë¡œì„¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if analysis_result:
                work_time = analysis_result.get('work_time_analysis', {})
                meal_time = analysis_result.get('meal_time_analysis', {})
                activity = analysis_result.get('activity_analysis', {})
                timeline = analysis_result.get('timeline_analysis', {})
                
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'success',
                    'work_time_analysis': work_time,
                    'meal_time_analysis': meal_time,
                    'activity_analysis': activity,
                    'timeline_analysis': timeline,
                    'data_quality': analysis_result.get('data_quality', {})
                })
            else:
                results.append({
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_data'
                })
            
        except Exception as e:
            results.append({
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'error',
                'error': str(e)
            })
    
    return results