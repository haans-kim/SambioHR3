"""
ê°„ì†Œí™”ëœ ë°°ì¹˜ í”„ë¡œì„¸ì„œ - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” ìµœì í™” ë²„ì „
ì¡°ì§ë³„ ê·¼ë¬´ë¶„ì„ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from datetime import date, datetime, timedelta
import pickle
import logging
import time
from multiprocessing import Pool, Manager, cpu_count
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import sqlite3
from pathlib import Path
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))


class SimpleBatchProcessor:
    """ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” ê°„ì†Œí™”ëœ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, num_workers: int = 4, db_path: str = None):
        """
        Args:
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’ 4ë¡œ ì•ˆì „í•˜ê²Œ ì„¤ì •)
            db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        """
        self.num_workers = num_workers
        
        # ë¡œê±° ì„¤ì • (ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            logging.basicConfig(level=logging.INFO)
        
        # ê¸°ì¡´ DB ì—°ê²° (SQLite)
        if db_path:
            self.db_path = db_path
        else:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ìƒëŒ€ ê²½ë¡œë¡œ ì°¾ê¸°
            db_file = Path(project_root) / 'data' / 'sambio_human.db'
            if db_file.exists():
                self.db_path = str(db_file)
            else:
                self.db_path = 'data/sambio_human.db'
        
        # ë°ì´í„° ìºì‹œ
        self.data_cache = {}
        
        self.logger.info(f"SimpleBatchProcessor ì´ˆê¸°í™” (ì›Œì»¤: {self.num_workers}, DB: {self.db_path})")
    
    def preload_data_for_date(self, target_date: date) -> Dict[str, pd.DataFrame]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œ
        ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ëª¨ë“  ì§ì›ì´ ê³µìœ 
        """
        self.logger.info(f"ğŸ“¥ {target_date} ë°ì´í„° ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        start_time = time.time()
        
        # DB íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(self.db_path).exists():
            self.logger.error(f"DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
            raise FileNotFoundError(f"Database file not found: {self.db_path}")
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # 1. íƒœê·¸ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
            # ENTE_DTê°€ ë‚ ì§œ, ì¶œì…ì‹œê°ì´ ì‹œê°„
            prev_date = target_date - timedelta(days=1)
            
            # ENTE_DTë¥¼ ë‚ ì§œë¡œ ë³€í™˜ (YYYYMMDD í˜•ì‹ì¼ ê°€ëŠ¥ì„±)
            target_date_str = target_date.strftime('%Y%m%d')
            prev_date_str = prev_date.strftime('%Y%m%d')
            
            tag_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ENTE_DT, ì¶œì…ì‹œê°, DR_NM, INOUT_GB,
                       CENTER, BU, TEAM, GROUP_A, PART
                FROM tag_data 
                WHERE ENTE_DT BETWEEN {prev_date_str} AND {target_date_str}
                ORDER BY ì‚¬ë²ˆ, ì¶œì…ì‹œê°
            """
            tag_data = pd.read_sql_query(tag_query, conn)
            self.logger.info(f"  íƒœê·¸ ë°ì´í„°: {len(tag_data):,}ê±´")
            
            # 2. ì‹ì‚¬ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
            meal_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ì·¨ì‹ì¼ì‹œ, ì •ì‚°ì¼, ì‹ë‹¹ëª…, 
                       ì‹ì‚¬êµ¬ë¶„ëª…, ì„±ëª…, ë¶€ì„œ
                FROM meal_data
                WHERE DATE(ì •ì‚°ì¼) BETWEEN '{prev_date}' AND '{target_date}'
            """
            meal_data = pd.read_sql_query(meal_query, conn)
            self.logger.info(f"  ì‹ì‚¬ ë°ì´í„°: {len(meal_data):,}ê±´")
            
            # 3. Claim ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
            claim_query = f"""
                SELECT ì‚¬ë²ˆ as employee_id, ê·¼ë¬´ì¼, WORKSCHDTYPNM, 
                       ê·¼ë¬´ì‹œê°„, ì‹œì‘, ì¢…ë£Œ, ì„±ëª…, ë¶€ì„œ, ì§ê¸‰
                FROM claim_data
                WHERE DATE(ê·¼ë¬´ì¼) = '{target_date}'
            """
            claim_data = pd.read_sql_query(claim_query, conn)
            self.logger.info(f"  Claim ë°ì´í„°: {len(claim_data):,}ê±´")
            
            # 4. ì¥ë¹„ ë°ì´í„° ë¡œë“œ (í…Œì´ë¸”ì´ ìˆì„ ê²½ìš°ë§Œ)
            try:
                equipment_query = f"""
                    SELECT * FROM equipment_data
                    WHERE DATE(datetime) BETWEEN '{prev_date}' AND '{target_date}'
                """
                equipment_data = pd.read_sql_query(equipment_query, conn)
                self.logger.info(f"  ì¥ë¹„ ë°ì´í„°: {len(equipment_data):,}ê±´")
            except:
                equipment_data = pd.DataFrame()
                self.logger.info(f"  ì¥ë¹„ ë°ì´í„°: ì—†ìŒ")
            
            # 5. ê·¼íƒœ ë°ì´í„° ë¡œë“œ (í…Œì´ë¸”ì´ ìˆì„ ê²½ìš°ë§Œ)
            try:
                attendance_query = f"""
                    SELECT * FROM attendance_data
                    WHERE DATE(work_date) = '{target_date}'
                """
                attendance_data = pd.read_sql_query(attendance_query, conn)
                self.logger.info(f"  ê·¼íƒœ ë°ì´í„°: {len(attendance_data):,}ê±´")
            except:
                attendance_data = pd.DataFrame()
                self.logger.info(f"  ê·¼íƒœ ë°ì´í„°: ì—†ìŒ")
            
        finally:
            conn.close()
        
        # ìºì‹œì— ì €ì¥
        self.data_cache = {
            'tag_data': tag_data,
            'meal_data': meal_data,
            'claim_data': claim_data,
            'equipment_data': equipment_data,
            'attendance_data': attendance_data,
            'target_date': target_date
        }
        
        elapsed = time.time() - start_time
        self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
        
        return self.data_cache
    
    def analyze_employee_batch(self, employee_id: str, target_date: date) -> Dict[str, Any]:
        """
        ê°œë³„ ì§ì› ë¶„ì„ (ë°°ì¹˜ìš© ìµœì í™”)
        ê¸°ì¡´ execute_analysisì™€ í˜¸í™˜ë˜ëŠ” ê²°ê³¼ ë°˜í™˜
        """
        try:
            # ìºì‹œëœ ë°ì´í„°ì—ì„œ ì§ì› ë°ì´í„° í•„í„°ë§
            if 'tag_data' not in self.data_cache:
                self.logger.error("ë°ì´í„°ê°€ ì‚¬ì „ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            # 1. íƒœê·¸ ë°ì´í„° í•„í„°ë§
            tag_data = self.data_cache['tag_data']
            # employee_idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            emp_tag_data = tag_data[tag_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            if emp_tag_data.empty:
                return {
                    'employee_id': employee_id,
                    'analysis_date': target_date.isoformat(),
                    'status': 'no_data'
                }
            
            # 2. ì‹ì‚¬ ë°ì´í„° í•„í„°ë§
            meal_data = self.data_cache['meal_data']
            emp_meal_data = meal_data[meal_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            # 3. Claim ë°ì´í„° í•„í„°ë§
            claim_data = self.data_cache['claim_data']
            emp_claim_data = claim_data[claim_data['employee_id'].astype(str) == str(employee_id)].copy()
            
            # 4. ì¥ë¹„ ë°ì´í„° í•„í„°ë§
            equipment_data = self.data_cache.get('equipment_data', pd.DataFrame())
            if not equipment_data.empty:
                emp_equipment_data = equipment_data[equipment_data['employee_id'] == employee_id].copy()
            else:
                emp_equipment_data = pd.DataFrame()
            
            # 5. ê·¼íƒœ ë°ì´í„° í•„í„°ë§
            attendance_data = self.data_cache.get('attendance_data', pd.DataFrame())
            if not attendance_data.empty:
                emp_attendance_data = attendance_data[attendance_data['employee_id'] == employee_id].copy()
            else:
                emp_attendance_data = pd.DataFrame()
            
            # 6. ê°„ë‹¨í•œ ë¶„ì„ ìˆ˜í–‰ (execute_analysisì˜ í•µì‹¬ ë¡œì§ë§Œ)
            
            # ê·¼ë¬´ ì‹œê°„ ê³„ì‚° (2êµëŒ€ ê·¼ë¬´ ê³ ë ¤)
            if not emp_tag_data.empty:
                try:
                    # ENTE_DT(ë‚ ì§œ)ì™€ ì¶œì…ì‹œê°(ì‹œê°„) ê²°í•©
                    emp_tag_data['datetime'] = emp_tag_data.apply(
                        lambda row: pd.to_datetime(f"{row['ENTE_DT']} {str(row['ì¶œì…ì‹œê°']).zfill(6)}", 
                                                  format='%Y%m%d %H%M%S', errors='coerce'),
                        axis=1
                    )
                    
                    # NaT ì œê±°
                    emp_tag_data = emp_tag_data.dropna(subset=['datetime'])
                    
                    if not emp_tag_data.empty:
                        # 2êµëŒ€ ê·¼ë¬´ ì‹œìŠ¤í…œ: target_date ê¸°ì¤€ìœ¼ë¡œ ê·¼ë¬´ ì‹œê°„ ê³„ì‚°
                        target_start = pd.to_datetime(f"{target_date} 00:00:00")
                        target_end = pd.to_datetime(f"{target_date} 23:59:59")
                        
                        # ë¨¼ì € target_dateì˜ íƒœê·¸ë§Œ í•„í„°ë§
                        day_tags = emp_tag_data[
                            (emp_tag_data['datetime'] >= target_start) & 
                            (emp_tag_data['datetime'] <= target_end)
                        ]
                        
                        if not day_tags.empty:
                            first_tag = day_tags['datetime'].min()
                            last_tag = day_tags['datetime'].max()
                            
                            # ì•¼ê°„ ê·¼ë¬´ì¸ ê²½ìš° ë‹¤ìŒë‚  ì˜¤ì „ íƒœê·¸ë„ í™•ì¸
                            if last_tag.hour >= 20:  # ì•¼ê°„ ê·¼ë¬´ ê°€ëŠ¥ì„±
                                next_day_start = target_end + pd.Timedelta(seconds=1)
                                next_day_end = next_day_start + pd.Timedelta(hours=12)
                                
                                next_day_tags = emp_tag_data[
                                    (emp_tag_data['datetime'] >= next_day_start) & 
                                    (emp_tag_data['datetime'] <= next_day_end)
                                ]
                                
                                if not next_day_tags.empty:
                                    last_tag = next_day_tags['datetime'].max()
                            
                            total_hours = (last_tag - first_tag).total_seconds() / 3600
                            # ìµœëŒ€ 12ì‹œê°„ìœ¼ë¡œ ì œí•œ
                            total_hours = min(total_hours, 12)
                        else:
                            total_hours = 0
                            first_tag = None
                            last_tag = None
                    else:
                        total_hours = 0
                        first_tag = None
                        last_tag = None
                except Exception as e:
                    self.logger.warning(f"ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                    total_hours = 0
                    first_tag = None
                    last_tag = None
            else:
                total_hours = 0
                first_tag = None
                last_tag = None
            
            # Claim ë°ì´í„°ì—ì„œ ì˜ˆì • ê·¼ë¬´ì‹œê°„
            if not emp_claim_data.empty:
                # ê·¼ë¬´ì‹œê°„ íŒŒì‹± (ì˜ˆ: "08:00~17:00" í˜•ì‹)
                work_time_str = emp_claim_data.iloc[0].get('ê·¼ë¬´ì‹œê°„', '')
                try:
                    if '~' in work_time_str:
                        start_str, end_str = work_time_str.split('~')
                        # ì‹œê°„ ê³„ì‚° ë¡œì§ (ê°„ë‹¨íˆ 8ì‹œê°„ìœ¼ë¡œ ê°€ì •)
                        scheduled_hours = 8
                    else:
                        scheduled_hours = 8
                except:
                    scheduled_hours = 8
                    
                work_type = emp_claim_data.iloc[0].get('WORKSCHDTYPNM', 'ì¼ë°˜ê·¼ë¬´')
            else:
                scheduled_hours = 8
                work_type = 'ì¼ë°˜ê·¼ë¬´'
            
            # ì‹ì‚¬ íšŸìˆ˜
            meal_count = len(emp_meal_data)
            
            # ì‹¤ì œ ê·¼ë¬´ì‹œê°„ ì¶”ì • (ê°„ë‹¨í•œ ë¡œì§)
            # ì´ ì‹œê°„ì—ì„œ ì‹ì‚¬ì‹œê°„(30ë¶„ * íšŸìˆ˜) ë¹¼ê¸°
            actual_work_hours = max(0, total_hours - (meal_count * 0.5))
            
            # íš¨ìœ¨ì„± ê³„ì‚°
            if scheduled_hours > 0:
                efficiency_ratio = (actual_work_hours / scheduled_hours) * 100
            else:
                efficiency_ratio = 0
            
            # í™œë™ ìš”ì•½ (ê°„ë‹¨ ë²„ì „)
            activity_summary = {
                'WORK': actual_work_hours * 60,  # ë¶„ ë‹¨ìœ„
                'MEAL': meal_count * 30,  # ì‹ì‚¬ë‹¹ 30ë¶„ ê°€ì •
                'REST': max(0, (total_hours - actual_work_hours - meal_count * 0.5) * 60)
            }
            
            # ê²°ê³¼ ë°˜í™˜ (ê¸°ì¡´ execute_analysisì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹)
            return {
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'success',
                'work_start': first_tag.isoformat() if first_tag else None,
                'work_end': last_tag.isoformat() if last_tag else None,
                'work_time_analysis': {
                    'total_hours': total_hours,
                    'actual_work_hours': actual_work_hours,
                    'scheduled_hours': scheduled_hours,
                    'efficiency_ratio': efficiency_ratio
                },
                'meal_time_analysis': {
                    'total_meal_time': meal_count * 30,
                    'meal_count': meal_count
                },
                'activity_summary': activity_summary,
                'work_type': work_type,
                'tag_count': len(emp_tag_data),
                'attendance_hours': total_hours,  # ì¡°ì§ ë¶„ì„ì—ì„œ ì‚¬ìš©
                'meeting_time': 0,  # ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” 0
                'movement_time': 0,  # ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” 0
                'rest_time': activity_summary['REST'] / 60,  # ì‹œê°„ ë‹¨ìœ„
                'work_estimation_rate': efficiency_ratio,
                'data_reliability': 80 if len(emp_tag_data) > 10 else 50
            }
            
        except Exception as e:
            self.logger.error(f"ì§ì› {employee_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'employee_id': employee_id,
                'analysis_date': target_date.isoformat(),
                'status': 'error',
                'error': str(e)
            }
    
    @staticmethod
    def _analyze_worker(args):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  ë¶„ì„ í•¨ìˆ˜"""
        processor, employee_id, target_date = args
        return processor.analyze_employee_batch(employee_id, target_date)
    
    def batch_analyze_employees(self, employee_ids: List[str], target_date: date) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ì›ì„ ë³‘ë ¬ë¡œ ë¶„ì„
        """
        self.logger.info(f"ğŸš€ ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(employee_ids)}ëª…, {self.num_workers}ê°œ ì›Œì»¤")
        start_time = time.time()
        
        # 1. ë°ì´í„° ì‚¬ì „ ë¡œë“œ (í•œ ë²ˆë§Œ!)
        self.preload_data_for_date(target_date)
        
        # 2. ìˆœì°¨ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©) ë˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬
        if self.num_workers == 1:
            # ìˆœì°¨ ì²˜ë¦¬ (ë””ë²„ê¹…ìš©)
            results = []
            for i, emp_id in enumerate(employee_ids):
                result = self.analyze_employee_batch(emp_id, target_date)
                results.append(result)
                if (i + 1) % 100 == 0:
                    elapsed = time.time() - start_time
                    rate = (i + 1) / elapsed
                    remaining = (len(employee_ids) - i - 1) / rate if rate > 0 else 0
                    self.logger.info(f"  ì§„í–‰: {i+1}/{len(employee_ids)} "
                                   f"({rate:.1f}ëª…/ì´ˆ, ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„)")
        else:
            # ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
            # ThreadPool ì‚¬ìš© (ë°ì´í„°ê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆìœ¼ë¯€ë¡œ I/O bound ì‘ì—…)
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            results = []
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                # ì‘ì—… ì œì¶œ
                future_to_emp = {
                    executor.submit(self.analyze_employee_batch, emp_id, target_date): emp_id 
                    for emp_id in employee_ids
                }
                
                # ê²°ê³¼ ìˆ˜ì§‘
                completed = 0
                for future in as_completed(future_to_emp):
                    emp_id = future_to_emp[future]
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        self.logger.error(f"ì§ì› {emp_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
                        results.append({
                            'employee_id': emp_id,
                            'analysis_date': target_date.isoformat(),
                            'status': 'error',
                            'error': str(e)
                        })
                    
                    completed += 1
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if completed % 100 == 0 or completed == len(employee_ids):
                        elapsed = time.time() - start_time
                        rate = completed / elapsed if elapsed > 0 else 0
                        remaining = (len(employee_ids) - completed) / rate if rate > 0 else 0
                        self.logger.info(f"  ì§„í–‰: {completed}/{len(employee_ids)} "
                                       f"({rate:.1f}ëª…/ì´ˆ, ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„)")
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r in results if r.get('status') == 'success')
        
        self.logger.info(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        self.logger.info(f"  - ì´ ì§ì›: {len(employee_ids)}ëª…")
        self.logger.info(f"  - ì„±ê³µ: {success_count}ëª…")
        self.logger.info(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        self.logger.info(f"  - ì²˜ë¦¬ ì†ë„: {len(employee_ids)/elapsed:.1f}ëª…/ì´ˆ")
        
        return results
    
    def save_results_to_db(self, results: List[Dict[str, Any]]) -> int:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥
        ê¸°ì¡´ daily_analysis_results í…Œì´ë¸” ì‚¬ìš©
        """
        self.logger.info(f"ğŸ’¾ {len(results)}ê±´ DB ì €ì¥ ì‹œì‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        try:
            for result in results:
                if result.get('status') != 'success':
                    continue
                
                # ë°ì´í„° ì¤€ë¹„
                data = {
                    'employee_id': result['employee_id'],
                    'analysis_date': result['analysis_date'],
                    'work_start': result.get('work_start'),
                    'work_end': result.get('work_end'),
                    'total_hours': result['work_time_analysis']['total_hours'],
                    'actual_work_hours': result['work_time_analysis']['actual_work_hours'],
                    'claimed_work_hours': result['work_time_analysis']['scheduled_hours'],
                    'efficiency_ratio': result['work_time_analysis']['efficiency_ratio'],
                    'meal_count': result['meal_time_analysis']['meal_count'],
                    'tag_count': result.get('tag_count', 0),
                    'updated_at': datetime.now().isoformat()
                }
                
                # UPSERT ì¿¼ë¦¬
                cursor.execute("""
                    INSERT OR REPLACE INTO daily_analysis_results 
                    (employee_id, analysis_date, work_start, work_end,
                     total_hours, actual_work_hours, claimed_work_hours,
                     efficiency_ratio, meal_count, tag_count, updated_at)
                    VALUES 
                    (:employee_id, :analysis_date, :work_start, :work_end,
                     :total_hours, :actual_work_hours, :claimed_work_hours,
                     :efficiency_ratio, :meal_count, :tag_count, :updated_at)
                """, data)
                
                saved_count += 1
                
                if saved_count % 100 == 0:
                    conn.commit()  # ì£¼ê¸°ì ìœ¼ë¡œ ì»¤ë°‹
                    self.logger.info(f"  {saved_count}ê±´ ì €ì¥...")
            
            conn.commit()  # ìµœì¢… ì»¤ë°‹
            
        except Exception as e:
            self.logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            conn.rollback()
            
        finally:
            conn.close()
        
        self.logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {saved_count}ê±´")
        return saved_count


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_simple_batch_processor():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"""
    logging.basicConfig(level=logging.INFO)
    
    processor = SimpleBatchProcessor(num_workers=1)
    
    # í…ŒìŠ¤íŠ¸í•  ì§ì› ID (ì˜ˆì‹œ)
    test_employees = ['20170124', '20170125', '20170126']
    test_date = date(2025, 6, 15)
    
    # ë¶„ì„ ì‹¤í–‰
    results = processor.batch_analyze_employees(test_employees, test_date)
    
    # ê²°ê³¼ ì¶œë ¥
    for result in results:
        if result.get('status') == 'success':
            print(f"\nì§ì› {result['employee_id']}:")
            print(f"  ê·¼ë¬´ì‹œê°„: {result['work_time_analysis']['actual_work_hours']:.1f}ì‹œê°„")
            print(f"  íš¨ìœ¨ì„±: {result['work_time_analysis']['efficiency_ratio']:.1f}%")
            print(f"  íƒœê·¸ ìˆ˜: {result.get('tag_count', 0)}ê°œ")
    
    # DB ì €ì¥ í…ŒìŠ¤íŠ¸
    saved = processor.save_results_to_db(results)
    print(f"\nDB ì €ì¥: {saved}ê±´")


if __name__ == "__main__":
    test_simple_batch_processor()