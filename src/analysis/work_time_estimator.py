"""
ê·¼ë¬´ì‹œê°„ ì¶”ì •ë¥  ë° ì‹ ë¢°ë„ ê³„ì‚° ëª¨ë“ˆ
íƒœê·¸ ë°ì´í„°ì˜ ë°€ë„ì™€ íŒ¨í„´ì— ë”°ë¥¸ ì¶”ì • ì‹ ë¢°ë„ ê³„ì‚°
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
from typing import Dict, List, Tuple, Optional
import logging
from .office_worker_estimator import OfficeWorkerEstimator

logger = logging.getLogger(__name__)


class WorkTimeEstimator:
    """ê·¼ë¬´ì‹œê°„ ì¶”ì • ë° ì‹ ë¢°ë„ ê³„ì‚°"""
    
    def __init__(self):
        # ì§êµ°ë³„ ê¸°ë³¸ ì¶”ì •ë¥  (ìƒì‚°ì§ vs ì‚¬ë¬´ì§)
        self.BASE_ESTIMATION_RATES = {
            'production': 0.85,  # ìƒì‚°ì§: íƒœê·¸ ë°ì´í„° í’ë¶€
            'office': 0.65,      # ì‚¬ë¬´ì§: íƒœê·¸ ë°ì´í„° ì œí•œì 
            'unknown': 0.70      # ë¯¸ë¶„ë¥˜
        }
        
        # ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
        self.DATA_QUALITY_WEIGHTS = {
            'tag_coverage': 0.3,      # íƒœê·¸ ì»¤ë²„ë¦¬ì§€ (ë¹ˆë„)
            'activity_density': 0.3,   # í™œë™ ë°€ë„ (Oíƒœê·¸, Knox)
            'time_continuity': 0.2,    # ì‹œê°„ ì—°ì†ì„±
            'location_diversity': 0.2   # ìœ„ì¹˜ ë‹¤ì–‘ì„±
        }
        
        # ì‚¬ë¬´ì§ íŠ¹í™” ì¶”ì •ê¸°
        self.office_estimator = OfficeWorkerEstimator()
    
    def calculate_estimation_metrics(self, daily_data: pd.DataFrame, 
                                    employee_info: Dict = None) -> Dict:
        """
        ê·¼ë¬´ì‹œê°„ ì¶”ì • ì§€í‘œ ê³„ì‚°
        
        Returns:
            ì¶”ì • ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        metrics = {
            'estimation_rate': 0.0,      # ì¶”ì •ë¥  (0-100%)
            'confidence_interval': (0, 0), # ì‹ ë¢°êµ¬ê°„
            'variance': 0.0,              # ë¶„ì‚°
            'data_quality_score': 0.0,    # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
            'estimation_type': 'unknown', # ì¶”ì • ìœ í˜•
            'quality_breakdown': {},       # í’ˆì§ˆ ì„¸ë¶€ í•­ëª©
            'office_estimation': None      # ì‚¬ë¬´ì§ íŠ¹í™” ì¶”ì • ê²°ê³¼
        }
        
        if daily_data.empty:
            return metrics
        
        # 1. ì§êµ° íŒë³„
        job_type = self.identify_job_type(daily_data, employee_info)
        metrics['estimation_type'] = job_type
        
        # 2. ì‚¬ë¬´ì§ì¸ ê²½ìš° íŠ¹í™” ì¶”ì • ì‚¬ìš©
        if job_type == 'office':
            office_result = self.office_estimator.estimate_office_work_time(
                daily_data, employee_info
            )
            metrics['office_estimation'] = office_result
            
            # ê¼¬ë¦¬ë¬¼ê¸° í™•ë¥ ì´ ë†’ìœ¼ë©´ ë‚®ì€ ì‹ ë¢°ë„
            if office_result['tailgating_probability'] > 0.7:
                metrics['estimation_rate'] = 30.0
                metrics['confidence_interval'] = (20, 40)
                metrics['variance'] = 0.08
                metrics['data_quality_score'] = 0.3
                
                # í’ˆì§ˆ ì„¸ë¶€ í•­ëª©
                metrics['quality_breakdown'] = {
                    'tag_coverage': 0.2,
                    'activity_density': 0.2,
                    'time_continuity': 0.3,
                    'location_diversity': 0.4,
                    'tailgating_warning': True
                }
                
                logger.warning(f"ì‚¬ë¬´ì§ ê¼¬ë¦¬ë¬¼ê¸° ì˜ì‹¬: í™•ë¥  {office_result['tailgating_probability']:.1%}")
                return metrics
        
        # 2. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        quality_scores = self.assess_data_quality(daily_data)
        metrics['quality_breakdown'] = quality_scores
        
        # 3. ì¢…í•© ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        total_quality = sum(
            score * self.DATA_QUALITY_WEIGHTS.get(key, 0)
            for key, score in quality_scores.items()
        )
        metrics['data_quality_score'] = total_quality
        
        # 4. ì¶”ì •ë¥  ê³„ì‚°
        base_rate = self.BASE_ESTIMATION_RATES[job_type]
        adjusted_rate = self.adjust_estimation_rate(base_rate, total_quality)
        metrics['estimation_rate'] = adjusted_rate * 100  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
        
        # 5. ë¶„ì‚° ë° ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        variance = self.calculate_variance(daily_data, job_type)
        metrics['variance'] = variance
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (95% ì‹ ë¢°ìˆ˜ì¤€)
        std_dev = np.sqrt(variance)
        confidence_margin = 1.96 * std_dev  # 95% ì‹ ë¢°êµ¬ê°„
        
        metrics['confidence_interval'] = (
            max(0, adjusted_rate - confidence_margin) * 100,
            min(100, adjusted_rate + confidence_margin) * 100
        )
        
        return metrics
    
    def identify_job_type(self, daily_data: pd.DataFrame, 
                          employee_info: Dict = None) -> str:
        """
        ì§êµ° íŒë³„ (ìƒì‚°ì§/ì‚¬ë¬´ì§)
        
        Returns:
            'production', 'office', ë˜ëŠ” 'unknown'
        """
        # ì§ì› ì •ë³´ì—ì„œ ì§êµ° í™•ì¸
        if employee_info:
            # ë¶€ì„œëª…ì´ë‚˜ ì§êµ°ìœ¼ë¡œ íŒë³„
            dept = str(employee_info.get('ë¶€ì„œ', '')).lower()
            position = str(employee_info.get('ì§ê¸‰', '')).lower()
            job_type = str(employee_info.get('ì§êµ°', '')).lower()
            
            # ì‚¬ë¬´ì§ í‚¤ì›Œë“œ
            office_keywords = ['ì‚¬ë¬´', 'ê´€ë¦¬', 'ê²½ì˜', 'ì¸ì‚¬', 'ì¬ë¬´', 'ì˜ì—…', 'ë§ˆì¼€íŒ…', 
                             'ê¸°íš', 'ì§€ì›', 'ì´ë¬´', 'ê²½ë¦¬', 'it', 'ì „ì‚°', 'ì—°êµ¬', 'ê°œë°œ']
            # ìƒì‚°ì§ í‚¤ì›Œë“œ
            production_keywords = ['ìƒì‚°', 'ì œì¡°', 'í˜„ì¥', 'ê¸°ìˆ ', 'í’ˆì§ˆ', 'ê³µì •', 'ì¡°ë¦½', 
                                  'í¬ì¥', 'ë¬¼ë¥˜', 'ì°½ê³ ', 'ìš´ì†¡']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in office_keywords:
                if keyword in dept or keyword in position or keyword in job_type:
                    logger.info(f"ì‚¬ë¬´ì§ íŒë³„: {dept} / {position} / {job_type}")
                    return 'office'
            
            for keyword in production_keywords:
                if keyword in dept or keyword in position or keyword in job_type:
                    logger.info(f"ìƒì‚°ì§ íŒë³„: {dept} / {position} / {job_type}")
                    return 'production'
        
        # íƒœê·¸ íŒ¨í„´ìœ¼ë¡œ ì¶”ì • (ì‚¬ë¬´ì§ì€ íƒœê·¸ê°€ ì ìŒì„ ê³ ë ¤)
        total_records = len(daily_data)
        
        if total_records == 0:
            return 'unknown'
        
        # ì‹œê°„ë‹¹ íƒœê·¸ ìˆ˜ ê³„ì‚°
        time_col = 'timestamp' if 'timestamp' in daily_data.columns else 'datetime'
        if time_col in daily_data.columns:
            if daily_data[time_col].dtype == 'object' or daily_data[time_col].dtype == 'str':
                daily_data[time_col] = pd.to_datetime(daily_data[time_col])
            
            time_range = (daily_data[time_col].max() - daily_data[time_col].min()).total_seconds() / 3600
            tags_per_hour = total_records / time_range if time_range > 0 else 0
            
            # ì‚¬ë¬´ì§: ì‹œê°„ë‹¹ íƒœê·¸ 5ê°œ ë¯¸ë§Œ (ë“œë¬¸ íƒœê·¸)
            if tags_per_hour < 5:
                logger.info(f"ì‚¬ë¬´ì§ ì¶”ì •: ì‹œê°„ë‹¹ íƒœê·¸ {tags_per_hour:.1f}ê°œ")
                return 'office'
            # ìƒì‚°ì§: ì‹œê°„ë‹¹ íƒœê·¸ 10ê°œ ì´ìƒ (ë¹ˆë²ˆí•œ íƒœê·¸)
            elif tags_per_hour > 10:
                logger.info(f"ìƒì‚°ì§ ì¶”ì •: ì‹œê°„ë‹¹ íƒœê·¸ {tags_per_hour:.1f}ê°œ")
                return 'production'
        
        return 'unknown'
    
    def assess_data_quality(self, daily_data: pd.DataFrame) -> Dict[str, float]:
        """
        ë°ì´í„° í’ˆì§ˆ í‰ê°€
        
        Returns:
            í’ˆì§ˆ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ (ê° í•­ëª© 0-1 ì ìˆ˜)
        """
        scores = {}
        
        # timestamp ë˜ëŠ” datetime ì»¬ëŸ¼ í™•ì¸
        time_col = 'timestamp' if 'timestamp' in daily_data.columns else 'datetime'
        
        # 1. íƒœê·¸ ì»¤ë²„ë¦¬ì§€ (ì‹œê°„ë‹¹ íƒœê·¸ ìˆ˜)
        if len(daily_data) > 0 and time_col in daily_data.columns:
            # datetime ì»¬ëŸ¼ì„ pd.Timestampë¡œ ë³€í™˜
            if daily_data[time_col].dtype == 'object' or daily_data[time_col].dtype == 'str':
                daily_data[time_col] = pd.to_datetime(daily_data[time_col])
            
            time_range = (daily_data[time_col].max() - daily_data[time_col].min()).total_seconds() / 3600
            if time_range > 0:
                tags_per_hour = len(daily_data) / time_range
                # ì‹œê°„ë‹¹ 10ê°œ ì´ìƒì´ë©´ 100%, 2ê°œ ì´í•˜ë©´ 20%
                scores['tag_coverage'] = min(1.0, max(0.2, tags_per_hour / 10))
            else:
                scores['tag_coverage'] = 0.2
        else:
            scores['tag_coverage'] = 0.0
        
        # 2. í™œë™ ë°€ë„ (Oíƒœê·¸, Knox ë°ì´í„° ë¹„ìœ¨)
        activity_count = 0
        if 'INOUT_GB' in daily_data.columns:
            activity_count += (daily_data['INOUT_GB'] == 'O').sum()
        if 'source' in daily_data.columns:
            activity_count += daily_data['source'].isin(['Knox_Approval', 'Knox_Mail', 'EAM', 'LAMS', 'MES']).sum()
        
        activity_ratio = activity_count / len(daily_data) if len(daily_data) > 0 else 0
        scores['activity_density'] = min(1.0, activity_ratio * 3)  # 33% ì´ìƒì´ë©´ 100%
        
        # 3. ì‹œê°„ ì—°ì†ì„± (íƒœê·¸ ê°„ ì‹œê°„ ê°„ê²©)
        if len(daily_data) > 1 and time_col in daily_data.columns:
            time_gaps = daily_data[time_col].diff().dropna()
            if len(time_gaps) > 0:
                median_gap = time_gaps.median().total_seconds() / 60  # ë¶„ ë‹¨ìœ„
                
                # ì¤‘ê°„ ê°„ê²©ì´ 10ë¶„ ì´í•˜ë©´ 100%, 60ë¶„ ì´ìƒì´ë©´ 20%
                if median_gap <= 10:
                    scores['time_continuity'] = 1.0
                elif median_gap >= 60:
                    scores['time_continuity'] = 0.2
                else:
                    scores['time_continuity'] = 1.0 - (median_gap - 10) / 50 * 0.8
            else:
                scores['time_continuity'] = 0.2
        else:
            scores['time_continuity'] = 0.2
        
        # 4. ìœ„ì¹˜ ë‹¤ì–‘ì„± (ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ íƒœê·¸)
        if 'DR_NM' in daily_data.columns:
            unique_locations = daily_data['DR_NM'].nunique()
            # 10ê°œ ì´ìƒ ìœ„ì¹˜ë©´ 100%, 2ê°œ ì´í•˜ë©´ 30%
            scores['location_diversity'] = min(1.0, max(0.3, unique_locations / 10))
        else:
            scores['location_diversity'] = 0.3
        
        return scores
    
    def adjust_estimation_rate(self, base_rate: float, quality_score: float) -> float:
        """
        ë°ì´í„° í’ˆì§ˆì— ë”°ë¥¸ ì¶”ì •ë¥  ì¡°ì •
        
        Args:
            base_rate: ê¸°ë³¸ ì¶”ì •ë¥  (ì§êµ°ë³„)
            quality_score: ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (0-1)
            
        Returns:
            ì¡°ì •ëœ ì¶”ì •ë¥  (0-1)
        """
        # í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ ì¶”ì •ë¥  ìƒìŠ¹, ë‚˜ì˜ë©´ í•˜ë½
        # í’ˆì§ˆ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Â±20% ì¡°ì •
        adjustment_factor = 1 + (quality_score - 0.5) * 0.4
        adjusted_rate = base_rate * adjustment_factor
        
        # 0-1 ë²”ìœ„ë¡œ ì œí•œ
        return max(0.3, min(0.95, adjusted_rate))
    
    def calculate_variance(self, daily_data: pd.DataFrame, job_type: str) -> float:
        """
        ì¶”ì • ë¶„ì‚° ê³„ì‚°
        
        Returns:
            ë¶„ì‚° ê°’
        """
        base_variance = {
            'production': 0.01,  # ìƒì‚°ì§: ë‚®ì€ ë¶„ì‚°
            'office': 0.04,      # ì‚¬ë¬´ì§: ë†’ì€ ë¶„ì‚°
            'unknown': 0.025     # ë¯¸ë¶„ë¥˜: ì¤‘ê°„ ë¶„ì‚°
        }
        
        variance = base_variance[job_type]
        
        # ë°ì´í„°ê°€ ì ìœ¼ë©´ ë¶„ì‚° ì¦ê°€
        if len(daily_data) < 50:
            variance *= 2.0
        elif len(daily_data) < 100:
            variance *= 1.5
        
        # timestamp ë˜ëŠ” datetime ì»¬ëŸ¼ í™•ì¸
        time_col = 'timestamp' if 'timestamp' in daily_data.columns else 'datetime'
        
        # ì‹œê°„ ê°„ê²©ì´ ë¶ˆê·œì¹™í•˜ë©´ ë¶„ì‚° ì¦ê°€
        if len(daily_data) > 1 and time_col in daily_data.columns:
            # datetime ì»¬ëŸ¼ì„ pd.Timestampë¡œ ë³€í™˜
            if daily_data[time_col].dtype == 'object' or daily_data[time_col].dtype == 'str':
                daily_data[time_col] = pd.to_datetime(daily_data[time_col])
                
            time_gaps = daily_data[time_col].diff().dropna()
            if len(time_gaps) > 0:
                gap_std = time_gaps.std().total_seconds() / 60  # ë¶„ ë‹¨ìœ„
                
                if gap_std > 30:  # í‘œì¤€í¸ì°¨ 30ë¶„ ì´ìƒ
                    variance *= 1.5
        
        return variance
    
    def create_estimation_summary(self, metrics: Dict, 
                                 actual_work_hours: float = None) -> Dict:
        """
        ì¶”ì • ìš”ì•½ ì •ë³´ ìƒì„±
        
        Returns:
            UI í‘œì‹œìš© ìš”ì•½ ì •ë³´
        """
        summary = {
            'title': self.get_estimation_title(metrics['estimation_rate']),
            'color': self.get_estimation_color(metrics['estimation_rate']),
            'description': self.get_estimation_description(metrics),
            'recommendations': self.get_recommendations(metrics)
        }
        
        # ì‹¤ì œ ê·¼ë¬´ì‹œê°„ê³¼ ì¶”ì • ì •ë³´ ê²°í•©
        if actual_work_hours is not None:
            lower_bound = actual_work_hours * metrics['confidence_interval'][0] / 100
            upper_bound = actual_work_hours * metrics['confidence_interval'][1] / 100
            
            summary['estimated_hours'] = {
                'point_estimate': actual_work_hours,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound,
                'display': f"{actual_work_hours:.1f}ì‹œê°„ (Â±{(upper_bound-lower_bound)/2:.1f}ì‹œê°„)"
            }
        
        return summary
    
    def get_estimation_title(self, rate: float) -> str:
        """ì¶”ì •ë¥ ì— ë”°ë¥¸ íƒ€ì´í‹€"""
        if rate >= 90:
            return "ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„"
        elif rate >= 80:
            return "ë†’ì€ ì‹ ë¢°ë„"
        elif rate >= 70:
            return "ë³´í†µ ì‹ ë¢°ë„"
        elif rate >= 60:
            return "ë‚®ì€ ì‹ ë¢°ë„"
        else:
            return "ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„"
    
    def get_estimation_color(self, rate: float) -> str:
        """ì¶”ì •ë¥ ì— ë”°ë¥¸ ìƒ‰ìƒ"""
        if rate >= 90:
            return "#2E7D32"  # ì§„í•œ ì´ˆë¡
        elif rate >= 80:
            return "#43A047"  # ì´ˆë¡
        elif rate >= 70:
            return "#FFA726"  # ì£¼í™©
        elif rate >= 60:
            return "#EF5350"  # ë¹¨ê°•
        else:
            return "#B71C1C"  # ì§„í•œ ë¹¨ê°•
    
    def get_estimation_description(self, metrics: Dict) -> str:
        """ì¶”ì • ì„¤ëª… ìƒì„±"""
        rate = metrics['estimation_rate']
        quality = metrics['data_quality_score']
        job_type = metrics['estimation_type']
        
        job_type_kr = {
            'production': 'ìƒì‚°ì§',
            'office': 'ì‚¬ë¬´ì§', 
            'unknown': 'ë¯¸ë¶„ë¥˜'
        }[job_type]
        
        return (
            f"{job_type_kr} ê·¼ë¬´ìë¡œ ì¶”ì •ë˜ë©°, "
            f"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ëŠ” {quality*100:.1f}%ì…ë‹ˆë‹¤. "
            f"ì¶”ì •ë¥  {rate:.1f}%ë¡œ ê·¼ë¬´ì‹œê°„ì„ ì‚°ì¶œí–ˆìŠµë‹ˆë‹¤."
        )
    
    def get_recommendations(self, metrics: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        breakdown = metrics.get('quality_breakdown', {})
        
        if breakdown.get('tag_coverage', 1) < 0.5:
            recommendations.append("ğŸ“ íƒœê·¸ ë¦¬ë”ê¸° ì¶”ê°€ ì„¤ì¹˜ë¡œ ë°ì´í„° ìˆ˜ì§‘ ê°œì„  í•„ìš”")
        
        if breakdown.get('activity_density', 1) < 0.5:
            recommendations.append("ğŸ’» ì‹œìŠ¤í…œ ì‚¬ìš© ë¡œê·¸ ì—°ë™ í™•ëŒ€ í•„ìš”")
        
        if breakdown.get('time_continuity', 1) < 0.5:
            recommendations.append("â° íƒœê·¸ ì¸ì‹ ê°„ê²©ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤")
        
        if breakdown.get('location_diversity', 1) < 0.5:
            recommendations.append("ğŸ—ºï¸ ì´ë™ ê²½ë¡œ íƒœê·¸ í¬ì¸íŠ¸ ë³´ê°• í•„ìš”")
        
        if not recommendations:
            recommendations.append("âœ… ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
        
        return recommendations